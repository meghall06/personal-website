[
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "\nTalks\n",
    "section": "",
    "text": "RLadies Paris 2022-09-08\n    \n    Keeping it Tidy\n       \n            \n                \n                \n                    R\n                \n                \n                \n                    tidyverse\n                \n                \n            \n            \n    Using the tidyverse to organize, transform, and visualize data.\n    \n        Details\n    \n    \n    \n        Slides\n    \n    \n    \n    \n        Website\n    \n    \n    \n  \n\n  \n    \n      rstudio::conf(2022) 2022-07-28\n    \n    Cultivating Your Own R Ecosystem as a Solo Contributor\n       \n            \n                \n                \n                    R\n                \n                \n                \n                    tidyverse\n                \n                \n            \n            \n    Learn how you can begin to build your own R ecosystem, step by step, to increase the efficiency and impact of your work, even as a solo contributor.\n    \n        Details\n    \n    \n    \n        Slides\n    \n    \n    \n    \n  \n\n  \n    \n      North East Association of Institutional Research (NEAIR) 2022-07-12\n    \n    Streamlining with R\n       \n            \n                \n                \n                    R\n                \n                \n                \n                    tidyverse\n                \n                \n            \n            \n    A tidyverse introduction for R beginners.\n    \n        Details\n    \n    \n    \n        Slides\n    \n    \n    \n    \n    \n        Repo\n    \n    \n  \n\n  \n    \n      Greater Boston useR Group 2022-04-13\n    \n    The Power of Internal Packages\n       \n            \n                \n                \n                    R\n                \n                \n                \n                    tidyverse\n                \n                \n            \n            \n    A short introduction to creating internal organizational packages.\n    \n        Details\n    \n    \n    \n        Slides\n    \n    \n    \n    \n  \n\n  \n    \n      UConn Sports Analytics Symposium 2021-07-21\n    \n    An Introduction to Hockey Analytics with R\n       \n            \n                \n                \n                    R\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    hockey\n                \n                \n            \n            \n    Using R for hockey analytics—tips, tricks, and best practices.\n    \n        Details\n    \n    \n    \n        Slides\n    \n    \n    \n    \n  \n\n  \n    \n      RStudio - R in Sports Analytics 2021-06-15\n    \n    Extending R Markdown\n       \n            \n                \n                \n                    R\n                \n                \n                \n                    tidyverse\n                \n                \n            \n            \n    Discussing the power of R Markdown as an underutilized tool to communicate your analyses through PDFs, slides, and websites.\n    \n        Details\n    \n    \n    \n        Slides\n    \n    \n    \n    \n  \n\n  \n    \n      RLadies Vancouver 2021-03-25\n    \n    How Hockey Taught Me R\n       \n            \n                \n                \n                    R\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    hockey\n                \n                \n            \n            \n    Learning R through hockey, by working through some data manipulation techniques by calculating some box score hockey stats.\n    \n        Details\n    \n    \n    \n        Slides\n    \n    \n    \n    \n  \n\n  \n    \n      Hockey Analytics Night in Canada 2021-02-18\n    \n    Using R for Your Big Data Cup Project\n       \n            \n                \n                \n                    R\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    hockey\n                \n                \n            \n            \n    A short & gentle intro to R, plus some tips and tricks, geared around the Big Data Cup.\n    \n        Details\n    \n    \n    \n        Slides\n    \n    \n    \n    \n  \n\n  \n    \n      Carnegie Mellon Sports Analytics Conference 2020-10-25\n    \n    Leveling Up With The Tidyverse (And Hockey Data)\n       \n            \n                \n                \n                    R\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    hockey\n                \n                \n            \n            \n    Level up your R programming in this workshop targeted to beginner and intermediate R users.\n    \n        Details\n    \n    \n    \n        Slides\n    \n    \n    \n    \n  \n\n  \n    \n      Hockey Analytics Night in Canada 2020-04-18\n    \n    Moving Beyond Excel for Your Hockey Analysis\n       \n            \n                \n                \n                    R\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    hockey\n                \n                \n            \n            \n    Moving beyond Excel to make your analysis more reproducible, efficient, and shareable.\n    \n        Details\n    \n    \n    \n        Slides\n    \n    \n    \n    \n  \n\n  \n    \n      Columbus Blue Jackets Hockey Analytics Conference 2020-02-08\n    \n    The Anatomy of a Power Kill\n       \n            \n                \n                \n                    hockey\n                \n                \n            \n            \n    Examining the rise of the power kill.\n    \n        Details\n    \n    \n    \n        Slides\n    \n    \n    \n    \n  \n\n  \n    \n      Ottawa Hockey Analytics Conference 2019-11-16\n    \n    Discrete Defensive Strategies on the Penalty Kill\n       \n            \n                \n                \n                    hockey\n                \n                \n            \n            \n    Tracking data from the 2018-19 season to discuss broad defensive themes on the penalty kill.\n    \n        Details\n    \n    \n    \n        Slides\n    \n    \n    \n    \n  \n\n  \n    \n      Rochester Institute of Technology Sports Analytics Conference 2019-09-14\n    \n    Tracking Increasing Offense on the Penalty Kill\n       \n            \n                \n                \n                    hockey\n                \n                \n            \n            \n    Examining the last four seasons of NHL data to explore the trend of increasing offense on the penalty kill.\n    \n        Details\n    \n    \n    \n        Slides\n    \n    \n    \n    \n  \n\n  \n    \n      Seattle Hockey Analytics Conference 2019-03-10\n    \n    Aggression and Success in Goalie Pulling\n       \n            \n                \n                \n                    hockey\n                \n                \n            \n            \n    Exploring the trend of when goalies get pulled in the NHL.\n    \n        Details\n    \n    \n    \n        Slides\n    \n    \n    \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2024-02-11-moving-to-quarto/index.html",
    "href": "blog/2024-02-11-moving-to-quarto/index.html",
    "title": "Moving My Website from blogdown to Quarto",
    "section": "",
    "text": "I’ve been a devoted Quarto user for nearly two years and finally (!) got around to moving my website over to Quarto. (In my defense, I spent 2023 not doing anything “extra” outside of work and enjoying my new baby girl.) This post is not meant to be a comprehensive guide to creating or moving a Quarto website—there are, among many others, great ones here on moving from distill and here on moving from blogdown—but rather a collection of things I had to figure out as a fairly experienced Quarto user moving my website over from Hugo Apéro/blogdown."
  },
  {
    "objectID": "blog/2024-02-11-moving-to-quarto/index.html#site-organization",
    "href": "blog/2024-02-11-moving-to-quarto/index.html#site-organization",
    "title": "Moving My Website from blogdown to Quarto",
    "section": "Site organization",
    "text": "Site organization\nFirst things first: where do all the files go? My Quarto website structure is fairly similar to the Hugo/blogdown structure, only even simpler. Before, I had a content folder that held blog and talks folders; now those folders are right at the root of the website.\nHere’s what’s included:\n\n_quarto.yml: YAML options for the overall website\nindex.qmd: file for the homepage\n404.qmd: my own 404 page in case someone finds a broken link\ntheme.scss: CSS for the theme of my website\nblog/ and talks/: described below\nstatic/: described below\n_site/: what Quarto creates when it renders the site and what Netlify builds off of\n_freeze/: what Quarto creates when you set freeze: auto to avoid rerunning code all the time\n_extensions/: what Quarto creates when you use extensions"
  },
  {
    "objectID": "blog/2024-02-11-moving-to-quarto/index.html#blog-and-talks-folders",
    "href": "blog/2024-02-11-moving-to-quarto/index.html#blog-and-talks-folders",
    "title": "Moving My Website from blogdown to Quarto",
    "section": "blog and talks folders",
    "text": "blog and talks folders\nBoth of these folders contain an index.qmd file to specify the YAML options of the landing page, which lists all of the blog posts or talks. Here’s what mine looks like in the talks folder:\ntitle: &lt;p class=\"center-text\"&gt;Talks&lt;/p&gt;\nauthor: \"\"\ncomments: false\nlisting:\n  contents: \"index.qmd\"\n  sort: \"date desc\"\n  type: \"default\"\n  date-format: \"iso\"\n  fields: [image, date, title, subtitle, author, summary, description, slides, extsite, repo]  \n  page-size: 6\n  template: talk-listing.ejs\npage-layout: full\ntitle-block-banner: true\nformat:\n  html:\n    css: talks.css\nThe listing section specifies which files should be included on the listings page: anything titled index.qmd in the folder and subfolders will get a listing. Mine are sorted descending by date, with six items per page, and I specify the layout of the page with the talk-listing.ejs file.\n\nListing templates\nControlling the look of my listing pages was the hardest part of the entire process! I don’t love the look of a standard listing page, but it was tough figuring out how to make changes. Quarto has some documentation on creating custom listings pages, but you need to create an EJS template, which I wasn’t familiar with. Per the documentation: EJS templates allow you to generate HTML using plain javascript, making it easy to loop through items and output their values in your custom HTML.\nNormally, I would start with the code of the standard/default option and tinker around until I figured out how to change the elements I wanted. But the Quarto default templates were more complex than I needed and didn’t serve as a great starting point. So I went to option number two: adapt the work of people who know more than I do. I looked at lots of Quarto websites—specifically, lots of Quarto listings pages—in an attempt to find something custom that I liked well enough to use as a starting point. I liked the publications page of the AffCom lab at KU as well as the talks page on Andrew Heiss’s site. Looking at those templates (here and here) was enough to get me started.\nThe template for my blog is pretty simple with the title of the post, the publication date floated to the right, the categories, and the summary. The template for my talks page is similar but also includes buttons for any talk-related links I have like slides, a website, or a repo. The template files pull out the correct elements in the appropriate order from the YAML options of the index.qmd files, and then my .css pages create the style that I want for those listings.\n\n\nMetadata files\nIncluding a _metadata.yml file in both my blog and talks folder is useful to specify YAML options that should apply to all of the index.qmd files within those folders. This way I don’t have to, for example, add my name as the author in each file or specify my css file or customize my table of contents each time I write a blog post.\n# Date format\ndate-format: \"YYYY-MM-DD\"\n\n# Enable banner style title blocks\ntitle-block-banner: \"#1F2041\"\n\n# Add blog-post class to the banner header area\nformat: \n  html:\n    css: blog.css\n    knitr:\n      opts_chunk:\n        dev: png\n        dev.args:\n          bg: transparent\n\n# Default for table of contents\ntoc: true\ntoc-title: CONTENTS\ntoc-location: left\n\nfreeze: true\n\n# Default author\nauthor:\n  - name: Meghan Hall\n\n\nChanging metadata labels\nThis was one of those little things that fell into the “not necessary but would be nice” category. The standard metadata labels for author and date look like so:\n\nThis worked nicely for my blog posts, but for my talk pages, I wanted it to look more like the following:\n\nI almost gave up more than once on this change since it was such a small detail, but I figured it out! And maybe there’s a simpler way to achieve this, but I supplied my own title-block.html file in the talks directory to more clearly specify the output of that title block.\nI created my file based on my own website’s generated html for a blog post (which you can access on Chrome by right-clicking and selecting View Page Source), plus the standard Quarto title-block.html file."
  },
  {
    "objectID": "blog/2024-02-11-moving-to-quarto/index.html#static-files",
    "href": "blog/2024-02-11-moving-to-quarto/index.html#static-files",
    "title": "Moving My Website from blogdown to Quarto",
    "section": "Static files",
    "text": "Static files\nMy website has blog posts as well as landing pages for all of the talks I’ve given (the blog and talks folder, respectively, as described above), but it also needs to hold the various slide decks for all of those talks. When using blogdown, it was easy to drop all of that content into a static folder that you could link to from the root of your website.\nBut where should those files live within the Quarto structure? Netlify builds your Quarto site from the _site directory, but that directory rebuilds when you rerender your site. Google came to the rescue, and I followed Garrick Aden-Buie’s advice to write a little post-render script.\nThe post-render section of my _quarto.yml code instructs Quarto to copy everything in my static folder over to the _site folder after everything has finished rendering.\nproject:\n  type: website\n  render:\n    - \"blog/**/*.qmd\"\n    - \"talks/**/*.qmd\"\n    - \"index.qmd\"\n    - \"blog/index.qmd\"\n    - \"talks/index.qmd\"\n    - \"404.qmd\"\n  post-render:\n    - \"cp -r static/. _site/\"\nThe period after static/ is important because I want those files (most of which are contained in a slides directory) to be copied directly over to the root of my website in the _site folder without the static folder itself. That way the links are formatted like slides/rstudioconf, not static/slides/rstudioconf."
  },
  {
    "objectID": "blog/2024-02-11-moving-to-quarto/index.html#redirects",
    "href": "blog/2024-02-11-moving-to-quarto/index.html#redirects",
    "title": "Moving My Website from blogdown to Quarto",
    "section": "Redirects",
    "text": "Redirects\nWhen migrating your website over to Quarto, chances are some of your links might change. For example, on my old website a sample link for a talk might be /talk/rladiesparis/, but now I’ve changed it to /talks/2022-09-08-rladies-paris/. I have those old links floating around on Twitter and on other websites, so I’d like to make sure that they still work and point to the new links. Enter: redirects.\nIf you use Netlify (which I highly recommed, as the Quarto/GitHub/Netlify integration is so seamless), you can supply a redirects file that lists all the old urls and their new locations. Netlify builds from Quarto’s _site directory, so just put the _redirects file (here’s mine) in your _site folder, right? I tried that first, but it doesn’t work because the _site directory rebuilds when you rerender your site. Instead, I moved my file into the static folder, which gets copied over to _site as described above.\nMy use case here for redirects is pretty simple: I had a small number of urls to switch, and I don’t expect to frequently add to that list. If your situation is more complicated and/or you want that _redirects file to automatically generate shorter urls when you create new pages, there are options described here and here."
  },
  {
    "objectID": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html",
    "href": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html",
    "title": "Tips for Custom Parameterized PDFs in Quarto",
    "section": "",
    "text": "Creating parameterized documents (i.e., documents with outputs—text, plots, tables, etc—that change based on the values of the parameters that you select) is such a common task for data analysts, and it’s a task that can be made much easier with Quarto. (Quarto is a new open-source technical publishing system from RStudio. It is similar in many ways to R Markdown, except that it doesn’t require R, supports more languages, and combines the functionality of many R Markdown packages, e.g., xaringan, bookdown.) Those documents can take many output forms, but this post focuses specifically on PDF reports. For the example report I’m walking through here, I used data from the wonderful palmerpenguins data package because who doesn’t like 🐧?!\nThe full PDF is available here, and below is a screenshot, along with some annotations as to what elements in the report change automatically based on our parameters.\n\nToday’s focus is mostly on the Quarto elements: how to customize the look of our parameterized PDF (like those colors and fonts!) and how to render it easily for the parameter values that I want. I won’t go into too much detail about how I incorporated the parameters into the R code itself, but the full .qmd file is here and I’m happy to answer any questions about those elements! (Throughout, I am coming from the perspective of using R and RStudio, though many of the customization elements will still apply if you are using a different coding language and/or IDE.)"
  },
  {
    "objectID": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#parameterized-reports-in-r-and-quarto",
    "href": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#parameterized-reports-in-r-and-quarto",
    "title": "Tips for Custom Parameterized PDFs in Quarto",
    "section": "",
    "text": "Creating parameterized documents (i.e., documents with outputs—text, plots, tables, etc—that change based on the values of the parameters that you select) is such a common task for data analysts, and it’s a task that can be made much easier with Quarto. (Quarto is a new open-source technical publishing system from RStudio. It is similar in many ways to R Markdown, except that it doesn’t require R, supports more languages, and combines the functionality of many R Markdown packages, e.g., xaringan, bookdown.) Those documents can take many output forms, but this post focuses specifically on PDF reports. For the example report I’m walking through here, I used data from the wonderful palmerpenguins data package because who doesn’t like 🐧?!\nThe full PDF is available here, and below is a screenshot, along with some annotations as to what elements in the report change automatically based on our parameters.\n\nToday’s focus is mostly on the Quarto elements: how to customize the look of our parameterized PDF (like those colors and fonts!) and how to render it easily for the parameter values that I want. I won’t go into too much detail about how I incorporated the parameters into the R code itself, but the full .qmd file is here and I’m happy to answer any questions about those elements! (Throughout, I am coming from the perspective of using R and RStudio, though many of the customization elements will still apply if you are using a different coding language and/or IDE.)"
  },
  {
    "objectID": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#yes-this-means-latex",
    "href": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#yes-this-means-latex",
    "title": "Tips for Custom Parameterized PDFs in Quarto",
    "section": "Yes, this means LaTeX",
    "text": "Yes, this means LaTeX\nSimilarly to how Quarto uses reveal.js to make HTML presentations, Quarto uses LaTeX to make PDF documents. Part of the beauty of Quarto is that you don’t need to learn LaTeX—you can create great PDFs without it. But just like I enjoy learning more about CSS to customize my Quarto HTML products, I enjoy learning more about LaTex to customize my PDF products!\nStandard disclaimer: I am not a LaTeX expert! The tips I offer below are certainly not the only solutions, and depending on your particular use cases they may not be the best solutions. But I think they are useful for someone getting their feet wet with LaTeX and learning how to use LaTeX elements to customize the look of their Quarto PDFs.\nLet’s review how things work structurally: Quarto documents start with a YAML header. Below is the YAML header for this PDF.\n---\nformat: \n  pdf: \n    mainfont: \"Avenir\"\n    sansfont: \"Avenir\"\n    geometry:\n      - top=0.75in\n      - right=0.75in\n      - bottom=0.75in\n      - left=0.75in\n      - heightrounded\n    number-sections: true\n    include-in-header: \n      - \"penguin-header.tex\"\n    toc: false\neditor: source\nexecute:\n  warning: false\n  echo: false\nparams:\n  species: 'Adelie'\n  year: 2009\n---\nYou’ll see that the include-in-header option references a penguin-header.tex file. You can include raw LaTeX directly in the body of your Quarto document, which we’ll incorporate below, and/or you can reference LaTeX files (that’s the .tex extension) that can be executed before your Quarto document (as part of what’s known as the LaTeX preamble). Having a file like this is handy whether you want to customize an entire header page (resources on that available below) or to specify a few LaTeX packages and commands before you get into your Quarto document.\nThis is the content of our penguin-header.tex file:\n\\let\\paragraph\\oldparagraph\n\\let\\subparagraph\\oldsubparagraph\n\n\\usepackage{xcolor}\n\\usepackage{titlesec}\n\\usepackage[parfill=0pt]{parskip}\n\\usepackage{fontspec}\n\n\\setsansfont{Prompt}[\n    Path=Prompt/,\n    Scale=0.9,\n    Extension = .ttf,\n    UprightFont=*-Regular,\n    BoldFont=*-Bold,\n    ItalicFont=*-Italic,\n    ]\n\n\\definecolor{Chinstrap}{HTML}{C25BCC}\n\\definecolor{Gentoo}{HTML}{047075}\n\\definecolor{Adelie}{HTML}{FF6600}\n\n\\titleformat{\\section}\n  {\\sffamily\\Large\\bfseries}{\\thesection}{1em}{}[{\\titlerule[0.8pt]}]\nYou specify LaTeX packages like \\usepackage{xcolor}. Here, we’re adding a package to help us define custom colors, a package for custom fonts, and a package to help make nifty section headers."
  },
  {
    "objectID": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#custom-fonts",
    "href": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#custom-fonts",
    "title": "Tips for Custom Parameterized PDFs in Quarto",
    "section": "Custom fonts",
    "text": "Custom fonts\nIf you want to use a system font in your PDF (meaning it’s already on your computer), you can adjust the values of the mainfont, sansfont and monofont options in the header YAML.\n---\nformat: \n  pdf: \n    mainfont: \"Avenir\"\n    sansfont: \"Avenir\"\nIf you would rather use a Google font, you can download it from the site and save the files into the same directory as your Quarto document. For the example below, I downloaded the Prompt font and added the following code to my penguin-header.tex file. This is setting my sans serif font to Prompt.\n\\setsansfont{Prompt}[\n    Path=Prompt/,\n    Scale=0.9,\n    Extension = .ttf,\n    UprightFont=*-Regular,\n    BoldFont=*-Bold,\n    ItalicFont=*-Italic,\n    ]\nAnd then the command \\sffamily is the first line (below the YAML) in my Quarto document and states that I want to use the font I specified in the penguin-header.tex file. This is an example of including raw LaTeX in the body of your document."
  },
  {
    "objectID": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#custom-colors",
    "href": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#custom-colors",
    "title": "Tips for Custom Parameterized PDFs in Quarto",
    "section": "Custom colors",
    "text": "Custom colors\nUsing a LaTeX package like xcolor allows us to define custom colors like so: \\definecolor{Chinstrap}{HTML}{C25BCC}. The penguin-header.tex file defines a color for each penguin species—Chinstrap, Adelie, and Gentoo—that we can use to render different colors depending on which parameter value of species we select.\nBut how to connect the custom-defined color to our parameter so that the right one is used?\nIn an R code block, you can refer to your report parameters with params$species. If you want to reference these parameters in the Quarto text, enclose like so: `r params$species`. And what’s so neat about Quarto is that you can use this inline code as part of the LaTeX code. So the following command will set the text color to the custom color we defined based on the species determined via the parameter: \\color{`r params$species`}. If our parameter selects the Adelie species, then that piece of code will evaluate as \\color{Adelie} and our orange-y #FF6600 will be used. If you want to set the color back to black, use \\color{black}."
  },
  {
    "objectID": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#rendering-parameterized-reports",
    "href": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#rendering-parameterized-reports",
    "title": "Tips for Custom Parameterized PDFs in Quarto",
    "section": "Rendering parameterized reports",
    "text": "Rendering parameterized reports\nThere are three ways to render this type of parameterized report using Quarto. In increasing order of complexity (and usefulness!):\nFirst. Manually change the parameter values in the header YAML of your .qmd file and hit Render in RStudio. This will create a PDF file in the same directory as your .qmd file with the same name as your .qmd file but with a .pdf extension. This is great for one-offs or for testing!\nSecond. If you prefer to work in the terminal, you can use the following command: quarto render penguins.qmd -P species:'Adelie' -P year:'2009'\nThird. For this palmerpenguins data, we have potentially nine separate PDF reports to create: three years for three species. That’s a little annoying to do via the first two methods, and you can imagine how really annoying it could get if you had even more parameter combinations. Thankfully, we can leverage the quarto::quarto_render() function in R to automatically create a report for each parameter combination we want and name that file in a way that we specify.\nrunpdfs &lt;- function(species, year) {\n  quarto::quarto_render(\n    \"penguins.qmd\",\n    output_format = \"pdf\",\n    execute_params = list(species = species, year = year),\n    output_file = glue::glue(\"{species} {year}.pdf\")\n  )\n}\n\npurrr::map2(unique(penguins$species), unique(penguins$year), \n            runpdfs)\nThe first chunk of code defines a new function called runpdfs that is based on the quarto::quarto_render() function. The two arguments to runpdfs are the two Quarto report parameters we have: species and year. Those arguments are used in the execute_params argument to specify the parameter values to Quarto and in the output_file argument to specify the name of the files.\nThe second chunk of code uses map2() from purrr to iteratively run that new function, runpdfs, for every unique combination of species and year from the penguins data set.\nThe code above will result in nine PDF files. The two available examples are Adelie 2009.pdf and Gentoo 2007.pdf."
  },
  {
    "objectID": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#resources-for-learning-more",
    "href": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#resources-for-learning-more",
    "title": "Tips for Custom Parameterized PDFs in Quarto",
    "section": "Resources for learning more",
    "text": "Resources for learning more\nThe Quarto documentation for PDFs covers the basics of creating PDFs in Quarto and also lists all the available PDF options. There’s also the list of LaTeX variables for Pandoc.\nFor more specifics about customizing LaTeX within Quarto, I found the title page resources from NMFS Open Science to be incredibly helpful for inspiration and also for learning what the possibilities are.\nOne of my personal favorite methods for learning is reviewing others’ code. The files for this example, penguins.qmd and penguins-header.tex, are available on my GitHub and were designed to showcase some basic LaTeX structure and commands."
  },
  {
    "objectID": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html",
    "href": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html",
    "title": "A Beginner’s Introduction to Mixed Effects Models",
    "section": "",
    "text": "Venturing beyond the safe and comfortable land of a basic linear regression model generally requires a good reason (of which there are many!). I recently ran up against one of those boundaries thanks to the structure of the data that I was working with, and I ended up learning a lot about and eventually building a mixed effects model. I’m really proud of that model, but sadly, it’s confidential and also likely boring if you aren’t in my line of work.\nSo to illustrate some of what I learned about mixed effects models (also sometimes known as multilevel models), I’m going to use a small and silly hockey example. (Silly because this example is small, just for tutorial purposes. An actual model to gain insights on this dependent variable would have more data, selected with more care. Make no conclusions from this!) I’ll use this example to discuss when you might want to use a mixed effects model, what exactly we mean by mixed effects, and how to run this kind of model in R using either lme4 or tidymodels. I’ve also included some of my favorite resources on this topic at the end.\nMy data set for this example has 80 observations and five variables (a sample is shown below). Points per 60, abbreviated as pp60, is our dependent variable: it’s the measure of a player’s total points in a given season per 60 minutes of game play. We also have two independent variables: the player’s position (either D for defenseman or F for forward) and their time on ice (abbreviated as toi), the average minutes they played per game.\n\n\n\n\n\n\n\n\nSeason\nPlayer\nPoints per 60\nPosition\nTime on Ice\n\n\n\n\n2018\nAuston Matthews\n3.47\nF\n18.55\n\n\n2019\nAuston Matthews\n3.27\nF\n20.97\n\n\n2020\nAuston Matthews\n3.53\nF\n21.55\n\n\n2021\nAuston Matthews\n4.23\nF\n20.61\n\n\n2018\nConnor Brown\n1.54\nF\n13.79\n\n\n2019\nConnor Brown\n1.81\nF\n20.11\n\n\n2020\nConnor Brown\n2.06\nF\n18.24\n\n\n2021\nConnor Brown\n1.82\nF\n20.06\n\n\n\n\n\n\n\nOne of the basic assumptions of a linear regression model is that your observations are independent of each other. The glimpse of the data set above should set off immediate alarm bells because these observations are not independent: we have four observations per player (one for each of the previous four seasons), and I wouldn’t be comfortable saying that a player’s offensive performance this year is necessarily fully independent from his performance last year. This is a good example of longitudinal data, where there are repeated observations over time of the same subject, and longitudinal data is a good use case for a mixed effects model."
  },
  {
    "objectID": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#when-are-mixed-effects-appropriate",
    "href": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#when-are-mixed-effects-appropriate",
    "title": "A Beginner’s Introduction to Mixed Effects Models",
    "section": "",
    "text": "Venturing beyond the safe and comfortable land of a basic linear regression model generally requires a good reason (of which there are many!). I recently ran up against one of those boundaries thanks to the structure of the data that I was working with, and I ended up learning a lot about and eventually building a mixed effects model. I’m really proud of that model, but sadly, it’s confidential and also likely boring if you aren’t in my line of work.\nSo to illustrate some of what I learned about mixed effects models (also sometimes known as multilevel models), I’m going to use a small and silly hockey example. (Silly because this example is small, just for tutorial purposes. An actual model to gain insights on this dependent variable would have more data, selected with more care. Make no conclusions from this!) I’ll use this example to discuss when you might want to use a mixed effects model, what exactly we mean by mixed effects, and how to run this kind of model in R using either lme4 or tidymodels. I’ve also included some of my favorite resources on this topic at the end.\nMy data set for this example has 80 observations and five variables (a sample is shown below). Points per 60, abbreviated as pp60, is our dependent variable: it’s the measure of a player’s total points in a given season per 60 minutes of game play. We also have two independent variables: the player’s position (either D for defenseman or F for forward) and their time on ice (abbreviated as toi), the average minutes they played per game.\n\n\n\n\n\n\n\n\nSeason\nPlayer\nPoints per 60\nPosition\nTime on Ice\n\n\n\n\n2018\nAuston Matthews\n3.47\nF\n18.55\n\n\n2019\nAuston Matthews\n3.27\nF\n20.97\n\n\n2020\nAuston Matthews\n3.53\nF\n21.55\n\n\n2021\nAuston Matthews\n4.23\nF\n20.61\n\n\n2018\nConnor Brown\n1.54\nF\n13.79\n\n\n2019\nConnor Brown\n1.81\nF\n20.11\n\n\n2020\nConnor Brown\n2.06\nF\n18.24\n\n\n2021\nConnor Brown\n1.82\nF\n20.06\n\n\n\n\n\n\n\nOne of the basic assumptions of a linear regression model is that your observations are independent of each other. The glimpse of the data set above should set off immediate alarm bells because these observations are not independent: we have four observations per player (one for each of the previous four seasons), and I wouldn’t be comfortable saying that a player’s offensive performance this year is necessarily fully independent from his performance last year. This is a good example of longitudinal data, where there are repeated observations over time of the same subject, and longitudinal data is a good use case for a mixed effects model."
  },
  {
    "objectID": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#what-are-mixed-effects-exactly",
    "href": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#what-are-mixed-effects-exactly",
    "title": "A Beginner’s Introduction to Mixed Effects Models",
    "section": "What are mixed effects, exactly?",
    "text": "What are mixed effects, exactly?\nBefore we continue through this example, what exactly are these mixed effects that we’re referring to here? A mixed effects model contains both fixed and random effects. Fixed effects are the same as what you’re used to in a standard linear regression model: they’re exploratory/independent variables that we assume have some sort of effect on the response/dependent variable. These are often the variables that we’re interested in making conclusions about.\nWhere a mixed effects model differentiates itself is through the inclusion of random effects, which allow us to skirt past that independence assumption. Random effects are simply categorical variables that we’re interested in controlling for—even if we aren’t particularly interested in quantifying their impacts or knowing more about the specific levels—because we know they’re likely influencing the patterns that might emerge. This variable often has many possible levels, and there’s likely just a sample of the population in your data. In the example we’re going through today, the random effect is the player.\nIn this example, given the repeated observations, I want to allow for the possibility that there is some type of individual player effect that is not measured by my other (independent, fixed) variables. I’m not particularly interested in quantifying the effect of “being Player X” on scoring rate, I just want to address that the effect exists. (In contrast, I am interested in the effect of position and the effect of time on ice, our two fixed variables.)\nFor example, Connor Brown and Auston Matthews are both forwards who averaged around 20 minutes per game last year. By the two fixed effects included in this very small model, they would produce a very similar response variable. But as anyone who watches hockey can tell you, Auston Matthews is much more offensively talented than Connor Brown, and in this model, the random effect of the player will address some of that. Random effects are useful for capturing the impact of persistent characteristics that might not be observable elsewhere in the explanatory data. In this example, it can be thought of as a proxy for player “talent” in a way.\nIf those random effects are correlated with variables of interest, leaving them out could lead to biased fixed effects. Including them can help more reliably isolate the influence of the fixed effects of interest and more accurately model a clustered system."
  },
  {
    "objectID": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#prepping-the-data",
    "href": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#prepping-the-data",
    "title": "A Beginner’s Introduction to Mixed Effects Models",
    "section": "Prepping the data",
    "text": "Prepping the data\nWe can plot this data to understand it a bit better. We have four observations each for 20 separate players, 10 forwards and 10 defensemen, and we can see that in general, forwards tend to score more points and defensemen tend to play more minutes.\n\n\n\n\n\n\n\n\n\nBecause this data is so small, I don’t need to do much to get it in better shape for modeling—I’m just going to make sure the position variable is a factor. This df data frame will be the basis of the model.\n\ndf &lt;- data_raw %&gt;% \n  mutate(position = factor(position))"
  },
  {
    "objectID": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#running-the-model-with-lme4",
    "href": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#running-the-model-with-lme4",
    "title": "A Beginner’s Introduction to Mixed Effects Models",
    "section": "Running the model with lme4",
    "text": "Running the model with lme4\nThe lme4 package in R was built for mixed effects modeling (more resources for this package are listed below). If you’ve used the lm function to build models in R, the model formulas will likely look familiar.\nThe simplest version of a mixed effects model uses random intercepts. In this case, the random effect allows each group (or player, in this case) to have a different intercept in the model, while the effects of the other variables remain fixed. The code below creates the m1 model with pp60 as the response variable, position and toi as the fixed effects, and (1 | player) as the random effect for the intercept. The | is just a special interaction to make sure that the model has different effects for each level of the grouping factor (in this case, for each player).\n\nm1 &lt;- lmer(pp60 ~ position + toi + (1 | player), \n           data = df)\n\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: pp60 ~ position + toi + (1 | player)\n   Data: df\n\nREML criterion at convergence: 115.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.1327 -0.4468 -0.1237  0.3489  3.0031 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n player   (Intercept) 0.4126   0.6423  \n Residual             0.1192   0.3452  \nNumber of obs: 80, groups:  player, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) -0.16546    0.58054  -0.285\npositionF    1.48931    0.31861   4.674\ntoi          0.06254    0.02531   2.471\n\nCorrelation of Fixed Effects:\n          (Intr) postnF\npositionF -0.573       \ntoi       -0.932  0.358\n\n\nsummary(m1), as shown above, will produce a familiar-looking results table where you can examine the fixed effects. Being a forward is associated with a higher scoring rate, which makes sense. There’s also some information about the random effects, mainly how much variance we have among the levels and how much residual variance there is. We can tell in this example that the player is explaining a lot of the variance left over from the fixed effects.\nTo view the individual random effects, use the ranef function from the lme4 package. We can see from this sample that players known to be offensive stars, like Auston Matthews and Connor McDavid, have understandably high values.\n\nranef(m1)\n\n\n\n\n\n\n\n\n\ngrpvar\nterm\ngrp\ncondval\ncondsd\n\n\n\n\nplayer\n(Intercept)\nAndy Greene\n-0.3922248\n0.1666845\n\n\nplayer\n(Intercept)\nAuston Matthews\n0.9550296\n0.1666845\n\n\nplayer\n(Intercept)\nBrett Pesce\n-0.1144402\n0.1666845\n\n\nplayer\n(Intercept)\nCalle Jarnkrok\n-0.5214549\n0.1666845\n\n\nplayer\n(Intercept)\nConnor Brown\n-0.6018354\n0.1666845\n\n\nplayer\n(Intercept)\nConnor McDavid\n1.5091598\n0.1666845\n\n\nplayer\n(Intercept)\nEsa Lindell\n-0.3970716\n0.1666845\n\n\nplayer\n(Intercept)\nGarnet Hathaway\n-0.5542113\n0.1666845\n\n\nplayer\n(Intercept)\nJason Spezza\n0.1923681\n0.1666845\n\n\nplayer\n(Intercept)\nNeal Pionk\n0.1609786\n0.1666845\n\n\n\n\n\n\n\nJust like with other models, this model can be predicted with predict(m1). If you don’t want random effects to be included in your prediction, use predict(m1, re.form = NA)."
  },
  {
    "objectID": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#running-the-model-with-tidymodels",
    "href": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#running-the-model-with-tidymodels",
    "title": "A Beginner’s Introduction to Mixed Effects Models",
    "section": "Running the model with tidymodels",
    "text": "Running the model with tidymodels\nIf you’re familiar with and prefer using the tidymodels framework, you can use the lmer engine to run this model. You’ll need both the tidymodels and multilevelmod packages installed.\nSet the specification with set_engine(\"lmer\") and then fit like you usually would with tidymodels.\n\nlmer_spec &lt;- \n  linear_reg() %&gt;% \n  set_engine(\"lmer\")\n\nusing_tidymodels &lt;- \n  lmer_spec %&gt;% \n  fit(pp60 ~ position + toi + (1 | player),\n      data = df)\n\nusing_tidymodels\n\nparsnip model object\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: pp60 ~ position + toi + (1 | player)\n   Data: data\nREML criterion at convergence: 115.8825\nRandom effects:\n Groups   Name        Std.Dev.\n player   (Intercept) 0.6423  \n Residual             0.3452  \nNumber of obs: 80, groups:  player, 20\nFixed Effects:\n(Intercept)    positionF          toi  \n   -0.16546      1.48931      0.06254  \n\n\nThe code above results in the same model and the same fixed effects we saw before. Other elements of the tidymodels workflows are available; more details are here.\nTo get the random effects of a given model built this way, run lme4::ranef(using_tidymodels$fit)."
  },
  {
    "objectID": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#more-applications",
    "href": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#more-applications",
    "title": "A Beginner’s Introduction to Mixed Effects Models",
    "section": "More applications",
    "text": "More applications\nThis particular example is focused on longitudinal data, but mixed effects models are useful whenever there’s any kind of clustering effect where the group is likely affecting the outcome. These effects can even be nested (e.g., studying test scores within schools that are within districts). Generally if the data has some sort of nested/hierarchical structure, that’s when you’ll start to see the “multilevel model” terminology, although the principles are the same.\nMore details on those multilevel models are available at the links below, but the model formulas would be very similar. If you have nested groups, a random effect structured like (1 | v1 / v2) would mean intercepts varying among v1 and v2 within v1.\nIf it’s not apparently obvious, how can you tell if your data is clustered? You can start by plotting it! The data behind our example is plotted below. Each color in the plot is a different player, and just by a little manual inspection, we can see that the data appears to be clustered.\n\n\n\n\n\n\n\n\n\nYou can also investigate clustering by creating a simple null model, which only has the intercept and the random effect. The summ function within the jtools package will helpfully provide the ICC, or intraclass correlation coefficient, which can help identify clustering. This data has a value of 0.89, which is quite high and good evidence that a mixed effects model is necessary here.\n\nm0 &lt;- lmer(pp60 ~ 1 + (1 | player), \n           data = df)\njtools::summ(m0)\n\n\n\n\n\nObservations\n80\n\n\nDependent variable\npp60\n\n\nType\nMixed effects linear regression\n\n\n\n\n\n\n\n\nAIC\n130.78\n\n\nBIC\n137.93\n\n\nPseudo-R² (fixed effects)\n0.00\n\n\nPseudo-R² (total)\n0.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\nEst.\nS.E.\nt val.\n\n\n\n\n(Intercept)\n1.78\n0.22\n8.10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Effects\n\n\n\nGroup\nParameter\nStd. Dev.\n\n\n\n\nplayer\n(Intercept)\n0.97\n\n\nResidual\n\n0.34\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrouping Variables\n\n\n\nGroup\n# groups\nICC\n\n\n\n\nplayer\n20\n0.89"
  },
  {
    "objectID": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#varying-slopes",
    "href": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#varying-slopes",
    "title": "A Beginner’s Introduction to Mixed Effects Models",
    "section": "Varying slopes",
    "text": "Varying slopes\nThe simple example above used a varying intercept, where each player (our random effect) had an adjustment to the overall intercept. But more complexity can be added to a mixed effects model by also incorporating random slopes, which would allow the effect of the selected variable(s) to vary across subjects.\nIf, for example, we wanted to incorporate an age variable into our model and we wanted the influence of that variable to vary by player, it would be incorporated like so, before the | player:\n\nm_slope &lt;- lmer(pp60 ~ position + toi + (1 + age | player), \n                data = df)\n\nThose resulting random effects would be available just like with the intercepts, by using ranef(m_slope)."
  },
  {
    "objectID": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#resources",
    "href": "blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/index.html#resources",
    "title": "A Beginner’s Introduction to Mixed Effects Models",
    "section": "Resources",
    "text": "Resources\nThis was intended to be a beginner’s guide to mixed effects modeling with a simple example. For more details, both technical and theoretical, I found all of the following resources helpful:\n\nAn interactive visualization: particularly useful for understanding random intercepts vs. slopes\nBeyond multiple linear regression: especially chapters 8 & 9, lots of examples and good interpretation notes\nMixed models with R: great online book\nCh. 9 of Data Analysis in R: useful for theory\nlme4 vignette: pretty technical but helpful formulas in table 2\nIntroduction to mixed effects modeling: useful walkthrough\nIntroduction to linear mixed models: good tutorial\nA video on multilevel modeling with lme4"
  },
  {
    "objectID": "blog/2021-08-21-an-introduction-to-cowplot/index.html",
    "href": "blog/2021-08-21-an-introduction-to-cowplot/index.html",
    "title": "An Introduction to cowplot",
    "section": "",
    "text": "This week’s #TidyTuesday data set is all about lemurs (with data from the Duke Lemur Center), so I figured I would stick with the animal theme and discuss a few functions from Claus Wilke’s cowplot package, which is my personal favorite package for arranging plots made in R.\n(Because everyone always asks: I believe the package name comes from the longhorn mascot of the University of Texas, with which Wilke is affiliated.)\n\nToday’s data\nThe data set can be read in directly from the Tidy Tuesday Github repo as follows:\nlemurs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-08-24/lemur_data.csv')\ncowplot is available on CRAN, so it can be installed if necessary with install.packages(\"cowplot\") and then loaded with the other packages needed for today’s plots. I also loaded a tibble of some common species names, available in the Tidy Tuesday readme, to match with the taxonomic codes.\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(scales)\nlibrary(ggalt)\n\nmeg_theme &lt;- function () { \n  theme_linedraw(base_size=11, base_family=\"Avenir\") %+replace% \n    theme(\n      panel.background  = element_blank(),\n      plot.background = element_rect(fill = \"transparent\", color = NA), \n      legend.background = element_rect(fill = \"transparent\", color = NA),\n      legend.key = element_rect(fill = \"transparent\", color = NA),\n      axis.ticks = element_blank(),\n      panel.grid.major = element_line(color = \"grey90\", size = 0.3), \n      panel.grid.minor = element_blank(),\n      plot.title.position = \"plot\",\n      plot.title = element_text(size = 18, hjust = 0, vjust = 0.5, \n                                margin = margin(b = 0.2, unit = \"cm\")),\n      plot.subtitle = element_text(size = 10, hjust = 0, vjust = 0.5, \n                                   margin = margin(b = 0.4, unit = \"cm\")),\n      plot.caption = element_text(size = 7, hjust = 1, face = \"italic\", \n                                  margin = margin(t = 0.1, unit = \"cm\")),\n      axis.text.x = element_text(size = 13),\n      axis.text.y = element_text(size = 13)\n    )\n}\n\nspecies &lt;- tribble(\n  ~\"taxon\", ~\"species\",\n  #--|--|----\n  \"VRUB\",   \"Red ruffed lemur\",\n  \"PCOQ\",   \"Coquerel’s sifaka\",\n  \"VVV\",  \"Black-and-white ruffed lemur\",\n  \"EMAC\",   \"Black lemur\",\n  \"ECOL\",   \"Collared brown lemur\",\n  \"LCAT\",   \"Ring-tailed lemur\",\n  \"ERUF\",   \"Red-fronted brown lemur\",\n  \"EUL\",  \"Eulemur hybrid\",\n  \"ECOR\",   \"Crowned lemur\",\n  \"EFLA\",   \"Blue-eyed black lemur\",\n  \"EMON\",   \"Mongoose lemur\",\n  \"MMUR\",   \"Gray mouse lemur\",\n  \"MZAZ\",  \"Northern giant mouse lemur\",\n  \"NCOU\",   \"Slow loris\",\n  \"OGG\",    \"Northern greater galago\",\n  \"CMED\",   \"Fat-tailed dwarf lemur\"\n)\n\n\nInset images\n\nMy lemur knowledge is, admittedly, fairly limited. I found in the data that in many lemur species, females are actually heavier than males, and a quick Wikipedia trip informed me that females are often the dominant sex in the lemur social structure. These sample plots will examine the weight difference by sex and species.\nTo create the base plot above, I did some data manipulation to find the difference between the average weight, by species, for males and females. That new data frame, size, was the basis of a dumbbell plot, which I assigned to size_plot. The ggalt package has a very convenient geom_dumbbell function for creating dumbbell plots.\nsize &lt;- lemurs %&gt;% \n  # filter to non-pregnant adult records only\n  filter(preg_status == \"NP\" & sex != \"ND\" & age_category == \"adult\") %&gt;% \n  # group by the individual lemur and filter to only the highest recorded weight\n  group_by(dlc_id) %&gt;% \n  filter(max(weight_g) == weight_g) %&gt;% \n  # group by species and sex to find the average\n  group_by(taxon, sex) %&gt;% \n  summarize(avg = mean(weight_g),\n            total = n_distinct(dlc_id)) %&gt;% \n  mutate(sex = ifelse(sex == \"F\", \"Female\", \"Male\")) %&gt;% \n  # add an extra variable n to count the total number per species and filter\n  add_count(wt = total) %&gt;%\n  filter(n &gt; 40) %&gt;% \n  select(-c(total)) %&gt;% \n  # pivot so that there is one row per species and calculate the M/F difference\n  pivot_wider(names_from = sex, values_from = avg) %&gt;% \n  mutate(difference = Female - Male,\n         perc = (Female - Male) / Male) %&gt;% \n  # join in the common species name for labeling purposes\n  left_join(species, by = \"taxon\")\n\nsize_plot &lt;- size %&gt;% \n  ggplot(aes(x = Male, xend = Female, y = reorder(species, Female), \n       group = species)) +\n  geom_dumbbell(size = 2.5, color = \"#dddddd\",\n                colour_x = \"#e5ded6\", colour_xend = \"#6a414a\",\n                dot_guide = TRUE, dot_guide_size = 0.25) +\n  labs(y = NULL,\n       x = \"Weight in grams\",\n       title = \"Females are heavier than males in some lemur species\",\n       subtitle = \"Average of maximum adult non-pregnant weight recorded, among species with 40+ subjects\",\n       caption = \"#TidyTuesday, data & photo from Duke Lemur Center, plot by @MeghanMHall\") +\n  scale_x_continuous(limits = c(0, 5200)) +\n  annotate(\"text\", x = 5170, y = 15, label = \"Females\", \n       family = \"Avenir\", size = 3, color = \"#6a414a\") +\n  meg_theme() +\n  theme(panel.grid.major.y = element_blank(),\n        axis.text.y = element_text(size = 8, hjust = 1),\n        plot.caption = element_text(hjust = 0),\n        plot.caption.position = \"plot\")\nAnd now for cowplot. The draw_image function allows you to easily add an image to your plots, which can be convenient for sharing a hex image, a university/company logo, or in this case, a cute picture of lemurs. All you need is a url for the image, and then function arguments like x, y, and width allow you to resize the image and place it on the base plot created previously.\nThe code below starts with ggdraw and adds functions with +, just like in ggplot2 code you’re probably used to. draw_plot adds the base plot, size_plot, created in the previous section, and draw_image allows you to add and place the image. draw_plot_label adds a label to the plot, which can be useful for a mini caption or a notation like “A” or “1” if you want to refer to the inset image in the text of your paper or whatever document (hopefully created with R Markdown!). This label can be placed with the x and y arguments and customized with standard family and size arguments to control the font and size.\nggdraw() +\n  draw_plot(size_plot) +\n  draw_image(\"https://lemur.duke.edu/wordpress/wp-content/uploads/2018/06/infants-judith-and-mae-1024x578.jpg\",\n             x = 0.98, y = 0.35, hjust = 1, vjust = 1, \n             halign = 1, valign = 1, width = 0.4) +\n  draw_plot_label(\"Red ruffed lemurs\", x = 0.7,\n                   y = 0.38, size = 10, family = \"Avenir\")\n\n\nInset plots\n\nJust like you can use cowplot to add an inset image, you can add an inset plot. In the code below, I’m creating a separate data frame, finding the average weight for all species combined by sex, and then creating a very simple bar chart.\nsize_all &lt;- lemurs %&gt;% \n  # filter to non-pregnant adult records only, among species we used previously\n  filter(preg_status == \"NP\" & sex != \"ND\" & age_category == \"adult\" \n      & taxon %in% size$taxon) %&gt;% \n  # group by the individual lemur and filter to only the highest recorded weight\n  group_by(dlc_id) %&gt;% \n  filter(max(weight_g) == weight_g) %&gt;% \n  mutate(sex = ifelse(sex == \"F\", \"Female\", \"Male\"))\n\navg_plot &lt;- size_all %&gt;% \n  # group by sex and find average weight\n  group_by(sex) %&gt;% \n  summarize(avg = mean(weight_g)) %&gt;% \n  ggplot(aes(x = sex, y = avg, fill = sex)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"#6a414a\", \"#e5ded6\"), guide = FALSE) +\n  labs(x = NULL, \n       y = \"Average weight in grams\") +\n  scale_y_continuous(limits = c(0,2050)) +\n  geom_text(aes(label = round(avg, 0)), vjust = -0.5, family = \"Avenir\") +\n  meg_theme() +\n  theme(panel.grid.major.x = element_blank(),\n        axis.text.x = element_text(size = 10),\n        axis.text.y = element_text(size = 10))\nThe code below to create an inset plot is very similar to the code in the previous section to create an inset image. ggdraw starts with size_plot, the original plot created in the first section, and then draw_plot adds this new bar chart, avg_plot, onto the base plot, just like we added the image previously. The x, y, width, and height arguments control the size and the placement of the inset plot.\nI’ve also added draw_plot_label to show how you can add multiple labels, one for each plot, which would allow for easy reference in text. Here, thanks to vectorized values within the label, x, and y arguments, the label “1.” is placed to indicate the base plot, while “2.” is placed above the inset plot.\nggdraw(size_plot) +\n  draw_plot(avg_plot, x = .54, y = .08, width = .45, height = .4) +\n  draw_plot_label(label = c(\"1.\", \"2.\"), x = c(0, 0.94),\n                  y = c(0.9, 0.51), size = 12, family = \"Avenir\")\n\n\nArranging plots in a grid\n\nKnowing how to use inset images and inset plots is handy, but easily arranging plots side-by-side is likely a more common use case. And the plot_grid function makes it easy to combine two plots or in more complex cases, add multiple elements and grids together.\nThe code below creates a bar graph, first_plot, that plots, per species, the female weight difference over males. The bars are colored by whether females or males are on average heavier in that species. second_plot is a jitter plot showing the individual weights by sex in each species.\nfirst_plot &lt;- size %&gt;% \n  # create a new variable with the n to use as a label\n  mutate(label = paste0(species, \"\\nn = \", n)) %&gt;% \n  ggplot(aes(x = perc, y = reorder(label, perc), fill = perc &gt; 0)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"#e5ded6\", \"#6a414a\"), guide = FALSE) +\n  labs(y = NULL,\n       x = \"Average female weight difference over male\",\n       title = \"Females are heavier than males in some lemur species\",\n       subtitle = \"Maximum adult non-pregnant weights recorded, among species with 40+ subjects\",\n       caption = \"#TidyTuesday, data & photo from Duke Lemur Center, plot by @MeghanMHall\") +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  annotate(\"text\", x = 0.09, y = 10, label = \"Females are\\nheavier\", \n        family = \"Avenir\", size = 4, color = \"#6a414a\") +\n  annotate(\"text\", x = -0.1, y = 3, label = \"Males are\\nheavier\", \n        family = \"Avenir\", size = 4, color = \"#d1c9c0\") +\n  meg_theme() +\n  theme(panel.grid.major.y = element_blank(),\n        axis.text.y = element_text(size = 8, hjust = 1),\n        axis.text.x = element_text(size = 10),\n        plot.caption = element_text(size = 7, hjust = 0, face = \"italic\", \n                                    margin = margin(t = 0.2, unit = \"cm\")),\n        plot.caption.position = \"plot\",\n        plot.subtitle = element_text(size = 10, hjust = 0, vjust = 0.5, \n                                     margin = margin(b = 0.8, unit = \"cm\")))\n\nsecond_plot &lt;- size_all %&gt;% \n  left_join(select(size, perc, taxon, species), by = \"taxon\") %&gt;% \n  ggplot(aes(x = weight_g, y = reorder(species, perc), color = sex)) +\n  geom_jitter(alpha = 0.5) +\n  scale_color_manual(values = c(\"#6a414a\", \"#e5ded6\")) +\n  guides(color = guide_legend(override.aes = list(size = 5, alpha = 1))) +\n  labs(y = NULL,\n       color = NULL,\n       x = \"Weight in grams\") +\n  meg_theme() +\n  theme(panel.grid.major.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.text.x = element_text(size = 10),\n        legend.position = c(0.8, 0.5))\nTo place those plots neatly side-by-side, we can use plot_grid. You can add a label (or two); in this case, average weights and individual weights, which I made room for by adding a bit of extra margin space below the subtitle in the code for first_plot.\nThe rel_widths argument controls the relative size of each plot. Here the plot on the left (first_plot, the one listed first) is larger. align = \"h\" ensures that the axes are lined up appropriately.\nplot_grid(first_plot, second_plot, labels = c(\"average weights\", \n                                              \"individual weights\"), \n          label_size = 9, label_fontfamily = \"Avenir\", label_x = c(0.5, 0.2), \n          label_y = 0.92, align = \"h\", rel_widths = c(1.5, 1))\n\n\nSharing titles and legends\n\nThe example in the previous section, with the left-hand plot keeping its title and the right-hand plot keeping its legend, works nicely. But within cowplot there are also options to a) create a title that spans both plots and b) extract a legend from a single plot to more clearly cover both plots.\nThe first function below, plot_grid, combines first_plot and second_plot just like in the previous section, with two exceptions: I’ve added theme functions to remove the title and the subtitle from first_plot and the legend from second_plot. I assigned the result to plots.\nplots &lt;- plot_grid(first_plot + \n                     theme(plot.title = element_blank(), \n                           plot.subtitle = element_blank()), \n                   second_plot + \n                     theme(legend.position = \"none\"),\n                   align = \"h\", rel_widths = c(1.5, 1))\nThe second function, get_legend, will extract the legend from a particular plot, here second_plot. I’ve used a theme function to indicate that I want a legend in the “top” position, and it’s been assigned to legend.\nlegend &lt;- get_legend(second_plot + \n                       theme(legend.position = \"top\"))\nThe title object (which also includes the subtitle) is created manually with two draw_label functions. I also adjusted the margin a bit.\n\nA good mnemonic device for the margin function is that margins are trouble: listed in the order of top, right, bottom, left.\n\ntitle &lt;- ggdraw() + \n  draw_label(\"Females are heavier than males in some lemur species\",\n             fontfamily = \"Avenir\", size = 18,\n             x = 0, hjust = 0) +\n  draw_label(\"Maximum adult non-pregnant weight recorded, among species with 40+ subjects\",\n             fontfamily = \"Avenir\", size = 10,\n             y = 0.15, x = 0, hjust = 0) +\n  theme(plot.margin = margin(0, 0, 0, 7))\nThe final step is to use plot_grid again to put together the title, the legend, and the plots (which was itself created from plot_grid!). ncol = 1 indicates that the objects should be on top of each other in one column, and rel_heights controls the relative size of each object.\nplot_grid(title, legend, plots, ncol = 1,\n          rel_heights = c(0.1, 0.05, 1))"
  },
  {
    "objectID": "blog/2021-02-23-fogo-penalty-kill/index.html",
    "href": "blog/2021-02-23-fogo-penalty-kill/index.html",
    "title": "The FOGO Trend and the Importance of Faceoffs on the Penalty Kill",
    "section": "",
    "text": "The Baader–Meinhof phenomenon is a fancy term for a simple idea: when you learn about a concept, or hear a new word, then all of a sudden you start seeing it everywhere. It’s usually a form of cognitive bias because your increased attention to this new thing tends to make it seem like it’s occurring at a higher frequency than it actually is.\nThis happened to me earlier this year with FOGO. If you’re unaware, like I was, FOGO stands for “face off, get off.” It’s a popular term in lacrosse, but in hockey it generally refers to a player on the penalty kill whose main job is the faceoff—they take the faceoff and then leave the ice as soon as possible to be replaced by another player. All of a sudden, I started seeing FOGO references everywhere (i.e., on Twitter and in a few articles), so I assumed it might be a new trend.\n\nAre FOGOs real?\nHow could we go about determining whether this FOGO thing is actually in fact a trend? Since the aim of a FOGO is to take the faceoff and then leave the ice, we could see whether the average shift length of the player taking the faceoff is changing over the past few seasons. The graph below shows, for each team per season, the average shift length of the players who took the initial defensive zone faceoff of a 4v5 penalty kill. That is, how long did they stay on the ice after that faceoff during that penalty kill?\n\nIf the FOGO was becoming a trend, we might see this average shift length moving downward, but that doesn’t appear to be the case. (The wider variation seen in the 2020-21 season isn’t too surprising, as it’s still fairly early in the season and the averages haven’t necessarily stabilized yet.)\nWe could also look at this same average shift length, this time grouped by the result of that faceoff. Since after all, the ideal outcome of a FOGO is for the player to win the faceoff and then leave the ice. But, from the graph below, there doesn’t appear to be an emerging trend there, either.\n\nLastly, what if we looked at individual players? That is, who actually takes these faceoffs? (Again, restricting to the initial defensive zone faceoff of a 4v5 penalty kill.) In the graph below, each dot represents a player from one of the last several seasons. The x-axis shows the percent time they spent on their team’s PK (adjusted for games played, so a player missing a chunk of games for an injury, for example, wouldn’t be penalized), and the y-axis shows the percent of these initial defensive zone PK faceoffs they took.\n\nWe would expect to see any FOGOs up in that upper-left corner: players who take a lot of faceoffs on the penalty kill but don’t actually play on the PK very much. But as you can see, there’s only one solitary dot in that region: Jason Spezza of the Toronto Maple Leafs this season, who has taken 67% of these initial defensive zone faceoffs but doesn’t spend that much time on the penalty kill otherwise. No player over the past several seasons is particularly close to that specific combination—the closest is that blue dot a bit below, Claude Giroux of the Philadelphia Flyers who took 48% of those faceoffs in the 2019-20 season but didn’t get much other PK time.\n\n\nWhy this focus on PK faceoffs?\nMuch has been written about faceoffs and their importance—or lack thereof. So what about the specific situation of the defensive zone faceoff that kicks off a 4v5 penalty kill?\nA faceoff “win” can be subjective, so before we look at the potential impacts of a faceoff win, what actually happens after the initial defenzive zone faceoff on a 4v5 penalty kill? In an attempt to answer this question, I watched 113 of these faceoffs from the 2019-20 regular season to examine the relationship between a faceoff win, as per the recorded event in the NHL play-by-play data, and the primary goal of the penalty killing team: clearing the puck out of the offensive zone.\n\nAs shown in the graph above, the relationship was pretty stark in this sample. When the PK team lost that initial faceoff, they were only able to clear the puck on the resulting possession five percent of the time. But when they won that faceoff, their clear rate increased to nearly 63 percent.\nThat relationship is reflected in the graph below, which shows the distribution of shot attempts by the power play team per second, grouped by the result of that initial defensive zone faceoff. Penalty kill teams who lose that initial faceoff—meaning they’re likely stuck in their defensive zone and likely facing the power play’s top unit— see a spike in power play shot attempts for the first portion of the penalty.\n\nThis initial discrepancy sums up to quite a difference in shot attempt rates over the life of the penalty. Shown below are the power play shot attempt rates for the past several seasons, per the result of the initial faceoff from the perspective of the penalty killing team. (To keep in line with the rest of the analysis, this includes 5v4 penalties only that start with a defensive zone faceoff, from the perspective of the penalty killing team.) It’s a significant difference, with a lower shot attempt generation rate associated with a penalty kill faceoff win, and a similar relationship can be seen with the goals rate.\n\n\n\nShould FOGOs be real?\nSo, given the importance of the initial defensive zone faceoff on a 4v5 penalty kill, maybe FOGOs should be a thing! There’s of course the risk of your FOGO losing the faceoff and then being stuck on the ice in the defensive zone for a while, which might not be an ideal circumstance (especially if there’s a reason that player doesn’t play on the penalty kill in the first place.) But the impacts of an initial faceoff win might be enough to give it a shot.\nWho would be good potential FOGO candidates?\n\n\n\nSeason\nPlayer\nTeam\nFaceoff Win\nPK Time\n\n\n\n\n20192020\nBO.HORVAT\nVAN\n56.1%\n8.1%\n\n\n20192020\nCLAUDE.GIROUX\nPHI\n58.9%\n18.3%\n\n\n20192020\nGABRIEL.LANDESKOG\nCOL\n55.4%\n1.4%\n\n\n20192020\nJ.T..MILLER\nVAN\n58.0%\n12.1%\n\n\n20192020\nMATT.DUCHENE\nNSH\n55.8%\n0.8%\n\n\n20192020\nSIDNEY.CROSBY\nPIT\n55.0%\n6.5%\n\n\n20192020\nTYLER.SEGUIN\nDAL\n58.9%\n13.8%\n\n\n20202021\nCLAUDE.GIROUX\nPHI\n55.9%\n16.8%\n\n\n20202021\nDAVID.KREJCI\nBOS\n58.9%\n0.0%\n\n\n20202021\nJOHN.TAVARES\nTOR\n57.8%\n0.7%\n\n\n20202021\nLEON.DRAISAITL\nEDM\n56.5%\n14.5%\n\n\n20202021\nSIDNEY.CROSBY\nPIT\n57.9%\n3.3%\n\n\n20202021\nVINCENT.TROCHECK\nCAR\n55.7%\n18.8%\n\n\n\nShown above are players from this season and the previous one who have a 5v5 faceoff win percentage over 55% and play under 20% of their team’s PK minutes. I’ll keep my eye out for anyone joining Jason Spezza in the vaunted FOGO category.\nPlay-by-play data sourced via the Evolving-Hockey scraper!"
  },
  {
    "objectID": "blog/2020-08-17-shot-locations/index.html",
    "href": "blog/2020-08-17-shot-locations/index.html",
    "title": "NHL Shot Locations by Arena",
    "section": "",
    "text": "This data set includes all regular season NHL games from the 2015 season through the 2019 season, until the pause. “Shots” encompasses all unblocked shot attempts: goals, shots on goal, and missed shots. Blocked shots are excluded because those are recorded at the location of the block, not the location of the shot.\nIn the play-by-play game data made public by the NHL, most events include a location: an X coordinate (measuring from the center line, 0 to 100) and a Y coordinate (measuring from mid-rink, -43 to 43) to represent where on the ice the event took place. These locations are manually recorded, and since humans make mistakes, we would expect to see some random error that would even itself out over time. But by looking at all the shot locations per arena (i.e., all the shots recorded in an arena for a given season, for both the home team and all the away teams), we can investigate to see whether there is systematic error in certain arenas. That is, do certain arenas record shots less accurately?\nWe can look at this by comparing the distributions of shot locations in the entire league to each home arena. For example, when looking at the X coordinates in the graph below, the most popular recorded location for a shot is at 79 feet (for context, the goal line is at 89 feet). League-wide, over this time frame, 2.7% of all shots were recorded at that coordinate. However, the individual arena values range from 2.1% (Toronto) to 3.7% (Pittsburgh).\nYou can see the full distribution below for the X coordinate. The dark line is the league average, while the gray lines represent each individual arena.\n\nTwo initial reactions might be: those differences don’t look that large or maybe there are true characteristics that drive those discrepancies. Perhaps Pittsburgh, for example, takes so many more shots closer to the net that it skews the average shot location for all the shots recorded at that arena.\nHowever, we can test that by looking at the exact same distribution, but this time focused on away teams rather than by home arenas. So instead of being driven by all the shots recorded by the home arena, the individual lines below encompass all the shots recorded while a certain team was the away team.\n\nThere is much less variation than in the first graph, so it’s clear that there are some differences in recorded shot locations among arenas.\nThis question of possible systematic error is not a new one—Gabe Desjardins wrote an article for Arctic Ice Hockey back in 2010 that focused on the shot locations in Madison Square Garden in particular, and the graphs in that piece inspired my approach here. Alan Ryder in 2007 also found systematic bias in shot locations, particularly at MSG, and many others, including Michael Schuckers in 2014, have written about rink effects in the recording of events generally. I hadn’t seen shot location data like this recently and wanted to update it for the last handful of seasons.\nWhy does this matter? Expected goals models that use public data rely heavily on shot location, as it’s an important feature in those models. Being aware of possible systematic bias among arenas can add context to the results.\nIn order to summarize the differences per team over this time period, we can create a measure of comparison by finding the absolute value of the difference between the league average and the team’s value at each coordinate and summing those values. The top and bottom 10 teams (i.e., the teams that had the largest and smallest differences from the average, respectively) are shown below for the X coordinate, which has more variation.\n\n\n\n\n\n\n\nTeam\nMeasure\n\n\n\n\nT.B\n0.341\n\n\nCAR\n0.219\n\n\nNYR\n0.213\n\n\nARI\n0.209\n\n\nBOS\n0.208\n\n\nANA\n0.205\n\n\nPHI\n0.201\n\n\nSTL\n0.185\n\n\nMTL\n0.184\n\n\nTOR\n0.165\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeam\nMeasure\n\n\n\n\nEDM\n0.079\n\n\nL.A\n0.092\n\n\nNYI\n0.093\n\n\nNSH\n0.098\n\n\nCGY\n0.102\n\n\nWSH\n0.103\n\n\nVGK\n0.103\n\n\nCOL\n0.107\n\n\nDET\n0.112\n\n\nS.J\n0.119\n\n\n\n\n\n\n\n\n\nYou can explore this data by team and by year in the dashboard below, which is also available here. (If you’re on reading this on mobile, clicking the link will likely give you a better viewing experience.)"
  },
  {
    "objectID": "blog/2020-05-18-goalie-pulling/index.html",
    "href": "blog/2020-05-18-goalie-pulling/index.html",
    "title": "The State of Goalie Pulling in the NHL",
    "section": "",
    "text": "When people ask me how to get into sports analytics, I always suggest starting with a question that they’re interested in exploring and using that question as a framework for learning the domain knowledge and the technical skills they need. I feel comfortable giving this advice because it’s exactly how I got into hockey analytics: I was curious about goalie pulling, and I couldn’t find enough data to satisfy my curiosity. There are plenty of articles on when teams should pull their goalies, but aside from a 2015 article on FiveThirtyEight by Michael Lopez and Noah Davis, I couldn’t find much data on when NHL teams were actually pulling their goalies and if game trends were catching up to the mathematical recommendations. I presented some data on the topic at the Seattle Hockey Analytics Conference in March 2019, but the following analysis is broader and includes more seasons of data.\nRead the entire article on Hockey-Graphs here.\nThe Tableau dashboard that accompanies this article should show below and is also available here."
  },
  {
    "objectID": "blog/2020-08-03-last-change/index.html",
    "href": "blog/2020-08-03-last-change/index.html",
    "title": "Examining the Effect of the Last Change in Hockey",
    "section": "",
    "text": "I keep a running list of hockey things that I want to look into, and now that playoff bubble hockey has begun, with many people discussing what “home field” advantage looks like in a different building with no fans, it seems like a good time to cross off “last change” from the list. My curiosity about the effect of this advantage spawned some analysis that led to the Twitter thread linked here and now this article!\nA brief review of NHL rules: At the beginning of each period and after each stoppage, play resumes with a faceoff. There are, on average, 60 faceoffs in a game. One advantage for the home team is that they get the last line change, thus determining the player matchup until one of the teams makes another change. They control the matchup beginning at every faceoff, except those following an icing by the home team (when a player dumps the puck, untouched, over the center line and then the goal line), which happens, on average, four times per game.\nAll data is from the 2015-2019 seasons and references 5v5 play by the home team in the regular season, unless otherwise indicated."
  },
  {
    "objectID": "blog/2020-08-03-last-change/index.html#introduction",
    "href": "blog/2020-08-03-last-change/index.html#introduction",
    "title": "Examining the Effect of the Last Change in Hockey",
    "section": "Introduction",
    "text": "Introduction\nHome teams have historically had a slight advantage over away teams in terms of offense generation. (Throughout, as a proxy for offense generation, I’m using the rate of unblocked shot attempts—i.e., goals, shots on goals, and missed shots—per 60 minutes.) In the 6081 regular season games that form the basis of this analysis, home teams generated 42.6 unblocked shot attempts per 60 minutes at 5v5, while away teams generated 40.7. This difference holds for every season and is a slight but consistent advantage. (When you see “adjusted” statistics in hockey, they’re adjusted for venue as well as score state.)\nHow much of this advantage might be attributed to the fact that the home team gets the last change and gets to control the matchups for a certain part of the game? To address this, we first need to know how much of the game this actually affects. That is, what percent of the game involves a line matchup orchestrated by the home team? Since every game second can be classified as a “controlled matchup” (i.e., all seconds after a home-team-controlled faceoff, until there is a change) or not, we can calculate this, and it turns out to be pretty consistent across seasons at around 38 percent.\n\nThe variation is fairly small, as well: if you separate out this calculation per team, it only varies from 35 to 41 percent. (And if you isolate the matched line to just forward lines or just defense pairs, the matchup time increases to 42 percent.) So this means a majority of the game still involves line matchups that are created on the fly. However, 38% of game play at 5v5—which is, on average, just over 18 minutes per game—is still a significant amount, and having this information means that we can assess the home team in two ways.\n\nCompare the team to itself: How does a team generate offense when the matchup is controlled versus when it is not? Is the home team better when it gets to pick the lines?\nCompare the team to the other team: When the matchup is controlled, how does the home team generate offense compared to the away team? How much of an advantage do they actually have?"
  },
  {
    "objectID": "blog/2020-08-03-last-change/index.html#league-averages",
    "href": "blog/2020-08-03-last-change/index.html#league-averages",
    "title": "Examining the Effect of the Last Change in Hockey",
    "section": "League Averages",
    "text": "League Averages\nBoth of those questions, at the league level, can be answered by the graph below.\n\nBoth the home and away teams do a bit better here, in that their shot rates are higher, when the matchups aren’t controlled. But when they are, as logic might dictate, the home team does better, at 41.1 unblocked shot attempts per 60 minutes compared to 35.6 for the away team. In other words, about 15 percent more offense. (It’s worth noting that these numbers do not change much if you consider a “line matchup” to be either just forward lines or just defense pairs, rather than the entire five-skater group.)\nWe can also break this down further by the event zone of the faceoff that kicked off the controlled matchup.\n\nThe graph above shows the shot attempt rate when the matchup is controlled. As to be expected, it’s highest when the faceoff is in the offensive zone, and also as to be expected, the home team sees an advantage when compared to the away team: they generate 60.5 shot attempts (per 60 minutes) from an offensive zone faceoff, while the away team generates 55.2 (an advantage of about 10 percent). The home team sees this advantage when the faceoff is in the neutral and the defensive zones, as well, even though less offense overall is generated from starts in those areas. And by shifting the perspective, it’s of course true that the home team also does a better job suppressing shots, as well, when they control the matchup."
  },
  {
    "objectID": "blog/2020-08-03-last-change/index.html#teams-and-players",
    "href": "blog/2020-08-03-last-change/index.html#teams-and-players",
    "title": "Examining the Effect of the Last Change in Hockey",
    "section": "Teams and Players",
    "text": "Teams and Players\nWith the league averages as a comparison, we can evaluate individual team-seasons on these same metrics. First, within each team, we can compare controlled matchup game play to game play that doesn’t involve a controlled matchup, for games in which they are the home team. As shown in the earlier graph, home teams do slightly worse when the matchups are controlled, in that their shot rate is depressed by a little over five percent compared to when the matchups are not controlled. Let’s look at the teams that have the biggest difference in their shot rates between those states of play.\n \nIn both graphs, the league average of -5.5 percent is shown with a dashed line. The first graph shows the 10 teams who have the biggest positive difference, while the second shows the 10 teams who have the biggest negative difference. That is, teams like Carolina and Columbus from this season played better than we might expect when they were able to control the matchup, while teams like Chicago and the Islanders (over multiple seasons) played worse.\nNext, we can look at team-level data when the matchup is controlled. We already saw that the home team has the advantage there (the league average is 15 percent more offense, as shown above), but we can find out which teams have the largest advantage and which have the smallest. This likely correlates well to overall offensive performance—that is, even when a bad team has control of the matchup, they’re still not very good. And vice versa for the good teams. Shown below are the top 10 and bottom 10 teams, those who have the biggest and smallest advantage over the away team when they control the matchup as the home team. The dashed line is the league average at 15 percent.\n \nLastly, mostly just for fun, we can take a look at which players—forward lines and defense pairs—in this time frame have been used most often when the home team has control of the matchup.\n \nShown above are the players trusted for offensive zone faceoffs, and shown below are the lines and pairs most commonly used for defensive zone faceoffs, when the home teams want to suppress offense by the away team."
  },
  {
    "objectID": "blog/2020-08-03-last-change/index.html#coaches",
    "href": "blog/2020-08-03-last-change/index.html#coaches",
    "title": "Examining the Effect of the Last Change in Hockey",
    "section": "Coaches",
    "text": "Coaches\nTo take this one step further, we can look at coaches instead of just individual team-seasons, as the head coach is generally in charge of the line matchups. The data for each coach includes all home games for which he was the head coach over this time frame, possibly combining seasons and teams.\n\nSimilar to what we saw previously, the graphs above and below show the top and bottom 10 coaches, respectively, with the greatest difference in controlled play versus uncontrolled. Some coaches, like Rod Brind’Amour, show a greater positive difference with line matching than we might expect, while others, like Jeremy Colliton and Joel Quenneville, are below league average when comparing controlled matchup time to non-controlled matchup time."
  },
  {
    "objectID": "blog/2020-08-03-last-change/index.html#special-teams",
    "href": "blog/2020-08-03-last-change/index.html#special-teams",
    "title": "Examining the Effect of the Last Change in Hockey",
    "section": "Special Teams",
    "text": "Special Teams\nThe home team’s technical advantage of the last line change still applies during special teams play, but we might hypothesize that the effect would be smaller since special teams units are usually fairly standard—each team generally has two units each for the power play and penalty kill, and teams are often prepared for these matchups in advance.\n\nStill, we can explore the effect similarly to how we did so in the first section. In contrast to play at 5v5, in which home teams generate a little less offense when matchups are controlled versus when they are not, at 5v4 home teams generate much more here. This makes sense considering that power plays start with a faceoff, generally in the offensive zone and generally with the top unit, and the initial line matchup likely lasts longer. We can also see that the home team does have a slight overall advantage of around five percent (84.4 unblocked shot attempts per 60 minutes versus 80.1) when the matchup is controlled."
  },
  {
    "objectID": "blog/2020-08-03-last-change/index.html#playoffs",
    "href": "blog/2020-08-03-last-change/index.html#playoffs",
    "title": "Examining the Effect of the Last Change in Hockey",
    "section": "Playoffs",
    "text": "Playoffs\n\nLastly, we can take a look at playoff games, from the 2015 through 2018 seasons, and compare to what we saw at the beginning with regular season play (both at 5v5). Overall, the difference is not tremendous. In the regular season, home teams saw about a five percent decrease in offense when the matchup was controlled as compared to when it was not; that difference is slightly smaller in the playoffs with a three percent decrease. The home team advantage when the matchup is controlled was shown to be around 15 percent in the regular season, while it increases to about 23 percent in the playoffs (41.3 unblocked shot attempts per 60 minutes for the home team versus 33.6 for the away team)."
  },
  {
    "objectID": "blog/2020-08-03-last-change/index.html#conclusion",
    "href": "blog/2020-08-03-last-change/index.html#conclusion",
    "title": "Examining the Effect of the Last Change in Hockey",
    "section": "Conclusion",
    "text": "Conclusion\nSince the 2020 playoffs have started in the bubble, with “home” teams playing with no fans and often in a building that is not actually their home building, many fans, writers, and analysts have been publicly wondering about the value of being the home team. These home teams in the bubble are certainly missing some advantages of being in their own arenas (the routine, the energy of the fans, the possible penalty implications), but the advantage of having the last line change—and therefore controlling the matchups for an average of 18 5v5 minutes per game—cannot be discounted.\nPlay-by-play data sourced via the Evolving-Hockey scraper, position data sourced from their website. Coach data sourced from Natural Stat Trick."
  },
  {
    "objectID": "blog/2020-12-03-functions-nhl-standings-points/index.html",
    "href": "blog/2020-12-03-functions-nhl-standings-points/index.html",
    "title": "Learning Iterative User-Defined Functions with NHL Standings Points",
    "section": "",
    "text": "One of the major sticking points for many people learning R—especially for people without a lot of prior programming experience—is user-defined functions. This was certainly true for me, and I know I still don’t use them as often as I should! There’s confusion around how they work, both conceptually and in practice, as well as why they’re actually useful and when they’re applicable.\nI find it most useful to see full examples that clearly delineate why a user-defined function is necessary for a particular use case, how to construct the function, and how to apply it. That’s what I’m hoping to share today with this example about NHL standings points. I first wrote a version of this code earlier this year when I examined the state of goalie pulling in the NHL and wanted to know if teams were more aggressive in pulling their goalie when they were chasing a playoff spot (spoiler alert: they’re really not). But to be able to determine the playoff status of a team at the time of every game that they played, I quickly determined that I needed to write my own function.\n\nWhy functions?\nOne of the great things about R is that there are so many useful packages and functions out there that solve specific problems. So why would you bother writing your own?\nEfficiency and automation\nThe golden rule is that if you’ve copied and pasted code more than twice, it’s probably time to write a function instead. It’s more efficient, and it means that when you have to make changes, you’re less likely to make a mistake because you’re only updating code in one place instead of several. There are lots of good examples in the R for Data Science book.\nApplying code iteratively\nFunctions are also useful whenever you want to apply multiple segments of code to different segments of data. That is, you want to split your data into pieces, apply a function to each piece, and then put the pieces back together. This is the aspect that the following example focuses on, and to do so we’ll use basic tidyverse functions as well as a function from the purrr package (which is loaded as part of the core tidyverse set of packages).\n\n# Basic syntax of creating a function\n\nname_of_function &lt;- function(needed_value) {\n  # all the code goes here\n}\n\n# Using a function (works just like any other!)\n\nname_of_function(needed_value)\n\n\nTime for code\nThe data I’m starting with for this example is a csv file of play-by-play data, scraped with the Evolving Hockey R scraper, for the 2019-20 NHL season.\n\nlibrary(tidyverse)\n\nseason_19 &lt;- read_csv(\"pbp_201920.csv\")\n\n# Remove unnecessary variables and filter to regular season only\n\nseason_19_R &lt;- season_19 %&gt;%\n  filter(session == \"R\") %&gt;%\n  select(season, game_id, game_date, event_index, game_period, game_seconds,\n         event_type, event_team, home_team, away_team, home_score, away_score)\nThere’s some data manipulation that needs to be done before the function can be used: the data, which in its original form has hundreds of observations per game, needs to be grouped by game to determine the score of the game and whether or not the game ended in regulation. Then the data needs to be pivoted (so that there’s two rows per game, one for each team), and variables need to be added to calculate points, wins, etc.\n\n# Group by individual game and find the final score as well as the maximum\n# value of game_period, which will indicate whether it was a regulation game\n\ngame &lt;- season_19_R %&gt;%\n  group_by(season, game_id, game_date, home_team, away_team) %&gt;%\n  summarize(away_score = max(away_score),\n            home_score = max(home_score),\n            max_period = max(game_period))\n\n# Rearrange data to be one row per team per game\n\nteam_game &lt;- game %&gt;%\n  # pivot the home and away values to get one row per team\n  pivot_longer(home_team:away_team,\n               names_to = c(\"home_away\", \".value\"),\n               names_pattern = \"(.+)_(.+)\") %&gt;%\n  # calculate the points awarded (2 for a win, 1 for an OT/SO loss, 0 for a reg. loss)\n  # detail wins vs. regulation wins vs. regulation + OT wins for tie-breaking purposes\n  mutate(score = ifelse(home_away == \"home\", home_score, away_score),\n         opp_score = ifelse(home_away == \"home\", away_score, home_score),\n         points = case_when(score &gt; opp_score ~ 2,\n                            score &lt; opp_score & max_period &gt; 3 ~ 1,\n                            TRUE ~ 0),\n         win = ifelse(points == 2, 1, 0),\n         reg_win = ifelse(points == 2 & max_period == 3, 1, 0),\n         reg_OT_win = ifelse(points == 2 & max_period &lt;= 4, 1, 0),\n         division = case_when(team %in% c(\"BOS\", \"BUF\", \"DET\", \"FLA\", \"MTL\", \n                                          \"OTT\", \"T.B\", \"TOR\") ~ \"Atlantic\",\n                              team %in% c(\"PIT\", \"PHI\", \"WSH\", \"N.J\", \"NYI\", \n                                          \"NYR\", \"CBJ\", \"CAR\") ~ \"Metro\",\n                              team %in% c(\"NSH\", \"CHI\", \"STL\", \"WPG\", \"COL\", \n                                          \"DAL\", \"MIN\") ~ \"Central\",\n                              team %in% c(\"ARI\", \"L.A\", \"ANA\", \"S.J\", \"VAN\", \n                                          \"EDM\", \"CGY\", \"VGK\") ~ \"Pacific\"),\n         conference = ifelse(division %in% c(\"Atlantic\", \"Metro\"), \"East\", \"West\"),\n         game = 1) %&gt;%\n  select(-c(home_score, away_score, home_away, max_period)) %&gt;%\n  group_by(team) %&gt;%\n  # calculate the running counts for points, wins, reg. wins, reg. + OT wins, and games\n  mutate(points_running = cumsum(points),\n         wins_running = cumsum(win),\n         RW_running = cumsum(reg_win),\n         ROW_running = cumsum(reg_OT_win),\n         game_count = cumsum(game))\n  \nThe data structure now in the team_game data frame has one row per team-game, with the results and points from that game as well as a running sum of points and wins up until that date. So we know how many points each team has at the time of each game, but to determine their playoff spot status, we of course need to know how many points they have compared to the other teams in their division and their conference. This is where the function comes in!\nWhen you’re constructing a function with multiple parts, it’s often easier to write it as “normal” code first, just with a selected input. So in this case, let’s say we want to know the playoff status of each team as of December 1. We’d start with our team_game data frame and filter to only games that are before that date. Then we’d need to find the maximum points for each team (i.e., the points they’d have on December 1), take care of our tie-breakers, and determine how the teams are ranked within their division and their conference. Next, we could figure out who the wild card teams are and then what the points threshold is in each conference to have a playoff spot (e.g., on December 1, a team would need to have 30 points to be in a playoff spot in the Eastern Conference).\nBut of course, we want to know the playoff status of each team as of every day, not just December 1. To do that, we’d put all the code we wrote into a function and create an input for that first step of filtering our team_game data frame. In the function below, that input, which is the one argument in function(), is called simply date, and it’s used in the third line within the function, filter(game_date &lt; date).\n\n# Run the function to get the values for each game date\n\nstandings_points &lt;- function(date) {\n  \n  division &lt;- team_game %&gt;%\n    # this will filter to all the games before the selected standings date\n    filter(game_date &lt; date) %&gt;%\n    group_by(conference, division, team) %&gt;%\n    # find the max points, wins, etc. as of that date\n    summarize(points = max(points_running),\n              games = max(game_count),\n              RW = max(RW_running),\n              ROW = max(ROW_running),\n              wins = max(wins_running)) %&gt;%\n    # sort to take care of the tie-breakers\n    arrange(desc(points), games, desc(RW), desc(ROW), desc(wins)) %&gt;%\n    # find each team's rank within their division\n    mutate(div_rank = row_number()) %&gt;%\n    group_by(conference) %&gt;%\n    # find each team's rank within the conference\n    mutate(conf_rank = row_number(),\n           div_top3 = ifelse(div_rank &lt; 4, 1, 0),\n           standings_date = date)\n  \n  wildcard &lt;- division %&gt;%\n    # filter to teams that are not automatically in the playoffs\n    filter(div_top3 == 0) %&gt;%\n    group_by(conference) %&gt;%\n    # find the rank of the remaining teams and determine which two are wild card teams\n    mutate(conf_rank_new = row_number(),\n           wild_card = ifelse(conf_rank_new &lt; 3, 1, 0)) %&gt;%\n    filter(wild_card == 1) %&gt;%\n    ungroup()\n  \n  playoff_spot &lt;- division %&gt;%\n    # join back in the wild card data\n    left_join(select(wildcard, team, wild_card), by = \"team\") %&gt;%\n    # determine which teams are in a playoff spot\n    mutate(playoff_spot = ifelse(div_top3 == 1 | !(is.na(wild_card)), 1, 0))\n  \n  threshold &lt;- playoff_spot %&gt;%\n    # filter to playoff teams only at that time\n    filter(playoff_spot == 1) %&gt;%\n    group_by(conference) %&gt;%\n    # find the points threshold\n    summarize(pts_threshold = min(points))\n  \n  playoff_spot &lt;- playoff_spot %&gt;%\n    # join back in the threshold data\n    left_join(threshold, by = \"conference\")\n  \n}\n\n# Use the map_df function to apply to function to each unique value of game_date\n\nstandings_data &lt;- map_df(unique(team_game$game_date), standings_points)\n  \nThat last line of code above uses the map_df() function from the purrr package to apply the function iteratively. The second argument of map_df() is the name of our new function, standings_points, while the first argument is the input to the function (what’s referred to as date within the user-defined function code itself). We’re telling R to apply that function many times, with the inputs being each unique value of game_date from our team_game data frame that serves as the starting point of the function. map_df collects all the data from running the function many times and puts it into the resulting standings_data data frame, which has a row per team per date with their own results (from the team_game data frame) as well as all the variables created with the function (div_rank, conf_rank, playoff_spot, pts_threshold, etc.)\n\n\nWhat can be done with this?\nHaving data in this format allows us to do more analysis with our new standings-related variables, and the two simple examples below use the pts_threshold variable (the number of standings points that would result in a playoff spot in each conference, on each given day).\nWe could select a particular team and look at how their accumulation of standings points throughout the season compares to the playoff threshold.\n\n\n# Example 1: graph progress through season compared to playoff threshold\n\ngraph &lt;- standings_data %&gt;%\n  ungroup() %&gt;%\n  select(team, standings_date, points, pts_threshold) %&gt;%\n  pivot_longer(points:pts_threshold)\n\ngraph %&gt;%\n  filter(team == \"T.B\") %&gt;%\n  ggplot(aes(x = standings_date, y = value, group = name)) +\n  geom_line(aes(linetype = name, color = name)) +\n  scale_linetype_manual(values = c(\"dotted\", \"solid\")) +\n  scale_color_manual(values = c(\"#2364AA\", \"#808080\")) +\n  labs(title = \"Playoff status through 2019-20 NHL season\",\n       subtitle = \"Solid: points threshold for playoffs\\nDotted: points for Tampa\",\n       caption = \"@MeghanMHall\") +\n  ylab(\"Standings Points\") +\n  meg_theme() +\n  theme(legend.position = \"none\",\n        axis.title.x = element_blank())\nWe could also go back to our original play-by-play data and join in observations from our function result—here, the playoff threshold—so that we can compare measures by that metric. For example, we could look at the relationship between shooting percentage (the percentage of shots on goal that are goals) and standings points above the playoff threshold.\n\n\n# Example 2: shooting percentage by standings points\n\nseason_19_sh &lt;- season_19_R %&gt;%\n  # filter to shot on goal events only, excluding shootouts\n  filter(event_type %in% c(\"SHOT\", \"GOAL\") &\n         game_period &lt; 5) %&gt;%\n  select(game_id, game_date, event_type, event_team) %&gt;%\n  mutate(shot = 1,\n         goal = ifelse(event_type == \"GOAL\", 1, 0))\n\nsh_points &lt;- season_19_sh %&gt;%\n  # filter out October and November games\n  filter(!(lubridate::month(game_date) %in% c(10,11))) %&gt;%\n  # join in our threshold data\n  left_join(standings_data %&gt;% \n              ungroup %&gt;% \n              select(team, points, pts_threshold, standings_date), \n            by = c(\"event_team\" = \"team\",\n                   \"game_date\" = \"standings_date\")) %&gt;%\n  # find our variable of interest: standings points above the playoff threshold\n  mutate(threshold_diff = points - pts_threshold) %&gt;%\n  group_by(threshold_diff) %&gt;%\n  summarize(shots = sum(shot),\n            goals = sum(goal),    \n            sh_perc = goals / shots)\n\nsh_points %&gt;%\n  ggplot(aes(x = threshold_diff, y = sh_perc)) + \n  geom_point(aes(size = shots)) +\n  labs(title = \"Shooting percentage compared to standings points\",\n       subtitle = \"2019-20 regular NHL season, December onward\",\n       caption = \"@MeghanMHall\",\n       size = \"Total Shots\") +\n  ylab(\"Shooting Percentage\") +\n  xlab(\"Standings Points Above Playoff Threshold\") +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits = c(0, 0.13)) +\n  geom_vline(xintercept = 0, linetype=\"dashed\") +\n  meg_theme() +\n  theme(legend.position = c(0.85, 0.22))\nHopefully this showed you a concrete example of when it’s helpful to use a user-defined function iteratively, how to construct it, and how to apply it. If you’d like practice following this process, I have a sample play-by-play data set in my betweenthepipes package, and during a recent workshop, I went through a different example using this process with that sample data set. Those slides are available here."
  },
  {
    "objectID": "blog/2021-07-20-alternatives-to-simple-color-legends-in-ggplot2/index.html",
    "href": "blog/2021-07-20-alternatives-to-simple-color-legends-in-ggplot2/index.html",
    "title": "Alternatives to Simple Color Legends in ggplot2",
    "section": "",
    "text": "The beauty of visualizing data with R (ggplot2, specifically) is that nearly every single thing about your plot is customizable. However, that blessing can also be a curse because with so many options to make your plots look exactly how you want, it can be difficult to even know what all the options are! But learning to customize different parts of the plot is how you can begin to take your data visualizations to the next level and move beyond the standard, out-of-the-box ggplots. Legends, or the way aesthetic values are mapped to data, are an essential part of plots, and this post introduces a few different options for labeling colors effectively.\n\nToday’s data\nThe data set used in today’s example comes from this week’s edition of #TidyTuesday and contains variables of interest on drought conditions in the US from the US Drought Monitor.\nI can read in the data directly from the Tidy Tuesday Github repo as follows:\ndrought &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-07-20/drought.csv')\nAnd also load the libraries and the theme I’ll need for today’s plots:\nlibrary(tidyverse)\nlibrary(gghighlight)\nlibrary(scales)\nlibrary(ggtext)\nlibrary(ggrepel)\n\nmeg_theme &lt;- function () { \n  theme_linedraw(base_size=11, base_family=\"Avenir\") %+replace% \n    theme(\n      panel.background  = element_blank(),\n      plot.background = element_rect(fill = \"transparent\", color = NA), \n      legend.background = element_rect(fill = \"transparent\", color = NA),\n      legend.key = element_rect(fill = \"transparent\", color = NA),\n      axis.ticks = element_blank(),\n      panel.grid.major = element_line(color = \"grey90\", size = 0.3), \n      panel.grid.minor = element_blank(),\n      plot.title.position = \"plot\",\n      plot.title = element_text(size = 18, hjust = 0, vjust = 0.5, \n                                margin = margin(b = 0.2, unit = \"cm\")),\n      plot.subtitle = element_text(size = 10, hjust = 0, vjust = 0.5, \n                                   margin = margin(b = 0.4, unit = \"cm\")),\n      plot.caption = element_text(size = 7, hjust = 1, face = \"italic\", \n                                  margin = margin(t = 0.1, unit = \"cm\")),\n      axis.text.x = element_text(size = 13),\n      axis.text.y = element_text(size = 13)\n    )\n}\nI recommend using custom themes whenever possible as it saves lots of typing and requires you to learn about different parts of the plot and the options available to customize!\n\n\ngghighlight\n\nThe gghighlight package provides useful functionality for highlighting (and labeling) certain elements of a plot. This is particularly nice for line graphs, when there are often too many lines to apply color to each one (such as in the example above, with one line per state).\nFor the plot above, I’ve used the gghighlight function to highlight only certain states—specifically, those that have a maximum value above 40. Other possible arguments of that function allow you to specify the aesthetics for the labels (here I’ve adjusted the label size to be 4.5) as well as the aesthetics of the unhighlighted lines. I used the size and alpha aesthetics to make those lines smaller and more transparent, respectively. The scale_color_viridis function sets the color palette for the highlighted lines.\ndrought %&gt;% \n  filter(drought_lvl == \"D4\" & lubridate::year(valid_start) &gt;= 2010) %&gt;% \n  group_by(year = lubridate::year(valid_start), state_abb) %&gt;% \n  summarize(avg = mean(area_pct)) %&gt;% \n  ggplot(aes(x = year, y = avg, group = state_abb, color = state_abb)) +\n  geom_line(size = 2) +\n  gghighlight(max(avg) &gt; 40,\n            label_params = list(size = 4.5),\n            unhighlighted_params = list(size = 0.5, alpha = 0.5)) +\n  scale_color_viridis_d(option = \"D\") +\n  scale_x_continuous(breaks = seq(2010, 2021, 1),\n                     expand = expansion(mult = c(0.01, 0.07))) +\n  scale_y_continuous(labels = label_number(suffix = \"%\")) +\n  labs(x = NULL,\n       y = \"Percent of state (by area) in exceptional drought\",\n       title = \"Three states currently have over 40% of their area in\\nexceptional drought (the highest level)\",\n       subtitle = \"Data source: The U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the\\nUniversity of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and\\nAtmospheric Administration.\",\n       caption = \"@MeghanMHall\") +\n  meg_theme() +\n  theme(panel.grid.major.x = element_blank())\n\n\nggtext\n\nIf you only have one or two items that you’d like to highlight, the ggtext package allows you to specify those colors in the title of your plot instead of using a legend on the plot. This technique is appropriate for many types of graphs, but the example above is the same line graph as in the first example, just with two states highlighted. That highlighting is done in a different way than in the first example, just to show another option.\nIn the code below I’ve created a new variable called highlight to specify which lines I want highlighted, and that variable is mapped to the color, size, and alpha aesthetics within the ggplot function. The values of those aesthetics can be specified with the various scale_aesthetic_manual functions.\n(The arrange and fct_inorder functions were necessary to control the ordering of the lines so that the California and Utah lines were on top in the final plot.)\nTo adjust the colors within the title, you’ll notice that the title argument within the labs function has &lt;span style = 'color:#3B528BFF;'&gt;California&lt;/span&gt; and &lt;span style = 'color:#FDE725FF;'&gt;Utah&lt;/span&gt; to specify the color of those words. For that ggtext functionality to work appropriately, the following is necessary within the theme function: plot.title = element_markdown()).\n\nUseful tip: I wanted the colors for these states to be the same as they were in the first plot, which used a palette via scale_color_viridis. To extract hex codes from a plot you made with a palette, assign that plot to an object (say, plot) and then run ggplot_build(plot)$data.\n\ndrought %&gt;% \n  filter(drought_lvl == \"D4\" & lubridate::year(valid_start) &gt;= 2010) %&gt;% \n  group_by(year = lubridate::year(valid_start), state_abb) %&gt;% \n  summarize(avg = mean(area_pct)) %&gt;% \n  arrange(state_abb %in% c(\"CA\",\"UT\")) %&gt;% \n  mutate(state_abb = fct_inorder(state_abb), #\n         highlight = ifelse(state_abb %in% c(\"CA\",\"UT\"), state_abb, \"base\")) %&gt;% \n  ggplot(aes(x = year, y = avg, group = state_abb, color = highlight,\n             size = highlight, alpha = highlight)) +\n  geom_line(show.legend = FALSE) +\n  scale_color_manual(values = c(\"#3B528BFF\",\"#FDE725FF\",\"grey\")) +\n  scale_size_manual(values = c(2, 2, 0.5)) +\n  scale_alpha_manual(values = c(1, 1, 0.5)) +\n  scale_x_continuous(breaks = seq(2010, 2021, 1),\n                     expand = expansion(mult = c(0.01, 0.02))) +\n  scale_y_continuous(labels = label_number(suffix = \"%\")) +\n  labs(x = NULL,\n       y = \"Percent of state (by area) in exceptional drought\",\n       title = \"Percent of &lt;span style = 'color:#3B528BFF;'&gt;California&lt;/span&gt; and &lt;span style = 'color:#FDE725FF;'&gt;Utah&lt;/span&gt;, by area, in exceptional drought\",\n       subtitle = \"Data source: The U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the\\nUniversity of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and\\nAtmospheric Administration.\",\n       caption = \"@MeghanMHall\") +\n  meg_theme()+\n  theme(panel.grid.major.x = element_blank(),\n        plot.title = element_markdown())\n\n\nggrepel\n\nggrepel automatically moves and removes text labels to avoid overlapping. The plot above shows similar data as in the earlier plots but in scatter plot form, with the percent of state by population along the x-axis and a point for each state and year. The California and Utah points are highlighted like above.\nI created a label variable using paste to concatenate the state abbreviation as well as the year, and added that variable to the aesthetics of the geom_label_repel function.\ndrought %&gt;% \n  filter(drought_lvl == \"D4\" & lubridate::year(valid_start) &gt;= 2010) %&gt;% \n  group_by(year = lubridate::year(valid_start), state_abb) %&gt;% \n  summarize(avg_area = mean(area_pct),\n            avg_pop = mean(pop_pct)) %&gt;% \n  mutate(highlight = ifelse(state_abb %in% c(\"CA\",\"UT\"), state_abb, \"base\"),\n         label = if_else(highlight != \"base\",\n                         paste(state_abb, year), NA_character_)) %&gt;% \n  ggplot(aes(x = avg_pop, y = avg_area, color = highlight,\n             size = highlight)) +\n  geom_point() +\n  geom_label_repel(aes(label = label), size = 4) +\n  scale_color_manual(values = c(\"grey\",\"#3B528BFF\",\"#FDE725FF\")) +\n  scale_size_manual(values = c(1, 2, 2)) +\n  scale_y_continuous(labels = label_number(suffix = \"%\")) +\n  scale_x_continuous(labels = label_number(suffix = \"%\")) +\n  labs(x = \"Percent of state (by population) in exceptional drought\",\n       y = \"Percent of state (by area) in exceptional drought\",\n       title = \"Percent of state in exceptional drought\\nAnnual averages over the past 10 years\",\n       subtitle = \"Data source: The U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the\\nUniversity of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and\\nAtmospheric Administration.\",\n       caption = \"@MeghanMHall\") +\n  meg_theme() +\n  theme(legend.position = \"none\")\n\nOr, if you aren’t interested in labeling points with the year and instead want to just label one point each for the highlighted states like in the plot above, simply adjust the conditions for the label variable to label just two points.\ndrought %&gt;% \n  filter(drought_lvl == \"D4\" & lubridate::year(valid_start) &gt;= 2010) %&gt;% \n  group_by(year = lubridate::year(valid_start), state_abb) %&gt;% \n  summarize(avg_area = mean(area_pct),\n            avg_pop = mean(pop_pct)) %&gt;% \n  mutate(highlight = ifelse(state_abb %in% c(\"CA\",\"UT\"), state_abb, \"base\"),\n         label = case_when(avg_area &lt; 40 | highlight == \"base\" ~ NA_character_,\n                           state_abb == \"CA\" ~ \"California\",\n                           state_abb == \"UT\" ~ \"Utah\")) %&gt;% \n  ggplot(aes(x = avg_pop, y = avg_area, color = highlight,\n             size = highlight)) +\n  geom_point() +\n  geom_label_repel(aes(label = label), size = 4) +\n  scale_color_manual(values = c(\"grey\",\"#3B528BFF\",\"#FDE725FF\")) +\n  scale_size_manual(values = c(1, 2, 2)) +\n  scale_y_continuous(labels = label_number(suffix = \"%\")) +\n  scale_x_continuous(labels = label_number(suffix = \"%\")) +\n  labs(x = \"Percent of state (by population) in exceptional drought\",\n       y = \"Percent of state (by area) in exceptional drought\",\n       title = \"Percent of state in exceptional drought\\nAnnual averages over the past 10 years\",\n       subtitle = \"Data source: The U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the\\nUniversity of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and\\nAtmospheric Administration.\",\n       caption = \"@MeghanMHall\") +\n  meg_theme() +\n  theme(legend.position = \"none\")\n\n\nOverriding aesthetics of standard legends\n\nThere are also tweaks you can make to the standard color legends in ggplot to make them look a little nicer. The plot above shows the same data as in the previous example, but perhaps in this case you want a more standard legend instead of the option to label a specific point.\nThe highlight variable is still being used to identify which points should be colored, but if we were to simply add a color legend for that variable, you’d have three elements in that legend: blue, yellow, and gray (for the unhighlighted points). If you don’t want the gray element to show up in the legend, simply add a breaks argument to the scale_color_manual function with the elements you want to keep. Here, that’s breaks = c(\"CA\",\"UT\").\nIn this example I also moved the legend onto the plot itself, which often saves space, with legend.position = c(0.9, 0.2) and added a border to the legend. Using the guides function, I removed the automatic size legend with size = FALSE and overrode the overall point size aesthetic with color = guide_legend(override.aes = list(size = 5)) to make the points bigger in the legend than they are in the plot.\ndrought %&gt;% \n  filter(drought_lvl == \"D4\" & lubridate::year(valid_start) &gt;= 2010) %&gt;% \n  group_by(year = lubridate::year(valid_start), state_abb) %&gt;% \n  summarize(avg_area = mean(area_pct),\n            avg_pop = mean(pop_pct)) %&gt;% \n  mutate(highlight = ifelse(state_abb %in% c(\"CA\",\"UT\"), state_abb, \"base\")) %&gt;% \n  ggplot(aes(x = avg_pop, y = avg_area, color = highlight,\n             size = highlight)) +\n  geom_point() +\n  scale_color_manual(values = c(\"grey\",\"#3B528BFF\",\"#FDE725FF\"),\n                     breaks = c(\"CA\",\"UT\")) +\n  scale_size_manual(values = c(1, 2, 2)) +\n  scale_y_continuous(labels = label_number(suffix = \"%\")) +\n  scale_x_continuous(labels = label_number(suffix = \"%\")) +\n  labs(x = \"Percent of state (by population) in exceptional drought\",\n       y = \"Percent of state (by area) in exceptional drought\",\n       title = \"Percent of state in exceptional drought\\nAnnual averages over the past 10 years\",\n       subtitle = \"Data source: The U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the\\nUniversity of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and\\nAtmospheric Administration.\",\n       caption = \"@MeghanMHall\") +\n  guides(color = guide_legend(override.aes = list(size = 5)),\n         size = FALSE) +\n  meg_theme() +\n  theme(legend.position = c(0.9, 0.2),\n        legend.title = element_blank(),\n        legend.text = element_text(size = 14),\n        legend.background = element_rect(linetype = \"solid\", \n                                         size = 0.5, color = \"black\"))\n\nThe next several plots focus on 2021 data from California only, showing the percent of the state by land area that is in each drought category for each week of data collection. (You’ll notice some extra data manipulation is necessary to get correct values for the other drought levels, this is because the area_pct variable in this data set is sometimes a cumulative variable.)\nThis first example above has almost your standard color legend. The only changes I’ve made are to move the legend to the top of the plot, with legend.position = \"top\" within theme, and to specify that the legend items stay in one row instead of the standard two, with guides(fill = guide_legend(nrow = 1)).\n\nIf you were going to pursue this option, you’d need to relabel the factor levels of the drought_lvl variable so that it was clear what the levels mean. To get that “legend is unclear” label, I used one of my favorite fun functions: cowplot::stamp(plot, color = \"orange\", label = \"legend is unclear\", vjust = 6).\n\ndrought %&gt;% \n  filter(state_abb == \"CA\" & lubridate::year(valid_start) == 2021) %&gt;% \n  mutate(drought_lvl = fct_relevel(drought_lvl, \n                                   levels = c(\"D4\",\"D3\",\"D2\",\"D1\",\"D0\",\"None\"))) %&gt;% \n  arrange(map_date, drought_lvl) %&gt;% \n  mutate(prev = lag(area_pct),\n         value = ifelse(drought_lvl %in% c(\"D4\",\"None\"), \n                        area_pct, area_pct - prev)) %&gt;% \n  ggplot(aes(x = valid_start, y = value, fill = drought_lvl)) +\n  geom_area(color = \"white\") +\n  scale_fill_viridis_d(option = \"A\") +\n  scale_y_continuous(labels = label_number(suffix = \"%\"),\n                     expand = expansion(mult = c(0, 0))) +\n  scale_x_date(date_labels = '%b',\n               breaks = as.Date(c('2021/1/10','2021/2/10','2021/3/10',\n                                  '2021/4/10','2021/5/10','2021/6/10','2021/7/10')),\n               expand = expansion(mult = c(0, 0))) +\n  guides(fill = guide_legend(nrow = 1)) +\n  labs(x = NULL,\n       y = \"Percent of state (by area)\",\n       title = \"Drought status in California in 2021\",\n       subtitle = \"Data source: The U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the\\nUniversity of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and\\nAtmospheric Administration.\",\n       caption = \"@MeghanMHall\") +\n  meg_theme()+\n  theme(panel.grid.major = element_blank(),\n        legend.position = \"top\",\n        legend.title = element_blank(),\n        plot.subtitle = element_text(margin = margin(b = 0, unit = \"cm\")))\n\nIf you wished to move the legend onto the plot instead, you can change the background with legend.background = element_rect(fill = \"white\") and also adjust the margins with legend.margin, both within the theme layer.\nThis plot suffers from the same unclear legend problem as the one above.\nAlso note that in these plots for the x-axis, I’m using scale_x_date to get a little more functionality in how the dates are shown. I’m using breaks to fully specify at which dates I want my labels to be, but you could instead use date_breaks = '1 month' to show a label every month. date_labels specifies how the dates in the labels should be formatted: %b is the syntax for the abbreviated month name (i.e., Jan instead of January or 1), but all of the possible options are available at ?strftime.\ndrought %&gt;% \n  filter(state_abb == \"CA\" & lubridate::year(valid_start) == 2021) %&gt;% \n  mutate(drought_lvl = fct_relevel(drought_lvl, \n                                   levels = c(\"D4\",\"D3\",\"D2\",\"D1\",\"D0\",\"None\"))) %&gt;% \n  arrange(map_date, drought_lvl) %&gt;% \n  mutate(test = lag(area_pct),\n         value = ifelse(drought_lvl %in% c(\"D4\",\"None\"),\n                        area_pct, area_pct - test)) %&gt;% \n  ggplot(aes(x = valid_start, y = value, fill = drought_lvl)) +\n  geom_area(color = \"white\") +\n  scale_fill_viridis_d(option = \"A\") +\n  scale_y_continuous(labels = label_number(suffix = \"%\"),\n                     expand = expansion(mult = c(0, 0))) +\n  scale_x_date(date_labels = '%b',\n               breaks = as.Date(c('2021/1/10','2021/2/10','2021/3/10',\n                                  '2021/4/10','2021/5/10','2021/6/10','2021/7/10')),\n               expand = expansion(mult = c(0, 0))) +\n  guides(fill = guide_legend(nrow = 1)) +\n  labs(x = NULL,\n       y = \"Percent of state (by area)\",\n       title = \"Drought status in California in 2021\",\n       subtitle = \"Data source: The U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the\\nUniversity of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and\\nAtmospheric Administration.\",\n       caption = \"@MeghanMHall\") +\n  meg_theme()+\n  theme(panel.grid.major.x = element_blank(),\n        legend.position = c(0.35, 0.8),\n        legend.background = element_rect(fill = \"white\"),\n        legend.title = element_blank(),\n        legend.margin = margin(-4, 2.5, 2.5, 1.5))\n\n\nAnnotations\n\nBecoming familiar with various ways to annotate plots in ggplot2 is a great way to start making your plots look more custom. In the plot above, I’ve used several annotate functions to manually label each area with the label of that category.\nThese functions start with a \"text\" argument, and I can specify with x and y where the annotations should be. I also specify color, size, and font family, since annotations do not pick up the specified fonts from the selected theme.\ndrought %&gt;% \n  filter(state_abb == \"CA\" & lubridate::year(valid_start) == 2021) %&gt;% \n  mutate(drought_lvl = fct_relevel(drought_lvl, \n                                   levels = c(\"D4\",\"D3\",\"D2\",\"D1\",\"D0\",\"None\"))) %&gt;% \n  arrange(map_date, drought_lvl) %&gt;% \n  mutate(test = lag(area_pct),\n         value = ifelse(drought_lvl %in% c(\"D4\",\"None\"), \n                        area_pct, area_pct - test)) %&gt;% \n  ggplot(aes(x = valid_start, y = value, fill = drought_lvl)) +\n  geom_area(color = \"white\") +\n  scale_fill_viridis_d(option = \"A\") +\n  scale_y_continuous(labels = label_number(suffix = \"%\"),\n                     expand = expansion(mult = c(0, 0))) +\n  scale_x_date(date_labels = '%b',\n               breaks = as.Date(c('2021/1/10','2021/2/10','2021/3/10',\n                                  '2021/4/10', '2021/5/10','2021/6/10','2021/7/10')),\n               expand = expansion(mult = c(0, 0))) +\n  labs(x = NULL,\n       y = \"Percent of state (by area)\",\n       title = \"Drought status in California in 2021\",\n       subtitle = \"Data source: The U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the\\nUniversity of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and\\nAtmospheric Administration.\",\n       caption = \"@MeghanMHall\") +\n  annotate(\"text\", x = as.Date('2021/6/15'), y = 85, label = \"exceptional\\ndrought\", \n           size = 5, color = \"white\", family = \"Avenir\") +\n  annotate(\"text\", x = as.Date('2021/6/15'), y = 50, label = \"extreme\\ndrought\",\n           size = 5, color = \"white\", family = \"Avenir\") +\n  annotate(\"text\", x = as.Date('2021/3/15'), y = 50, label = \"severe\\ndrought\",\n           size = 5, color = \"white\", family = \"Avenir\") +\n  annotate(\"text\", x = as.Date('2021/3/15'), y = 25, label = \"moderate\\ndrought\",\n           size = 5, color = \"white\", family = \"Avenir\") +\n  annotate(\"text\", x = as.Date('2021/3/15'), y = 5, label = \"abnormally dry\",\n           size = 5, color = \"white\", family = \"Avenir\") +\n  meg_theme()+\n  theme(panel.grid.major = element_blank(),\n        legend.position = \"none\")\n\nThat annotation method also works if you have a stacked bar chart, for example, and would prefer to label the categories on either side of the plot. I’ve adjusted the available space with expand = expansion(mult = c(0.09, 0)) within scale_x_date and then added more annotate layers to specify the labels and the colors of those labels. (Remember that the hex codes within a palette are available with ggplot_build(plot)$data.)\nI tend to prefer this option when there is adequate space for each category; in this example it’s pretty clear what each label is referring to, but visually I don’t love how small the top and bottom categories are.\ndrought %&gt;% \n  filter(state_abb == \"CA\" & lubridate::year(valid_start) == 2021) %&gt;% \n  mutate(drought_lvl = fct_relevel(drought_lvl, \n                                   levels = c(\"D4\",\"D3\",\"D2\",\"D1\",\"D0\",\"None\"))) %&gt;% \n  arrange(map_date, drought_lvl) %&gt;% \n  mutate(test = lag(area_pct),\n         value = ifelse(drought_lvl %in% c(\"D4\",\"None\"),\n                        area_pct, area_pct - test)) %&gt;% \n  ggplot(aes(x = valid_start, y = value, fill = drought_lvl)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_viridis_d(option = \"A\") +\n  scale_y_continuous(labels = label_number(suffix = \"%\"),\n                     expand = expansion(mult = c(0, 0))) +\n  scale_x_date(date_labels = '%b',\n               breaks = as.Date(c('2021/1/10','2021/2/10','2021/3/10',\n                                  '2021/4/10','2021/5/10','2021/6/10','2021/7/10')),\n               expand = expansion(mult = c(0.09, 0))) +\n  labs(x = NULL,\n       y = \"Percent of state (by area)\",\n       title = \"Drought status in California in 2021\",\n       subtitle = \"Data source: The U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the\\nUniversity of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and\\nAtmospheric Administration.\",\n       caption = \"@MeghanMHall\") +\n  annotate(\"text\", x = as.Date('2020/12/15'), y = 97, label = \"exceptional\\ndrought\",\n           size = 4, color = \"#000004FF\", family = \"Avenir\") +\n  annotate(\"text\", x = as.Date('2020/12/15'), y = 75, label = \"extreme\\ndrought\",\n           size = 4, color = \"#3B0F70FF\", family = \"Avenir\") +\n  annotate(\"text\", x = as.Date('2020/12/15'), y = 45, label = \"severe\\ndrought\",\n           size = 4, color = \"#8C2981FF\", family = \"Avenir\") +\n  annotate(\"text\", x = as.Date('2020/12/15'), y = 15, label = \"moderate\\ndrought\",\n           size = 4, color = \"#DE4968FF\", family = \"Avenir\") +\n  annotate(\"text\", x = as.Date('2020/12/15'), y = 4, label = \"abnormally\\ndry\",\n           size = 4, color = \"#FE9F6DFF\", family = \"Avenir\") +\n  meg_theme()+\n  theme(panel.grid.major = element_blank(),\n        legend.position = \"none\")"
  },
  {
    "objectID": "blog/2022-02-22-increasing-the-flexibility-and-robustness-of-plots-in-ggplot2/index.html",
    "href": "blog/2022-02-22-increasing-the-flexibility-and-robustness-of-plots-in-ggplot2/index.html",
    "title": "Increasing the Flexibility and Robustness of Plots in ggplot2",
    "section": "",
    "text": "There is definitely a steep learning curve to using ggplot2 to make plots in R, and in my experience, there are several stages to that learning curve:\nStage 1: Have to look up how to code every single element whenever you’re just trying to make a f$*&ing bar graph.\nStage 2: Can make basic plots without looking things up!\nStage 3: Have learned about labels and themes and reference lines and annotations, and plots start to look much better.\nStage 4: Can make plots that are customized but in a flexible way so they’re robust to new and/or changing data.\nStage 5: Have learned all there is to know about know about ggplot2 and never have to google a thing. (I have not reached this stage so cannot independently confirm that it exists.)\nThis post is for people in stage 3 who would like to move into stage 4. Stage 3, where you’re learning to take advantage of all the customization that ggplot2 offers, is an awesome stage. That’s where you learn all about how to layer different plot elements together to get a plot that looks exactly how you want.\nBut a custom plot that is hard-coded to work perfectly for one subset of data, like a certain species of penguin or a certain sports team, might not look good if that data gets updated or if you want to use another group instead. This post will introduce a few techniques, like mapping more data elements to visual properties and using the glue function to combine string text with R expressions, to handle customizations in a more flexible way.\nThe end goal of this post will be a single function to create the following plot for any NHL team. (Hockey knowledge and/or interest is not at all required for this example. Those interested in data viz only can proceed straight to the ggplot2 code, while hockey folks can make a pit stop at the hockeyR code.)"
  },
  {
    "objectID": "blog/2022-02-22-increasing-the-flexibility-and-robustness-of-plots-in-ggplot2/index.html#getting-the-data-ready-to-plot",
    "href": "blog/2022-02-22-increasing-the-flexibility-and-robustness-of-plots-in-ggplot2/index.html#getting-the-data-ready-to-plot",
    "title": "Increasing the Flexibility and Robustness of Plots in ggplot2",
    "section": "Getting the data ready to plot",
    "text": "Getting the data ready to plot\nThe plot we’ll be making today requires four packages:\nlibrary(tidyverse)\nlibrary(hockeyR)\nlibrary(cowplot)\nlibrary(glue)\nIf you’re familiar with ggplot2, the tidyverse presumably needs no explanation. It will load not only ggplot2 but also packages like tidyr, dplyr, and stringr, which have handy functions that we need to clean and prep this data.\nhockeyR is a very useful package for loading NHL data—play-by-play data most notably, but also roster data and other useful information.\nThe cowplot package by Claus Wilke is used to combine plots, add images to plots, etc. I wrote a brief introduction here.\nglue makes it easy to combine literal strings with R expressions and/or data items.\nThe data from hockeyR requires quite a bit of prep to get it into a structure that we want for this plot. All of that code is available by expanding the following section:\n\n\nData cleaning/prep code:\n\n  todays_theme &lt;- function () { \n    theme_linedraw(base_size=11, base_family=\"Avenir\") %+replace% \n      theme(\n        panel.background  = element_blank(),\n        plot.background = element_rect(fill = \"transparent\", color = NA), \n        legend.background = element_rect(fill = \"transparent\", color = NA),\n        legend.key = element_rect(fill = \"transparent\", color = NA),\n        axis.ticks = element_blank(),\n        panel.grid.major = element_line(color = \"grey90\", size = 0.3), \n        panel.grid.minor = element_blank()\n      )\n  }\n  \n  # get pbp data, roster info, and team info from hockeyR\n  \n  data &lt;- load_pbp(2022)\n  roster &lt;- get_rosters(team = \"all\", season = 2022)\n  team_info &lt;- team_logos_colors\n  \n  position &lt;- roster %&gt;% \n    select(player, position) %&gt;% \n    unique() %&gt;% \n    mutate(position = case_when(position == \"F\" ~ \"forward\", \n                                TRUE ~ \"defenseman\"))\n  \n  # calculate the total points for each player\n  \n  points &lt;- data %&gt;% \n    # want all goals except for those in the shootout\n    filter(event_type == \"GOAL\" & period != 5) %&gt;% \n    # event players 1-3 are those who might get points on each goal\n    select(game_id, game_date, team = event_team_abbr, event_player_1_name, \n           event_player_2_name, event_player_3_name, event_player_1_type, \n           event_player_2_type, event_player_3_type) %&gt;% \n    # this will create a name/type combo for players 1 through 3\n    pivot_longer(event_player_1_name:event_player_3_type,\n                 names_to = c(NA, \".value\"),\n                 names_pattern = \"(.+)_(.+)\") %&gt;% \n    # only want the players who either scored the goal or got an assist\n    filter(type %in% c(\"Assist\", \"Scorer\")) %&gt;% \n    count(name, team, name = \"points\") %&gt;% \n    mutate(name = str_replace_all(name, \"\\\\.\", \" \"))\n  \n  # calculate TOI for each player\n  \n  TOI &lt;- data %&gt;% \n    # create a variable for the length of each event\n    mutate(length = case_when(lead(game_id) == game_id ~ \n                                lead(period_seconds) - period_seconds,\n                              TRUE ~ 0)) %&gt;% \n    select(length, game_id, home_abbreviation, away_abbreviation, \n           home_on_1:away_on_7) %&gt;% \n    filter(length &gt; 0) %&gt;% \n    pivot_longer(home_on_1:away_on_7,\n                 names_to = \"team\",\n                 values_to = \"name\") %&gt;% \n    filter(!is.na(name)) %&gt;% \n    mutate(team = ifelse(str_detect(team, \"home\"), home_abbreviation, \n                         away_abbreviation)) %&gt;% \n    select(-c(3:4)) %&gt;% \n    group_by(name, team) %&gt;% \n    # calculate total TOI and games played\n    summarize(TOI = sum(length) / 60,\n              GP = n_distinct(game_id)) %&gt;% \n    mutate(name = str_replace_all(name, \"\\\\.\", \" \")) %&gt;% \n    filter(TOI &gt; 200 & !(name %in% c(\"Marc Andre Fleury\",\n                                   \"Ukko Pekka Luukkonen\"))) %&gt;% \n    # need to make some name adjustments to account for discrepancies in the data\n    mutate(name = case_when(name == \"Drew O Connor\" ~ \"Drew O'Connor\",\n                            name == \"Logan O Connor\" ~ \"Logan O'Connor\",\n                            name == \"Liam O Brien\" ~ \"Liam O'Brien\",\n                            name == \"Ryan O Reilly\" ~ \"Ryan O'Reilly\",\n                            name == \"Jean Gabriel Pageau\" ~ \"Jean-Gabriel Pageau\",\n                            name == \"K Andre Miller\" ~ \"K'Andre Miller\",\n                            name == \"Marc Edouard Vlasic\" ~ \"Marc-Edouard Vlasic\",\n                            name == \"Pierre Edouard Bellemare\" ~ \"Pierre-Edouard Bellemare\",\n                            name == \"Nicolas Aube Kubel\" ~ \"Nicolas Aube-Kubel\",\n                            name == \"Oliver Ekman Larsson\" ~ \"Oliver Ekman-Larsson\",\n                            name == \"Pierre Luc Dubois\" ~ \"Pierre-Luc Dubois\",\n                            name == \"Ryan Nugent Hopkins\" ~ \"Ryan Nugent-Hopkins\",\n                            name == \"Zach Aston Reese\" ~ \"Zach Aston-Reese\",\n                            TRUE ~ name)) %&gt;% \n    # join in points data to calculate rate\n    left_join(points, by = c(\"name\", \"team\")) %&gt;% \n    mutate(points = replace_na(points, 0),\n           pts_per_60 = points * 60 / TOI)\n  \n  top_points &lt;- TOI %&gt;% \n    left_join(position, by = c(\"name\" = \"player\")) %&gt;% \n    # filter to only the top 10 players per team\n    group_by(team) %&gt;% \n    top_n(10, pts_per_60) %&gt;% \n    left_join(select(team_info, full_team_name, team = team_abbr),\n              by = \"team\")"
  },
  {
    "objectID": "blog/2022-02-22-increasing-the-flexibility-and-robustness-of-plots-in-ggplot2/index.html#a-hard-coded-plot",
    "href": "blog/2022-02-22-increasing-the-flexibility-and-robustness-of-plots-in-ggplot2/index.html#a-hard-coded-plot",
    "title": "Increasing the Flexibility and Robustness of Plots in ggplot2",
    "section": "A hard-coded plot",
    "text": "A hard-coded plot\n\nThe goal here is to create a plot that shows the top scorers (by scoring rate, or points per 60 minutes) for a single NHL team this season. The prep code in the previous section ends with a data frame called top_points, which has 320 observations (10 players per team) and several variables, including the player’s team, position, games played, and points rate.\nThis is a selection of a single observation from the top_points data frame:\n\n\n\nname\nteam\nGP\npts_per_60\nposition\nfull_team_name\n\n\n\n\nClayton Keller\nARI\n46\n2.625\nforward\nArizona Coyotes\n\n\n\nWhat are some of the specific customizations that have been used in the code below to generate the above plot?\nThe title argument within the labs layer specifies the “Arizona Coyotes.”\nIn order to avoid a color legend, a specific forward and a specific defenseman are labeled as such (in the first mutate function). Those labels are applied in the first geom_text layer.\nThere is an extra bit of info, the games played, added to the right side of this plot as an annotation. The first annotate layer places the “games” label at a specific x value, and the geom_label layer right below places that games played data at that same x value.\nThe second annotate layer includes some text on the team’s record and division standing.\nThere’s a reference line, the geom_vline layer, to represent the team average scoring rate.\nThere’s a logo in the upper-left corner, added by specifying the image URL in cowplot’s draw_image function.\nsingle_team &lt;- top_points %&gt;% \n  filter(team == \"ARI\")\n\nplot &lt;- single_team %&gt;% \n  # creates a \"position\" label for select players\n  mutate(label = case_when(str_detect(name, \"Keller\") | \n                             str_detect(name, \"Shayne\") ~ position,\n                           TRUE ~ NA_character_)) %&gt;% \n  # creates a bar graph with scoring rate on x, name on y, colored by position\n  ggplot(aes(x = pts_per_60, y = reorder(name, pts_per_60), fill = position)) +\n  geom_bar(stat = \"identity\") +\n  # specifies the bar colors\n  scale_fill_manual(values = c(\"#AD9490\",\"#E2DED4\")) +\n  # creates an annotation for games played data on the right side of the plot\n  annotate(\"text\", x = 3, y = 11, vjust = 1.5,\n           label = \"games\", size = 3.5, family = \"Avenir\") +\n  geom_label(data = single_team, aes(x = 3, label = GP), fill = \"#d3d3d3\",\n             family = \"Avenir\") +\n  labs(x = \"Points per 60 minutes\",\n       y = \"\",\n       title = \"Scoring rate leaders (200+ min.): Arizona Coyotes\",\n       subtitle = \"2021-22 season, through Feb. 10\") +\n  # adds a dashed reference line with a label\n  geom_vline(xintercept = 1.25, linetype = \"dashed\", color = \"#a39d9d\") +\n  annotate(\"text\", x = 1.25, y = 11, label = \"team avg.\", size = 3, \n           hjust = -0.1, vjust = 1.5, family = \"Avenir\") +\n  # adds the two position labels\n  geom_text(aes(label = label), x = 0.1, hjust = 0, family = \"Avenir\") +\n  # labels the inside of the bar with each player's scoring rate\n  geom_text(aes(label = round(pts_per_60, 1)), vjust = 0.5, hjust = 1.5,\n                       family = \"Avenir\") +\n  # adds the standings information to the lower right corner of the plot\n  annotate(\"text\", x = 2.4, y = 3, \n           label = \"record: 12-31-4\\ndivision rank: 8\", family = \"Avenir\") +\n  # places the bar right up against the y-axis, with a bit of space to the right\n  scale_x_continuous(expand = expansion(mult = c(0, .1))) +\n  todays_theme() +\n  theme(panel.grid.major.y = element_blank(),\n        legend.position = \"none\")\n\nggdraw() +\n  draw_plot(plot) +\n  draw_image(\"https://a.espncdn.com/i/teamlogos/nhl/500/ARI.png\",\n             x = -0.35, y = 0.42, scale = 0.15)\nNow, this could be fine if this was the only plot you ever wanted to create with this data. But what happens if, in that single_team data frame that feeds the plot, you replace ARI with a different team?"
  },
  {
    "objectID": "blog/2022-02-22-increasing-the-flexibility-and-robustness-of-plots-in-ggplot2/index.html#strengthening-your-code",
    "href": "blog/2022-02-22-increasing-the-flexibility-and-robustness-of-plots-in-ggplot2/index.html#strengthening-your-code",
    "title": "Increasing the Flexibility and Robustness of Plots in ggplot2",
    "section": "Strengthening your code",
    "text": "Strengthening your code\n\nRunning that exact same code and substituting the Colorado Avalanche for the Arizona Coyotes results in the above plot. And it’s not pretty! The mapped data (i.e., the players, the length and label of the bars, and the games played data) looks fine. But everything else that we customized via hard-coding looks terrible and/or is flat-out wrong. The logo, the title, the placement of the annotations, the text in the annotation, the reference line…all off. You could copy-and-paste the original code and update all the things hard-coded in the previous section for Arizona to instead work for Colorado, but please don’t. Each of these items can be adjusted to pull from the data and update automatically, like the basic data did here.\n\nThe title\nold code\ntitle = \"Scoring rate leaders (200+ min.): Arizona Coyotes\"\nglue is an extremely useful function for combining text with R expressions and/or variables. Instead of hard-coding “Arizona Coyotes” like in the old code above, we can simply replace the specific team name with {unique(single_team$full_team_name)}, which will pull the full_team_name variable from the single_team data frame.\nnew code\ntitle = glue::glue(\"Scoring rate leaders (200+ min.): {unique(single_team$full_team_name)}\")\n(Note: this particular example can also be achieved with a base function like paste0. I personally find the glue syntax easier to read since you can drop the R syntax, enclosed by curly brackets, right into the string itself. The glue package also has also useful functions like glue_data and glue_collapse.)\n\n\nThe color “legend”\nmutate(label = case_when(str_detect(name, \"Keller\") | \n                         str_detect(name, \"Shayne\") ~ position,\n                         TRUE ~ NA_character_))\n                         \ngeom_text(aes(label = label), x = 0.1, hjust = 0, family = \"Avenir\")\nIn this plot, the bars are colored by the player’s position. In order to avoid a color legend, in the old code above, a mutate function created a new variable called label to list the position for the first forward (Clayton Keller) and the first defenseman (Shayne Gostisbehere). Then those labels are applied with a geom_text layer.\nThis method obviously does not work if those players change! Instead, group the data by position and update the conditional statement of the label variable to only list the position if the scoring rate is equal to the maximum scoring rate for that position. That way the top player for each position is identified automatically instead of exactly specified. The label is applied in the exact same way with geom_text.\ngroup_by(position) %&gt;% \nmutate(label = ifelse(pts_per_60 == max(pts_per_60), position, NA)) %&gt;% \nungroup()\n                         \ngeom_text(aes(label = label), x = 0.1, hjust = 0, family = \"Avenir\")\n\n\nThe annotation placements\nannotate(\"text\", x = 3, y = 11, vjust = 1.5,\n         label = \"games\", size = 3.5, family = \"Avenir\")\n           \ngeom_label(data = single_team, aes(x = 3, label = GP), fill = \"#d3d3d3\",\n           family = \"Avenir\") +\nThis plot has extra data on the right side to show each player’s games played. The title of that column “games” is set via an annotate layer, while the data itself is added with a geom_label layer. The games played data is mapped to the single_team data frame, via the label = GP argument, so that piece updates automatically. It’s the x = 3 argument in both of these layers that does not work—that placement was appropriate in the first example, but the scoring rates for this team are higher, so keeping the placement at x = 3 here overlaps the bars.\nJust from looking at this graph, it seems like a placement of x = 5 would be more appropriate in this scenario. But what are we basing that on? The maximum value of the scoring rate, as we want this annotation to sit to the right of the longest bar. This can be done, as in the code below, by setting x equal to the maximum value, max(single_team$pts_per_60), multiplied by some figure to give the plot a bit of breathing room. You would experiment with the multiplier to figure out what works for your data, but in this case, a multiplier of 1.1 works well. So for both the annotate layer to place the title and the geom_label layer to place the data, x is set to max(single_team$pts_per_60) * 1.1.\nannotate(\"text\", x = max(single_team$pts_per_60) * 1.1, y = 11, vjust = 1.5,\n         label = \"games\", size = 3.5, family = \"Avenir\")\n           \ngeom_label(data = single_team, aes(x = max(single_team$pts_per_60) * 1.1, \n                                   label = GP), \n           fill = \"#d3d3d3\",family = \"Avenir\")\n\n\nThe annotation text\nannotate(\"text\", x = 2.4, y = 3, \n         label = \"record: 12-31-4\\ndivision rank: 8\", family = \"Avenir\")\nThis plot has another annotation, ideally in the lower-right corner, that lists the team’s record and its rank within its division. In the first example, this was hard-coded with simple text. At the time, the Coyotes’ record was 12-31-4, and they were ranked 8th in their division. But with the play-by-play data that we have, it’s possible to calculate each team’s record and its division standing. That code is below, if you’re interested.\n\n\nCode to get the records:\n\n  # calculate the result of the shootouts by which team has more goals\n  SO &lt;- data %&gt;% \n    group_by(game_id) %&gt;% \n    filter(max(period) == 5) %&gt;% \n    filter(event_type == \"GOAL\") %&gt;% \n    count(game_id, event_team_type) %&gt;% \n    pivot_wider(names_from = event_team_type, values_from = n) %&gt;% \n    mutate(SO_result = ifelse(home &gt; away, \"home\", \"away\"))\n  \n  records &lt;- data %&gt;% \n    # calculate the home and away score and game type, by how many game periods\n    # 3 periods: regulation; 4: overtime; 5: shootout\n    group_by(game_id, home_abbreviation, away_abbreviation, home_division_name,\n             away_division_name) %&gt;% \n    summarize(home = max(home_final),\n              away = max(away_final),\n              period = max(period)) %&gt;% \n    left_join(select(SO, -c(2:3)), by = \"game_id\") %&gt;% \n    # get standings points per game: 2 for win, 1 for OT/SO loss, 0 for reg loss\n    mutate(home = ifelse(is.na(SO_result) | SO_result == \"away\", home, home + 1),\n           away = ifelse(is.na(SO_result) | SO_result == \"home\", away, away + 1),\n           home_points = case_when(home &gt; away ~ 2,\n                                   home &lt; away & period &gt; 3 ~ 1,\n                                   TRUE ~ 0),\n           away_points = case_when(away &gt; home ~ 2,\n                                   away &lt; home & period &gt; 3 ~ 1,\n                                   TRUE ~ 0)) %&gt;% \n    select(-c(home:SO_result)) %&gt;% \n    pivot_longer(home_abbreviation:away_points,\n                 names_to = c(NA, \".value\"),\n                 names_sep = 5) %&gt;% \n    mutate(win = ifelse(points == 2, 1, 0),\n           loss = ifelse(points == 0, 1, 0),\n           OT = ifelse(points == 1, 1, 0)) %&gt;% \n    group_by(abbreviation, division_name) %&gt;% \n    # sum the points per team per division\n    summarize(wins = sum(win),\n              losses = sum(loss),\n              OT = sum(OT),\n              points = sum(points)) %&gt;% \n    # create the record text that will be used on the plot\n    mutate(record = str_c(wins, losses, OT, sep = \"-\")) %&gt;% \n    group_by(division_name) %&gt;% \n    arrange(division_name, desc(points)) %&gt;% \n    # calculate the division rank\n    mutate(rank = row_number())\n  \n  team_record &lt;- records %&gt;% \n    filter(abbreviation == \"COL\")\n\nThe code above results in a data frame called team_record, which looks in part like this:\n\n\n\nabbreviation\nrecord\npoints\nrank\n\n\n\n\nCOL\n33-8-4\n70\n1\n\n\n\nNow that this data exists in a data frame, we can again use glue to create the text for the annotate layer. And to correctly place the annotation on the plot, we can set the x using the same method discussed in the previous section.\nannotate(\"text\", x = max(single_team$pts_per_60) * 0.8, y = 3, \n         label = glue::glue(\"record: {team_record$record}\n                            division rank: {team_record$rank}\"), \n         family = \"Avenir\") +\n\n\nThe reference line\ngeom_vline(xintercept = 1.25, linetype = \"dashed\", color = \"#a39d9d\")\n\nannotate(\"text\", x = 1.25, y = 11, label = \"team avg.\", size = 3, \n           hjust = -0.1, vjust = 1.5, family = \"Avenir\") +\nThis plot includes a reference line to show the team average scoring rate. In the hard-coded example, the reference line and its label are both placed at the exact value of the line: xintercept = 1.25 and x = 1.25. But of course this value will shift if a different team is selected, so it’s easier to map it directly to the data.\nWe can create another new data frame called pts_rate_avg that will calculate avg, or the average scoring rate per team. That value can then define the xintercept in the geom_vline layer and x in the annotate layer.\n(You could avoid creating an extra data frame and instead set the x and xintercept equal to (TOI %&gt;% group_by(team) %&gt;% summarize(avg = mean(pts_per_60)) %&gt;% filter(team == \"COL\"))$avg), but I personally think that’s a bit harder to read.)\npts_rate_avg &lt;- TOI %&gt;% \n  group_by(team) %&gt;% \n  summarize(avg = mean(pts_per_60)) %&gt;% \n  filter(team == \"COL\")\n  \ngeom_vline(xintercept = pts_rate_avg$avg, \n             linetype = \"dashed\", color = \"#a39d9d\")\n             \nannotate(\"text\", x = pts_rate_avg$avg, y = 11, label = \"team avg.\", size = 3, \n           hjust = -0.1, vjust = 1.5, family = \"Avenir\") +\n\n\nThe logo\nggdraw() +\n  draw_plot(plot) +\n  draw_image(\"https://a.espncdn.com/i/teamlogos/nhl/500/ARI.png\",\n             x = -0.35, y = 0.42, scale = 0.15)\nIn the hard-coded example, we used draw_image from the cowplot package to add the team’s logo, via a specific URL. But luckily, the team_info data frame that we got from the hockeyR package back in the initial data prep phase holds the logo URL for each team. A left_join will add that variable, team_logo_espn, into the team_record data frame that we created previously, and then team_record$team_logo_espn can be specified in draw_image instead of the specific URL.\nteam_record &lt;- records %&gt;% \n  filter(abbreviation == \"COL\") %&gt;% \n  left_join(select(team_info, team_abbr, team_logo_espn),\n            by = c(\"abbreviation\" = \"team_abbr\"))\n            \nggdraw() +\n  draw_plot(plot) +\n  draw_image(team_record$team_logo_espn,\n             x = -0.37, y = 0.42, scale = 0.15)\n\n\nFull plot code:\n\n  plot &lt;- single_team %&gt;% \n    group_by(position) %&gt;% \n    mutate(label = ifelse(pts_per_60 == max(pts_per_60), position, NA)) %&gt;% \n    ungroup() %&gt;%\n    ggplot(aes(x = pts_per_60, y = reorder(name, pts_per_60), fill = position)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"#AD9490\",\"#E2DED4\")) +\n    annotate(\"text\", x = max(single_team$pts_per_60) * 1.1, y = 11, vjust = 1.5,\n             label = \"games\", size = 3.5, family = \"Avenir\") +\n    geom_label(data = single_team, aes(x = max(single_team$pts_per_60) * 1.1, \n                                       label = GP), \n               fill = \"#d3d3d3\",family = \"Avenir\") +\n    labs(x = \"Points per 60 minutes\",\n         y = \"\",\n         title = glue::glue(\"Scoring rate leaders (200+ min.): {unique(single_team$full_team_name)}\"),\n         subtitle = \"2021-22 season, through Feb. 10\") +\n    geom_vline(xintercept = pts_rate_avg$avg, \n               linetype = \"dashed\", color = \"#a39d9d\") +\n    annotate(\"text\", x = pts_rate_avg$avg, y = 11, label = \"team avg.\", size = 3, \n             hjust = -0.1, vjust = 1.5, family = \"Avenir\") +\n    geom_text(aes(label = label), x = 0.1, hjust = 0, family = \"Avenir\") +\n    geom_text(aes(label = round(pts_per_60, 1)), vjust = 0.5, hjust = 1.5,\n              family = \"Avenir\") +\n    annotate(\"text\", x = max(single_team$pts_per_60) * 0.8, y = 3, \n             label = glue::glue(\"record: {team_record$record}\n                                division rank: {team_record$rank}\"), \n             family = \"Avenir\") +\n    scale_x_continuous(expand = expansion(mult = c(0, .1))) +\n    todays_theme() +\n    theme(panel.grid.major.y = element_blank(),\n          legend.position = \"none\")\n  \n  ggdraw() +\n    draw_plot(plot) +\n    draw_image(team_record$team_logo_espn,\n               x = -0.37, y = 0.42, scale = 0.15)\n\nThanks to all of those adjustments (which are incorporated in the full plot code above), changing the team from the Arizona Coyotes to the Colorado Avalanche results in the plot below, which looks much better than the first attempt."
  },
  {
    "objectID": "blog/2022-02-22-increasing-the-flexibility-and-robustness-of-plots-in-ggplot2/index.html#creating-a-function",
    "href": "blog/2022-02-22-increasing-the-flexibility-and-robustness-of-plots-in-ggplot2/index.html#creating-a-function",
    "title": "Increasing the Flexibility and Robustness of Plots in ggplot2",
    "section": "Creating a function",
    "text": "Creating a function\nThe code in the previous section works perfectly well to create this plot. However, there are still three separate data frames where the team has to be specified: the main data frame single_team, the data frame with the win-loss record and standing info team_record, and the data frame with the averages to create the reference line pts_rate_avg. That kind of structure begs for a user-defined function—I’ve created plot_fn below, which means creating this plot for any team is as simple as plot_fn(\"CGY\").\n\nplot_fn &lt;- function(team_name) {\n  \n  single_team &lt;- top_points %&gt;% \n    filter(team == team_name)\n  \n  team_record &lt;- records %&gt;% \n    filter(abbreviation == team_name) %&gt;% \n    left_join(select(team_info, team_abbr, team_logo_espn),\n              by = c(\"abbreviation\" = \"team_abbr\"))\n  \n  pts_rate_avg &lt;- TOI %&gt;% \n    group_by(team) %&gt;% \n    summarize(avg = mean(pts_per_60)) %&gt;% \n    filter(team == team_name)\n  \n  plot &lt;- single_team %&gt;% \n    group_by(position) %&gt;% \n    mutate(label = ifelse(pts_per_60 == max(pts_per_60), position, NA)) %&gt;% \n    ungroup() %&gt;%\n    ggplot(aes(x = pts_per_60, y = reorder(name, pts_per_60), fill = position)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"#AD9490\",\"#E2DED4\")) +\n    annotate(\"text\", x = max(single_team$pts_per_60) * 1.1, y = 11, vjust = 1.5,\n             label = \"games\", size = 3.5, family = \"Avenir\") +\n    geom_label(data = single_team, aes(x = max(single_team$pts_per_60) * 1.1, \n                                       label = GP), \n               fill = \"#d3d3d3\",family = \"Avenir\") +\n    labs(x = \"Points per 60 minutes\",\n         y = \"\",\n         title = glue::glue(\"Scoring rate leaders (200+ min.): {unique(single_team$full_team_name)}\"),\n         subtitle = \"2021-22 season, through Feb. 10\") +\n    geom_vline(xintercept = pts_rate_avg$avg, \n               linetype = \"dashed\", color = \"#a39d9d\") +\n    annotate(\"text\", x = pts_rate_avg$avg, y = 11, label = \"team avg.\", size = 3, \n             hjust = -0.1, vjust = 1.5, family = \"Avenir\") +\n    geom_text(aes(label = label), x = 0.1, hjust = 0, family = \"Avenir\") +\n    geom_text(aes(label = round(pts_per_60, 1)), vjust = 0.5, hjust = 1.5,\n              family = \"Avenir\") +\n    annotate(\"text\", x = max(single_team$pts_per_60) * 0.8, y = 3, \n             label = glue::glue(\"record: {team_record$record}\n                              division rank: {team_record$rank}\"), \n             family = \"Avenir\") +\n    scale_x_continuous(expand = expansion(mult = c(0, .1))) +\n    todays_theme() +\n    theme(panel.grid.major.y = element_blank(),\n          legend.position = \"none\")\n  \n  ggdraw() +\n    draw_plot(plot) +\n    draw_image(team_record$team_logo_espn,\n               x = -0.37, y = 0.42, scale = 0.15)\n  \n}"
  },
  {
    "objectID": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html",
    "href": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html",
    "title": "Making Slides in Quarto with reveal.js",
    "section": "",
    "text": "Quarto is a new open-source technical publishing system from RStudio. It is similar in many ways to R Markdown (which isn’t going away, don’t worry!), except that it doesn’t require R, supports more languages, and combines the functionality of many R Markdown packages (e.g., xaringan, bookdown).\nI am a devoted R Markdown user, but I want to start exploring the new Quarto features. My first project was slides for a workshop—I would normally use the R Markdown-backed xaringan for HTML presentations, but I decided to try out the Quarto alternative, which uses reveal.js.\n\nThe slides I made with Quarto are here, and the code is on GitHub here.\n\nA lot of the functionality is similar to using xaringan or other similar R Markdown HTML formats, but there’s lots of new syntax to learn (as well as plenty of new features!), so I put this post together to compile a few things I learned while making a Quarto presentation for the first time."
  },
  {
    "objectID": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html#my-quarto-journey-begins",
    "href": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html#my-quarto-journey-begins",
    "title": "Making Slides in Quarto with reveal.js",
    "section": "",
    "text": "Quarto is a new open-source technical publishing system from RStudio. It is similar in many ways to R Markdown (which isn’t going away, don’t worry!), except that it doesn’t require R, supports more languages, and combines the functionality of many R Markdown packages (e.g., xaringan, bookdown).\nI am a devoted R Markdown user, but I want to start exploring the new Quarto features. My first project was slides for a workshop—I would normally use the R Markdown-backed xaringan for HTML presentations, but I decided to try out the Quarto alternative, which uses reveal.js.\n\nThe slides I made with Quarto are here, and the code is on GitHub here.\n\nA lot of the functionality is similar to using xaringan or other similar R Markdown HTML formats, but there’s lots of new syntax to learn (as well as plenty of new features!), so I put this post together to compile a few things I learned while making a Quarto presentation for the first time."
  },
  {
    "objectID": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html#quarto-resources",
    "href": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html#quarto-resources",
    "title": "Making Slides in Quarto with reveal.js",
    "section": "Quarto resources",
    "text": "Quarto resources\nMy favorite introduction to the publishing system itself is Alison Hill’s comprehensive post. Mine Çetinkaya-Rundel also has an ongoing blog with a new Quarto tip each day.\nFor specifics about making presentations with reveal.js in Quarto, the docs, demo presentation, and demo presentation code are all incredibly useful."
  },
  {
    "objectID": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html#emojis",
    "href": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html#emojis",
    "title": "Making Slides in Quarto with reveal.js",
    "section": "Emojis",
    "text": "Emojis\nFirst things first—how to use emojis in Quarto?? Thankfully, this was an easy one. By adding the following line to your YAML:\n---\nfrom: markdown+emoji\n---\nYou can reference emojis in text by writing :wave: to get 👋. The full list of emoji codes is available here.\n(This is a good list of what overall YAML options are available.)"
  },
  {
    "objectID": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html#custom-elements",
    "href": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html#custom-elements",
    "title": "Making Slides in Quarto with reveal.js",
    "section": "Custom elements",
    "text": "Custom elements\nThere are built-in presentation themes within Quarto, and just like with xaringan, you have the option to customize one of the provided themes or completely build your own. To do so, create a custom.scss file and reference it in your YAML:\n---\nformat: \n  revealjs:\n    theme: [moon, custom.scss]\n---\nMy custom.scss file that I used for my slides is here, and in addition to some “common” customizations like specifying background color, font color, link color, etc., there were a few specific aspects I wanted to change or add.\n\nCode block appearance\nBy default, code blocks in Quarto have the same background color as the slides themselves, with a lightened border (all of the default elements are listed here). I wanted a lighter background for my code block, with a dark border, so I made the following changes to my .scss file:\n$code-block-bg: #ded9ca;\n$code-block-border-color: #000000;\n\n\n\nFooter text\nThe default footer in Quarto reveal.js slides has text centered at the bottom of the page, but I wanted my footer to have smaller text that was aligned to the right. This seemed to be a very simple customization, but tinkering with the .footer class in my .scss file was not working and I couldn’t figure out why.\nLater, after I had given up 😑, I was browsing through the reveal.js GitHub issues (another technique I recommend for learning!) and happened to stumble upon one that just so happened to contain the solution to my footer problem! The default footer.css file is part of a reveal.js plugin that loads after any theme or custom files. And that’s how I learned about the !important property! Adding that property to my elements allowed me to finally edit the footer. As JJ wrote in that issue, this is “coloring outside the lines.”\n.reveal .footer {\n  font-size: 0.35em !important;\n  text-align: right !important;\n}\n\n\nSection header\nThe slides I developed have different sections, and I wanted to design something to sit in the top-right corner that would indicate the current section. To do so, I created a new class in my .scss file called .sectionhead.\n\nI wish I had a more specific recommendation for learning CSS/SCSS (please share if you have a favorite resource!), but everything I’ve learned I’ve gotten through either googling or by looking through the files of other people’s work that I like. Figuring out how to do this .sectionhead was searching how to make a text box in CSS and lots of playing around with the different elements until I got something I liked.\n.sectionhead {\n  font-size: 1em;\n  text-align: center;\n  color: $presentation-heading-color;\n  font-family: $presentation-heading-font;\n  background-color: $body-bg;\n  margin: 1px;\n  padding: 2px 2px 2px 2px;\n  width: 120px;\n  border-left: 10px solid;\n}\n\n\nFinding elements to customize\nThis is very specific and random, but there is a feature in Quarto slides that fades out text on a click. It appears to fade out by ~50%, but I wanted it to fade out even more. But where do you even start in figuring out how to customize that?\nTo reference the feature in slides, you use .fragment.semi-fade-out. So I searched the Quarto repo for that exact string and found it here, in the main .scss file for reveal. That file provided the default syntax, which I was able to copy into my custom.scss file and then edit to make the change I wanted to the opacity.\n.reveal .slides section .fragment.semi-fade-out {\n    opacity: 1;\n    visibility: inherit;\n\n    &.visible {\n        opacity: 0.25;\n        visibility: inherit;\n    }\n}"
  },
  {
    "objectID": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html#absolute-position",
    "href": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html#absolute-position",
    "title": "Making Slides in Quarto with reveal.js",
    "section": "Absolute position",
    "text": "Absolute position\nOne of the most useful features of Quarto reveal.js slides is absolute position, which allows you to specifically place elements on a slide.\n\nI used this to place images, of course, but I also used it to annotate screenshots and code with text (like in the example above), place my .sectionhead on each slide, and in cases like the slide below, add a little outline for emphasis.\n\nThese elements can also be added as fragments, i.e., to show up on a click. The following code places those two boxes (which I created with a new .blackbox class in my .scss file) on the slide. The . . . at the top of this code chunk indicates that they appear on a click after the rest of the slide code. You could also put a . . . between those two code sections to have the boxes show up on separate clicks.\n. . .\n\n::: {.absolute top=\"42%\" left=\"4%\" width=\"150\"}\n::: {.blackbox}\n:::\n:::\n\n::: {.absolute top=\"42%\" left=\"69%\" width=\"185\"}\n::: {.blackbox}\n:::\n:::"
  },
  {
    "objectID": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html#code-customizations",
    "href": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html#code-customizations",
    "title": "Making Slides in Quarto with reveal.js",
    "section": "Code customizations",
    "text": "Code customizations\nCode blocks in reveal.js allow for a lot of customizations, and these are the two I found most useful.\n\nCode folding\n\nFor some examples while teaching, I want the code available but don’t necessarily need it all on the screen, which could be unwieldy and/or distracting. The code-fold option is great for this, as the code is only available “on demand,” and you can even specify what text should be shown next to the arrow. The code below shows the code-fold and code-summary options in use.\nSide note: I like how the chunk options here are in the chunk itself, preceded by #|, instead of in the {r} brackets—it’s much more readable.\n\n\nexpand for full code\n```{r}\n#| code-fold: true\n#| code-summary: \"expand for full code\"\n#| fig-align: \"center\"\nfac_enr %&gt;% \n  filter(!is.na(avg_enr)) %&gt;% \n  ggplot(aes(x = year, y = avg_enr, group = rank, color = rank)) +\n  geom_line() +\n  geom_point() +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  labs(x = NULL, y = \"Average enrollment\",\n       title = \"Average undergraduate enrollment per rank over time\") +\n  theme_linedraw() +\n  theme(panel.grid.major.x = element_blank(),\n        axis.ticks = element_blank(),\n        legend.title = element_blank(),\n        legend.background = element_rect(fill = NA),\n        legend.key = element_rect(fill = NA),\n        legend.position = c(0.85, 0.82))\n```\n\n\n\n\nIncremental code highlighting\n\nCode highlighting isn’t a new feature in and of itself (you could definitely highlight code lines in xaringan), but here it’s very easy to “step through” code by highlighting various lines on a click. This is super useful for focusing attention during teaching and explaining code line-by-line.\nThe following option would start by highlighting lines 1 and 2 of the code block, then line 3 on a click, then line 4 on a click: #| code-line-numbers: \"1-2|3|4\""
  },
  {
    "objectID": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html#styling-text",
    "href": "blog/2022-07-12-making-slides-in-quarto-with-revealjs/index.html#styling-text",
    "title": "Making Slides in Quarto with reveal.js",
    "section": "Styling text",
    "text": "Styling text\nFiguring out how to style text was a bit of a learning curve for me—not because it’s difficult, it’s just different from xaringan and I couldn’t find it well-explained in the docs (though the docs are so comprehensive, I probably just missed it somewhere). The code for the demo presentation was useful for picking up some styling examples. My best overall tip is to keep up with other people’s work and when you spot something you like, find the GitHub file and figure out how they did it!\n\nIn-line\nIf you want, for example, red words, you’d type [red words]{style=\"color:#cc0000\"}.\n\n\nLarger chunks\nIf instead you wanted to adjust the style of a larger chunk of text, indicate the section with ::: (which needs to be echoed at the end) and use the same {style} syntax.\n::: {style=\"font-size: 1.5em; text-align: center\"}\ntext\n\ntext\n\ntext\n:::"
  },
  {
    "objectID": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html",
    "href": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html",
    "title": "Creating Custom Color Palettes with ggplot2",
    "section": "",
    "text": "One of the perks of making data visualizations in ggplot2 is that nearly everything is easily customizable, meaning your plots can stick to a common style with the fonts and elements specified by your organization’s or your own personal style guide (hopefully you have a custom theme to make this easier!). When it comes to using colors in data visualizations, if you find yourself typing in the same hex codes often enough that you start to memorize them—speaking from experience here—it’s probably time to codify those colors and palettes so you can reference them more easily when creating plots. The examples below will show you how to specify colors and palettes and then use those palettes to create your own scale_color and scale_fill functions for use in plots, for both discrete and continuous data.\nThis blog post by Simon Jackson and this one by Garrick Aden-Buie were both incredibly helpful in helping me figure out how to do this. I somewhat combined their approaches to get this method that worked for my needs, and hopefully either this method or one of theirs is helpful for you!"
  },
  {
    "objectID": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#why-custom-palettes",
    "href": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#why-custom-palettes",
    "title": "Creating Custom Color Palettes with ggplot2",
    "section": "",
    "text": "One of the perks of making data visualizations in ggplot2 is that nearly everything is easily customizable, meaning your plots can stick to a common style with the fonts and elements specified by your organization’s or your own personal style guide (hopefully you have a custom theme to make this easier!). When it comes to using colors in data visualizations, if you find yourself typing in the same hex codes often enough that you start to memorize them—speaking from experience here—it’s probably time to codify those colors and palettes so you can reference them more easily when creating plots. The examples below will show you how to specify colors and palettes and then use those palettes to create your own scale_color and scale_fill functions for use in plots, for both discrete and continuous data.\nThis blog post by Simon Jackson and this one by Garrick Aden-Buie were both incredibly helpful in helping me figure out how to do this. I somewhat combined their approaches to get this method that worked for my needs, and hopefully either this method or one of theirs is helpful for you!"
  },
  {
    "objectID": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#defining-custom-colors-and-palettes",
    "href": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#defining-custom-colors-and-palettes",
    "title": "Creating Custom Color Palettes with ggplot2",
    "section": "Defining custom colors and palettes",
    "text": "Defining custom colors and palettes\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\npenguins &lt;- palmerpenguins::penguins\n\nThese examples use the penguins data set from the wonderful palmerpenguins package. Let’s imagine that you work for Penguin Corp and regularly create reports and visualizations with various penguin-related data. Penguin Corp has a style guide that specifies the colors and palettes that should be used, and while you can always specify the colors manually in ggplot2 visualizations, it’s much easier to define the colors and palettes and then reference them through your own scale_fill and scale_color functions. (These are all functions that can be dropped easily into an internal package!)\nFirst, start by defining the various colors you need.\n\npenguin_corp_color &lt;- function(...) {\n\n  penguin_corp_colors &lt;- c(\n    `pink`     = \"#F7B1AB\",\n    `lavender` = \"#807182\",\n    `gray`     = \"#E3DDDD\",\n    `brown`    = \"#A45C3D\",\n    `purple`   = \"#1C0221\")\n\n  cols &lt;- c(...)\n\n  if (is.null(cols))\n    return (penguin_corp_colors)\n\n  penguin_corp_colors[cols]\n}\n\nThat function can be used to show the definition of a specific color:\n\npenguin_corp_color(\"lavender\")\n\n lavender \n\"#807182\" \n\n\nThat penguin_corp_color function then becomes the base of the penguin_corp_palette function below, where those defined colors are combined into palettes. Your organization might have primary and secondary palettes, or palettes designed for specific uses, but here we’ll define a main palette as well as a highlight palette for when we want just two colors.\n\npenguin_corp_palette &lt;- function(palette = \"main\", ...) {\n\n  penguin_corp_palettes &lt;- list(\n    `main` = penguin_corp_color(\"lavender\", \"gray\", \"pink\", \"brown\"),\n    \n    `highlight` = penguin_corp_color(\"purple\", \"gray\")\n  )\n\n  penguin_corp_palettes[[palette]]\n\n}\n\n\npenguin_corp_palette(\"main\")\n\n lavender      gray      pink     brown \n\"#807182\" \"#E3DDDD\" \"#F7B1AB\" \"#A45C3D\" \n\n\nThe show_col function from the scales package is a nifty way to showcase all the colors available in a given palette:\n\nscales::show_col(penguin_corp_palette(\"main\"), cex_label = 2)"
  },
  {
    "objectID": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#creating-your-own-scale_color-and-scale_fill-functions",
    "href": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#creating-your-own-scale_color-and-scale_fill-functions",
    "title": "Creating Custom Color Palettes with ggplot2",
    "section": "Creating your own scale_color and scale_fill functions",
    "text": "Creating your own scale_color and scale_fill functions\nHaving the colors and palettes defined is a great first step, but you can go even further and apply those into your own scale_fill and scale_color functions. The first step is a helper function, here called palette_gen. This function has two arguments, the name of the palette (“main” will be our default) and the direction (so you can flip the scale if necessary), and essentially creates another function that will be used in the scale_fill and scale_color functions. (The n there refers to the number of colors that would be needed for a particular plot.)\n\npalette_gen &lt;- function(palette = \"main\", direction = 1) {\n\n  function(n) {\n\n    if (n &gt; length(penguin_corp_palette(palette)))\n      warning(\"Not enough colors in this palette!\")\n\n    else {\n\n      all_colors &lt;- penguin_corp_palette(palette)\n\n      all_colors &lt;- unname(unlist(all_colors))\n\n      all_colors &lt;- if (direction &gt;= 0) all_colors else rev(all_colors)\n\n      color_list &lt;- all_colors[1:n]\n\n    }\n  }\n}\n\nThe function above is for discrete color scales. If you also want to use continuous color scales, the function below uses the existing colorRampPalette function to interpolate the necessary colors between the ones you have chosen in your palette.\n\npalette_gen_c &lt;- function(palette = \"main\", direction = 1, ...) {\n\n  pal &lt;- penguin_corp_palette(palette)\n\n  pal &lt;- if (direction &gt;= 0) pal else rev(pal)\n\n  colorRampPalette(pal, ...)\n\n}\n\nWith that helper function created, you can write the actual functions to be used with ggplot2. I’ve called mine scale_fill_penguin, which takes the same two arguments as before: palette and direction.\n\nscale_fill_penguin &lt;- function(palette = \"main\", direction = 1, ...) {\n\n  ggplot2::discrete_scale(\n    \"fill\", \"penguin\",\n    palette_gen(palette, direction),\n    ...\n  )\n}\n\nYou can use the same syntax for scale_color. (Fun fact: I learned from Garrick’s post that the ggplot2 convention is to create a scale_colour function and then replicate it as scale_color.)\n\nscale_colour_penguin &lt;- function(palette = \"main\", direction = 1, ...) {\n\n  ggplot2::discrete_scale(\n    \"colour\", \"penguin\",\n    palette_gen(palette, direction),\n    ...\n  )\n}\n\nscale_color_penguin &lt;- scale_colour_penguin\n\nAgain, those are for discrete color scales. If you need a continuous scale, use a function like this:\n\nscale_color_penguin_c &lt;- function(palette = \"main\", direction = 1, ...) {\n\n  pal &lt;- palette_gen_c(palette = palette, direction = direction)\n\n  scale_color_gradientn(colors = pal(256), ...)\n\n}"
  },
  {
    "objectID": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#using-these-colors-in-plots",
    "href": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#using-these-colors-in-plots",
    "title": "Creating Custom Color Palettes with ggplot2",
    "section": "Using these colors in plots",
    "text": "Using these colors in plots\nLet’s look at some examples to see the various ways these functions can be used to customize colors with ggplot2. Below is a very simple bar chart using the palmerpenguins::penguins data set.\n\npenguins %&gt;% \n  count(species) %&gt;% \n  ggplot(aes(x = species, y = n)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"The count of each species in the palmerpenguins data set\") +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  theme_here() +\n  theme(axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\nThe new penguin_corp_color function can be used to specifically define a color. In the plot below, penguin_corp_color(\"lavender\") is the fill argument in the geom_bar layer to make all of the bars that specific shade of lavender.\n\npenguins %&gt;% \n  count(species) %&gt;% \n  ggplot(aes(x = species, y = n)) +\n  geom_bar(stat = \"identity\", fill = penguin_corp_color(\"lavender\")) +\n  labs(title = \"The count of each species in the palmerpenguins data set\") +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  theme_here() +\n  theme(axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\nWe could also add fill = species to the aes layer (meaning that each species will have its own color) and then use scale_fill_penguin(palette = \"main\") to automatically apply our “main” color palette.\n\npenguins %&gt;% \n  count(species) %&gt;% \n  ggplot(aes(x = species, y = n, fill = species)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_penguin(palette = \"main\") +\n  labs(title = \"The count of each species in the palmerpenguins data set\") +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  theme_here() +\n  theme(axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        legend.position = \"none\",\n        panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\nOr if you want to use the custom colors but not a specific palette, add a scale_fill_manual layer and specify the values using the penguin_corp_color function.\n\npenguins %&gt;% \n  count(species) %&gt;% \n  ggplot(aes(x = species, y = n, fill = species)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = \n                      unname(c(penguin_corp_color(\"brown\",\"gray\",\"purple\")))) +\n  labs(title = \"The count of each species in the palmerpenguins data set\") +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  theme_here() +\n  theme(axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        legend.position = \"none\",\n        panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\nLastly, below is an example of a continuous color scale, with our scale_color_penguin_c function specifying the palette to be used.\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  ggplot(aes(x = sex, y = body_mass_g, color = bill_depth_mm)) +\n  geom_jitter(size = 3, width = 0.3) +\n  scale_color_penguin_c(palette = \"highlight\", direction = -1) +\n  labs(title = \"Bill depth and body mass by sex\",\n       y = \"Body mass (g)\",\n       color = \"Bill depth (mm)\") +\n  theme_here() +\n  theme(axis.ticks = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major.x = element_blank())"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "\nBlog Posts\n",
    "section": "",
    "text": "Moving My Website from blogdown to Quarto 2024-02-11 \n    \n            \n                \n                \n                    quarto\n                \n                \n            \n            \n    Tips on migrating a website from Hugo/blogdown to Quarto.\n  \n\n  \n    \n      Creating Custom Color Palettes with ggplot2 2022-10-13 \n    \n            \n                \n                \n                    R\n                \n                \n                \n                    ggplot2\n                \n                \n                \n                    data viz\n                \n                \n            \n            \n    How to define custom colors and color palettes in R and create your own scale_color and scale_fill functions to use in ggplot2.\n  \n\n  \n    \n      Tips for Custom Parameterized PDFs in Quarto 2022-09-25 \n    \n            \n                \n                \n                    R\n                \n                \n                \n                    quarto\n                \n                \n            \n            \n    Tips and tricks for creating parameterized reports with R and Quarto and customizing the PDFs with LaTeX.\n  \n\n  \n    \n      Making Slides in Quarto with reveal.js 2022-07-12 \n    \n            \n                \n                \n                    R\n                \n                \n                \n                    quarto\n                \n                \n            \n            \n    Some tips from my first experience making a presentation in Quarto instead of R Markdown.\n  \n\n  \n    \n      A Beginner's Introduction to Mixed Effects Models 2022-06-28 \n    \n            \n                \n                \n                    R\n                \n                \n            \n            \n    A brief introduction to mixed effects models and how to run them in R, with a silly hockey example\n  \n\n  \n    \n      Increasing the Flexibility and Robustness of Plots in ggplot2 2022-02-22 \n    \n            \n                \n                \n                    R\n                \n                \n                \n                    ggplot2\n                \n                \n                \n                    data viz\n                \n                \n            \n            \n    Techniques in ggplot2 to make your plots more flexible and robust to changing data.\n  \n\n  \n    \n      An Introduction to cowplot 2021-08-26 \n    \n            \n                \n                \n                    R\n                \n                \n                \n                    ggplot2\n                \n                \n                \n                    data viz\n                \n                \n            \n            \n    An introduction to the basic functions of the cowplot package for arranging plots together.\n  \n\n  \n    \n      Alternatives to Simple Color Legends in ggplot2 2021-07-20 \n    \n            \n                \n                \n                    R\n                \n                \n                \n                    ggplot2\n                \n                \n                \n                    data viz\n                \n                \n            \n            \n    Fun and custom-looking alternatives to simple color legends in ggplot2.\n  \n\n  \n    \n      The FOGO Trend and the Importance of Faceoffs on the Penalty Kill 2021-02-23 \n    \n            \n                \n                \n                    hockey analysis\n                \n                \n            \n            \n    Examining the FOGO \"trend\" and quantifying the impact of faceoffs on the penalty kill.\n  \n\n  \n    \n      Learning Iterative User-Defined Functions with NHL Standings Points 2020-12-03 \n    \n            \n                \n                \n                    R\n                \n                \n            \n            \n    Writing a user-defined function and applying it iteratively: an example with NHL standings points.\n  \n\n  \n    \n      NHL Shot Locations by Arena 2020-08-17 \n    \n            \n                \n                \n                    hockey analysis\n                \n                \n            \n            \n    Examining NHL shot locations by arena to look for possible systematic error.\n  \n\n  \n    \n      Examining the Effect of the Last Change in Hockey 2020-08-03 \n    \n            \n                \n                \n                    hockey analysis\n                \n                \n            \n            \n    Exploring the possible benefit of the home team having the last change in hockey.\n  \n\n  \n    \n      The State of Goalie Pulling in the NHL 2020-05-18 \n    \n            \n                \n                \n                    hockey analysis\n                \n                \n            \n            \n    A broad analysis of when NHL teams pull their goalie.\n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Meghan Hall",
    "section": "",
    "text": "Hi! I am currently the technical product manager of the hockey group at Zelus Analytics. Prior to moving into sports analytics, I spent nearly a decade working in data management in higher education. I use & love R, with a particular fondness for ggplot2 and quarto.\nI’ve given several talks and workshops, many focused on sports analytics and all available here, and in summer 2021, I taught the Statistical Graphics & Visualization course through the Department of Statistics & Data Science at Carnegie Mellon University. The course focused on data visualization using R and ggplot2, and the public-facing course website (built with distill) is here."
  }
]