[{"authors":["Meghan Hall"],"categories":null,"content":"Meghan Hall is a higher education data professional by day and an amateur hockey analyst by night. She contributes to Hockey-Graphs and has written for Nightingale, the publication of the Data Visualization Society. She is also an enthusiastic user of both R and Tableau and loves helping other people learn these tools to make their work easier.\nShe presented at the Seattle Hockey Analytics Conference in March 2019 on league-wide trends in goalie pulling and has presented on features of the penalty kill three times, at the Rochester Institute of Technology Sports Analytics Conference in August 2019, the Ottawa Hockey Analytics Conference in November 2019, and the Columbus Blue Jackets Hockey Analytics Conference (with Alison Lukan) in February 2020.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1567641600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Meghan Hall is a higher education data professional by day and an amateur hockey analyst by night. She contributes to Hockey-Graphs and has written for Nightingale, the publication of the Data Visualization Society. She is also an enthusiastic user of both R and Tableau and loves helping other people learn these tools to make their work easier.\nShe presented at the Seattle Hockey Analytics Conference in March 2019 on league-wide trends in goalie pulling and has presented on features of the penalty kill three times, at the Rochester Institute of Technology Sports Analytics Conference in August 2019, the Ottawa Hockey Analytics Conference in November 2019, and the Columbus Blue Jackets Hockey Analytics Conference (with Alison Lukan) in February 2020.","tags":null,"title":"Meghan Hall","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I'll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I'll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":[],"content":" In March 2020, the Greater Boston useR Group hosted a Virtual Tidy Tuesday Meetup, and I gave a 10-minute lightning talk on getting started with building custom themes in ggplot2. The slides should be embedded below!\n ","date":1585094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585094400,"objectID":"134e017b8ed72b6026518cb7e39dfe21","permalink":"/post/creating-custom-themes-in-ggplot2/","publishdate":"2020-03-25T00:00:00Z","relpermalink":"/post/creating-custom-themes-in-ggplot2/","section":"post","summary":"Slides from a useR group lightning talk on getting started with creating custom themes in ggplot2.","tags":[],"title":"Creating Custom Themes in ggplot2","type":"post"},{"authors":null,"categories":null,"content":"The swirl package is an incredibly neat learning tool that teaches you how to use R via interactive learning in the RStudio console. And an associated package called swirlify allows anyone to create lessons that can then be used by anyone using swirl.\nI've created a course called Hockey Data With Swirl that aims to teach you basic tidyverse functions using hockey data. The data set used in the swirl lesson is the same one used in my introduction to R at Hockey-Graphs, and the content is similar, but not quite identical. You can go through either tutorial, or both, in any order. The swirl package is just another learning tool that guides you through, question by question. Right now there is only one lesson in the course, but my goal is to add more in the future (such as a lesson for ggplot2!).\nIn order to go through the lesson, you need to have downloaded R and RStudio (the instructions for which are available in the Hockey-Graphs tutorial I linked above). In RStudio, open a script and run the following code:\ninstall.packages(\u0026#34;swirl\u0026#34;) library(swirl) install_course_github(\u0026#34;meghall06\u0026#34;, \u0026#34;Hockey_Data_With_Swirl\u0026#34;) swirl() The console prompts will take over from there. swirl works through the console, which allows for interactivity but doesn't make it easy to save your code and go back to it later. I've attempted to account for that by making available the code file that contains all of the questions and answers from the lesson.\nI hope you find the lesson useful, feel free to give feedback and let me know what other topics you'd be interested in!\n","date":1584230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584230400,"objectID":"fbb6d2ee33f806a294220ef63079d56e","permalink":"/post/hockey-data-with-swirl/","publishdate":"2020-03-15T00:00:00Z","relpermalink":"/post/hockey-data-with-swirl/","section":"post","summary":"The swirl package is an incredibly neat learning tool that teaches you how to use R via interactive learning in the RStudio console. And an associated package called swirlify allows anyone to create lessons that can then be used by anyone using swirl.\nI've created a course called Hockey Data With Swirl that aims to teach you basic tidyverse functions using hockey data. The data set used in the swirl lesson is the same one used in my introduction to R at Hockey-Graphs, and the content is similar, but not quite identical.","tags":null,"title":"Learning R with Hockey Data in Swirl","type":"post"},{"authors":null,"categories":null,"content":"Before we start, an important disclaimer: this is not a tutorial on how to thoughtfully build and thoroughly evaluate models. This is a gentle introduction to the tidymodels package (which, like the tidyverse, is actually a collection of packages), and in order to examine various functions and capabilities of those packages, we'll build two very simple models, using easily available NHL data, and go over a few ways to evaluate them.\nThe tidymodels package, which is fairly new, was designed to make it easier to create your model framework in a tidy way and consists of, among others, recipes (prepping models), parsnip (executing models), and yardstick (evaluating models). Here, we'll build two simple models to predict whether an NHL player is a forward or a defenseman.\nFind and prepare the data First, let's get our data preppedâ€”we're using 2018-19 data so we can have a full season. We'll get the position data by downloading from Natural Stat Trick, and we'll create our statistics from the raw play-by-play data, available via the Evolving Wild scraper. (Could you download all these summary statistics from NST instead? Definitely. But this is about learning, and it's great R practice [pRactice?] to generate them yourself from the play-by-play data.)\n One of the tidymodels packages called dials has a margin() function that will mask the margin() function in ggplot2. If you use the margin() function in your ggplot2 custom theme like I do, just load tidymodels before tidyverse and you should be fine.   library(tidymodels) library(tidyverse) # Read in files (pbp from Evolving Hockey, bios from Natural Stat Trick) pbp \u0026lt;- read_csv(\u0026#34;pbp_20182019_all.csv\u0026#34;) bios \u0026lt;- read_csv(\u0026#34;bios_1819.csv\u0026#34;) # Find player TOI and games played # To do so, you must pivot the data so there is one row per player # (instead of one row per event) # We don\u0026#39;t care about the ice time for the goalies (sorry, goalies) # so they will be filtered out # We also do some name changes to make things easier later player_TOI \u0026lt;- pbp %\u0026gt;% filter(event_length \u0026gt; 0) %\u0026gt;% select(game_id, event_length, home_on_1:away_goalie) %\u0026gt;% pivot_longer(home_on_1:away_on_6, names_to = \u0026#34;variable\u0026#34;, values_to = \u0026#34;player\u0026#34;) %\u0026gt;% filter(!(is.na(player)) \u0026amp; player != home_goalie \u0026amp; player != away_goalie) %\u0026gt;% mutate(player = case_when( player == \u0026#34;COLIN.WHITE2\u0026#34; ~ \u0026#34;COLIN.WHITE\u0026#34;, player == \u0026#34;ERIK.GUSTAFSSON2\u0026#34; ~ \u0026#34;ERIK.GUSTAFSSON\u0026#34;, player == \u0026#34;PATRICK.MAROON\u0026#34; ~ \u0026#34;PAT.MAROON\u0026#34;, TRUE ~ player )) %\u0026gt;% group_by(player) %\u0026gt;% summarize(games = n_distinct(game_id), TOI = sum(event_length) / 60) # Find basic player stats # To find individual stats, we again need to pivot the data to one row per player # but we\u0026#39;re using the event_players only (not the on ice players) # You\u0026#39;ll notice we\u0026#39;re filtering out the shootout (which is game_period 5) because # those goals don\u0026#39;t count # We\u0026#39;ll sum up blocked shots (event_player_2 is the player who blocked the shot, # event_player_1 is the one who generated it), total points, shots, unblocked shots, # hits (both give nand received) player_stats \u0026lt;- pbp %\u0026gt;% filter(event_type %in% c(\u0026#34;HIT\u0026#34;, \u0026#34;BLOCK\u0026#34;, \u0026#34;SHOT\u0026#34;, \u0026#34;MISS\u0026#34;, \u0026#34;GOAL\u0026#34;) \u0026amp; game_period \u0026lt; 5) %\u0026gt;% select(game_id, event_type, event_player_1:event_player_3) %\u0026gt;% pivot_longer(event_player_1:event_player_3, names_to = \u0026#34;number\u0026#34;, values_to = \u0026#34;player\u0026#34;) %\u0026gt;% filter(!(is.na(player))) %\u0026gt;% mutate(block = ifelse(event_type == \u0026#34;BLOCK\u0026#34; \u0026amp; number == \u0026#34;event_player_2\u0026#34;, 1, 0), point = ifelse(event_type == \u0026#34;GOAL\u0026#34;, 1, 0), shot = ifelse(number == \u0026#34;event_player_1\u0026#34; \u0026amp; event_type %in% c(\u0026#34;SHOT\u0026#34;, \u0026#34;GOAL\u0026#34;), 1, 0), fenwick = ifelse(number == \u0026#34;event_player_1\u0026#34; \u0026amp; event_type %in% c(\u0026#34;SHOT\u0026#34;, \u0026#34;GOAL\u0026#34;, \u0026#34;MISS\u0026#34;), 1, 0), hit = ifelse(number == \u0026#34;event_player_1\u0026#34; \u0026amp; event_type == \u0026#34;HIT\u0026#34;, 1, 0), hit_rec = ifelse(number == \u0026#34;event_player_2\u0026#34; \u0026amp; event_type == \u0026#34;HIT\u0026#34;, 1, 0), player = case_when( player == \u0026#34;COLIN.WHITE2\u0026#34; ~ \u0026#34;COLIN.WHITE\u0026#34;, player == \u0026#34;ERIK.GUSTAFSSON2\u0026#34; ~ \u0026#34;ERIK.GUSTAFSSON\u0026#34;, player == \u0026#34;PATRICK.MAROON\u0026#34; ~ \u0026#34;PAT.MAROON\u0026#34;, TRUE ~ player )) %\u0026gt;% group_by(player) %\u0026gt;% summarize(blocks = sum(block), points = sum(point), shots = sum(shot), fenwick = sum(fenwick), hits = sum(hit), hits_rec = sum(hit_rec)) # Join stats into TOI data frame and create rates player_TOI_stats \u0026lt;- player_TOI %\u0026gt;% left_join(player_stats, by = \u0026#34;player\u0026#34;) %\u0026gt;% mutate(points_60 = points * 60 / TOI, shots_60 = shots * 60 / TOI, fenwick_60 = fenwick * 60 / TOI, hits_60 = hits * 60 / TOI, hits_rec_60 = hits_rec * 60 / TOI, blocks_60 = blocks * 60 / TOI, TOI_game = TOI / games) %\u0026gt;% select(-c(blocks:hits_rec)) # Clean up the biographical data bios \u0026lt;- bios %\u0026gt;% mutate(player = str_to_upper(Player), player = str_replace(player, \u0026#34;\u0026#34;, \u0026#34;.\u0026#34;), defense = ifelse(Position == \u0026#34;D\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;F\u0026#34;)) %\u0026gt;% rename(height = `Height (in)`, weight = `Weight (lbs)`) %\u0026gt;% select(player, defense, height, weight) %\u0026gt;% mutate(player = str_replace_all(player, \u0026#34;ALEXANDER\u0026#34;, \u0026#34;ALEX\u0026#34;), player = str_replace_all(player, \u0026#34;ALEXANDRE\u0026#34;, \u0026#34;ALEX\u0026#34;), player = case_when( player == \u0026#34;CHRISTOPHER.TANEV\u0026#34; ~ \u0026#34;CHRIS.TANEV\u0026#34;, player == \u0026#34;DANNY.O\u0026#39;REGAN\u0026#34; ~ \u0026#34;DANIEL.O\u0026#39;REGAN\u0026#34;, player == \u0026#34;EVGENII.DADONOV\u0026#34; ~ \u0026#34;EVGENY.DADONOV\u0026#34;, player == \u0026#34;MATTHEW.BENNING\u0026#34; ~ \u0026#34;MATT.BENNING\u0026#34;, player == \u0026#34;MITCHELL.MARNER\u0026#34; ~ \u0026#34;MITCH.MARNER\u0026#34;, TRUE ~ player )) # Join biographical data into stats data # Filter to only keep players who played at least 20 games final_data \u0026lt;- player_TOI_stats %\u0026gt;% left_join(bios, by = \u0026#34;player\u0026#34;) %\u0026gt;% filter(games \u0026gt; 19) These stats are the ones that we're going to use in our model to predict whether a given player is a forward or a defenseman. Let's create at a few graphs, just to see how some of these data look.\n# The code for these four graphs is nearly the same, just change the x # and the title/labels final_data %\u0026gt;% ggplot(aes(x = weight, fill = defense)) + geom_density(alpha = 0.7, color = NA) + scale_fill_manual(values = c(\u0026#34;#0d324d\u0026#34;, \u0026#34;#a188a6\u0026#34;)) + labs( y = \u0026#34;Density\u0026#34;, x = \u0026#34;Weight (lbs)\u0026#34;, fill = NULL, title = \u0026#34;Weight by Position\u0026#34;, subtitle = \u0026#34;2018-19 NHL Season, 20+ Games Played Only\u0026#34;, caption = \u0026#34;Source: Natural Stat Trick\u0026#34; ) + meg_theme() + theme(legend.position = c(0.9, 0.9))             We can spot some differences here by position: defensemen tend to score at a lower rate and block shots at a higher rate than forwards do. They also tend to spend more time on the ice (by necessity, since there are generally half the number of defensemen as forwards on a dressed roster), which is one of the most well-known differences in the positions. In order to try to predict whether a given player is a forward or a defenseman, we're going to build two logistic regression models. One will have the average time on ice as its sole predictor variable, while the other will have all of these variables (average time on ice, height, weight, points per 60, shots per 60, unblocked shots per 60, hits per 60, hits received per 60, and blocked shots per 60) as predictor variables.\nGet data ready for modeling Our final_data data frame from above will be the base of our model_data (we're just removing two unnecessary variables), and we'll use set.seed() to create reproducible samples.\n# Rearrange our model data model_data \u0026lt;- final_data %\u0026gt;% select(player, defense, everything(), -c(games, TOI)) # Set the seed (very useful for reproducible samples!) set.seed(1234) # Split into training and testing data split_data \u0026lt;- initial_split(model_data, prop = 0.6, strata = defense) The last line of code above, which created a list called split_data, uses the helpful initial_split function from the rsample package. This allows us to create a training data set and a testing data set, an essential step when modeling. We will train the data on one data set and then test the models on a separate data set. You can set the proportion on your own, of how many observations will go to the training data, but it will default to 0.75 without a different specification. And why did I include defense as an optional strata argument?\n   As you can see above, our data set has nearly twice the amount of forwards as defensemen. By using the strata option, we can ensure that there's a similar proportion of forwards to defensemen in both our training and our testing data sets.\n# Create our testing and training data sets training_data \u0026lt;- training(split_data) testing_data \u0026lt;- testing(split_data) # Write the recipe for our small TOI_only model recipe_TOI_only \u0026lt;- training_data %\u0026gt;% recipe(defense ~ TOI_game) %\u0026gt;% step_center(all_predictors()) %\u0026gt;% step_scale(all_predictors()) %\u0026gt;% prep() recipe_TOI_only In the code above, we can create our basic training and test data sets and then move onto the useful functions of the recipes package. This package allows you to create a recipe in order to organize all of your processing steps for your model(s). You specify the arguments with the recipe() function and then specify processing steps with the various functions that begin with step_. There are dozens of these that will perform all sorts of functions (e.g., create dummy variables, input various values, take the log), but here we're just using step_center() and step_scale() to show you how to normalize variables. In order to specify variables for these step_ functions, you can use standard dplyr::select variables (e.g., starts_with(), ends_with()) or select by role (e.g., all_predictors(), all_outcomes()) or select by data type (e.g., all_numeric()). And you can of course select by variable name, as well.\nWe now have a recipe called recipe_TOI_only that looks like this.\n   Run our models # Extract our prepped training data  # and \u0026#34;bake\u0026#34; our testing data training_baked_TOI \u0026lt;- juice(recipe_TOI_only) testing_baked_TOI \u0026lt;- recipe_TOI_only %\u0026gt;% bake(testing_data) # Run the model with our training data logistic_glm_TOI \u0026lt;- logistic_reg(mode = \u0026#34;classification\u0026#34;) %\u0026gt;% set_engine(\u0026#34;glm\u0026#34;) %\u0026gt;% fit(defense ~ ., data = training_baked_TOI) Now that we have our recipe, we can apply it to our training and testing data. Since the training data was the base of the recipe, we can use the juice() function to extract it. And the bake() function will prep the test data. Then, we can actually run the model with functions from the parsnip package. The package handles many different kind of models, but here we're running a simple logistic regression and training it on our baked data.\n# Find the class predictions from our testing data # And add back in the true values from testing data predictions_class_TOI \u0026lt;- logistic_glm_TOI %\u0026gt;% predict(new_data = testing_baked_TOI) %\u0026gt;% bind_cols(testing_baked_TOI %\u0026gt;% select(defense)) # Find the probability predictions # And add all together predictions_TOI \u0026lt;- logistic_glm_TOI %\u0026gt;% predict(testing_baked_TOI, type = \u0026#34;prob\u0026#34;) %\u0026gt;% bind_cols(predictions_class_TOI) Now that the model has been trained, we can apply it to the testing data. The data frame we just created, predictions_TOI, looks like this. For each observation in our test data set, we have the predicted position and the probability that drove that prediction. We also brought in the defense variable from the test data set.\n   Just for fun, we can bring the player variable back from the original test data set and look at who was predicted the most incorrectly.\n# Look at who was predicted the most incorrectly # (Just for fun) most_wrong_TOI \u0026lt;- predictions_TOI %\u0026gt;% bind_cols(select(testing_data, player, TOI_game)) %\u0026gt;% mutate(incorrect = .pred_class != defense) %\u0026gt;% filter(incorrect == TRUE) %\u0026gt;% mutate(prob_actual = ifelse(defense == \u0026#34;D\u0026#34;, .pred_D, .pred_F)) %\u0026gt;% arrange(prob_actual)    As to be expected with such a simple model that's based solely on TOI, the predictions aren't so good for defensemen who don't play a lot of minutes or forwards who do. Let's move on to our kitchen sink model that includes all the variables.\n# Do the same process for our kitchen sink model recipe_kitchen_sink \u0026lt;- training_data %\u0026gt;% recipe(defense ~ weight + height + points_60 + shots_60 + fenwick_60 + hits_60 + hits_rec_60 + blocks_60 + TOI_game) %\u0026gt;% step_corr(all_predictors(), threshold = 0.8) %\u0026gt;% step_center(all_predictors()) %\u0026gt;% step_scale(all_predictors()) %\u0026gt;% prep() recipe_kitchen_sink training_baked_KS \u0026lt;- juice(recipe_kitchen_sink) testing_baked_KS \u0026lt;- recipe_kitchen_sink %\u0026gt;% bake(testing_data) # Run the model with our training data logistic_glm_KS \u0026lt;- logistic_reg(mode = \u0026#34;classification\u0026#34;) %\u0026gt;% set_engine(\u0026#34;glm\u0026#34;) %\u0026gt;% fit(defense ~ ., data = training_baked_KS) # Find the class predictions from our testing data # And add back in the true values from testing data predictions_class_KS \u0026lt;- logistic_glm_KS %\u0026gt;% predict(new_data = testing_baked_KS) %\u0026gt;% bind_cols(testing_baked_KS %\u0026gt;% select(defense)) # Find the probability predictions # And add all together predictions_KS \u0026lt;- logistic_glm_KS %\u0026gt;% predict(testing_baked_KS, type = \u0026#34;prob\u0026#34;) %\u0026gt;% bind_cols(predictions_class_KS) The code above looks very similar to the code from before, except we added an extra step in our recipe. The step_corr() function will study all the correlations among variables you specify and remove offenders, as it often isn't a good idea to have variables in your model that are highly correlated with each other. The default threshold for exclusion is 0.9, but you can specify whatever value you want. As you can see in the recipe below, our recipe automatically removed the shots_60 variable, which is (obviously) very highly correlated to the unblocked shot attempt variable, fenwick_60.\n   Evaluate our models In this section, I'm only going to show the code for one model (though we're evaluating two), but of course you would use the same code for both. (And if you were working with multiple models that you want to compare, it'd be a good idea to create functions to do these steps so that you aren't copying and pasting.)\nFirst we can create a confusion matrix, which simply plots the predicted values against the actual values.\n# Create a confusion matrix predictions_TOI %\u0026gt;% conf_mat(defense, .pred_class) %\u0026gt;% pluck(1) %\u0026gt;% as_tibble() %\u0026gt;% ggplot(aes(Prediction, Truth, alpha = n)) + geom_tile(show.legend = FALSE) + geom_text(aes(label = n), colour = \u0026#34;white\u0026#34;, alpha = 1, size = 8) + meg_theme() + theme(panel.grid.major = element_blank()) + labs( y = \u0026#34;Actual Position\u0026#34;, x = \u0026#34;Predicted Position\u0026#34;, fill = NULL, title = \u0026#34;Confusion Matrix\u0026#34;, subtitle = \u0026#34;TOI Only Model\u0026#34; )       Just from a brief look at this, the kitchen sink model clearly has higher accuracy (calculated as the number of correct predictions divided by the number of total predictions) than the TOI only model.\n# Find the accuracy predictions_TOI %\u0026gt;% accuracy(defense, .pred_class) # Find the logloss predictions_TOI %\u0026gt;% mn_log_loss(defense, .pred_D) # Find the area under the ROC curve (AUC) predictions_TOI %\u0026gt;% roc_auc(defense, .pred_D) # Create a tibble that holds all the evaluation metrics TOI_metrics \u0026lt;- tibble( \u0026#34;log_loss\u0026#34; = mn_log_loss(predictions_TOI, defense, .pred_D) %\u0026gt;% select(.estimate), \u0026#34;accuracy\u0026#34; = accuracy(predictions_TOI, defense, .pred_class) %\u0026gt;% select(.estimate), \u0026#34;auc\u0026#34; = roc_auc(predictions_TOI, defense, .pred_D) %\u0026gt;% select(.estimate) ) %\u0026gt;% unnest(everything()) %\u0026gt;% pivot_longer(everything(), names_to = \u0026#34;metric\u0026#34;, values_to = \u0026#34;value\u0026#34;) %\u0026gt;% mutate(model = \u0026#34;TOI_only\u0026#34;) The yardstick package is what holds a lot of these functions that are useful for model evaulation. We just defined accuracy, which you can calculate on your own from the confusion matrix and is also available via the accuracy() function. That's useful for determining how good the model is in a binary sense, while log loss (from the mn_log_loss() function) uses the probabilities to quantify how correct the predictions are. As an example, let's go back to our TOI only model and see that Aleksander Barkov (a forward) was given a 0.75 probability of being a defenseman. That's obviously incorrect. It's counted as an incorrect prediction for the accuracy metric, but log loss also takes into account that the prediction was quite wrong. If the prediction had instead given him a 0.51 probability of being a defenseman, the penalty would be less.\nWe can also create a tibble (a type of data frame) to hold all of these metrics. We'll use it to compare both models in a minute. The last metric included is the area under the ROC curve, known as AUC. The ROC curve graphs the false positive rate against the true positive rate and in a nutshell, quantifies how good the model is at distinguishing the groups.\nThe yardstick package also makes it really easy to graph the curve itself.\n# Look at the ROC curve predictions_TOI %\u0026gt;% roc_curve(defense, .pred_D) %\u0026gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path() + geom_abline(lty = 3) + coord_equal() + meg_theme() + labs( y = \u0026#34;True Positive Rate (Sensitivity)\u0026#34;, x = \u0026#34;False Positive Rate\u0026#34;, fill = NULL, title = \u0026#34;ROC Curve\u0026#34;, subtitle = \u0026#34;TOI Only Model\u0026#34; )       The ideal ROC curve is one that goes high up into the top left corner (as to maximize the area underneath it), so again, it appears that our kitchen sink model is performing better here. Lastly, let's use the tibbles we created to hold the evaulation metrics and graph to compare.\nmetrics_compare \u0026lt;- TOI_metrics %\u0026gt;% bind_rows(KS_metrics) metrics_compare %\u0026gt;% ggplot(aes(fill = model, y = value, x = metric)) + geom_bar(position = \u0026#34;dodge\u0026#34;, stat = \u0026#34;identity\u0026#34;) + scale_fill_manual(values = c(\u0026#34;#7A8B99\u0026#34;, \u0026#34;#A9DDD6\u0026#34;)) + meg_theme() + labs( y = \u0026#34;Value\u0026#34;, x = \u0026#34;Metric\u0026#34;, fill = NULL, title = \u0026#34;Comparing Our Models\u0026#34;, subtitle = \u0026#34;Higher is Better: Accuracy and AUC\\nLower is Better: Log Loss\u0026#34; ) + geom_text(aes(label = round(value, 3)), vjust = -0.5, size = 3, position = position_dodge(width= 0.9)) + theme(legend.position = c(0.86, 0.9))    We saw previously from the confusion matrices that the accuracy for the kitchen sink model is higher, and this tells us that the AUC is higher, as well, while the log loss is lower (which is good). Thanks to the evaluation metrics of the yardstick package (and there are many more than the few we viewed!), we have evidence that compared to the TOI only model, the kitchen sink model makes more accurate predictions and is better at distinguishing between the groups.\ntidymodels is a pretty neat set of packages, and I hope this little tutorial was useful in introducing some of the many features. Here are a handful of other resources I have found helpful as I continue to learn more about this package:\n https://juliasilge.com/blog/intro-tidymodels/ https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c https://www.benjaminsorensen.me/post/modeling-with-parsnip-and-tidymodels/ https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/  ","date":1583712000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583712000,"objectID":"b5142e061a0e95bbcd107bd2a3d8c63a","permalink":"/post/tidymodels-intro/","publishdate":"2020-03-09T00:00:00Z","relpermalink":"/post/tidymodels-intro/","section":"post","summary":"Before we start, an important disclaimer: this is not a tutorial on how to thoughtfully build and thoroughly evaluate models. This is a gentle introduction to the tidymodels package (which, like the tidyverse, is actually a collection of packages), and in order to examine various functions and capabilities of those packages, we'll build two very simple models, using easily available NHL data, and go over a few ways to evaluate them.","tags":null,"title":"Exploring tidymodels With Hockey Data","type":"post"},{"authors":null,"categories":null,"content":"Last year, I tracked 1146 minutes of penalty kills, spread across 12 teams from the 2018-19 season. The teams were chosen semi-randomly (to get a decent distribution of shot attempt rates, both for and against), and games were selected randomly to end up with about a quarter of all penalty kill minutes for that team during 2018-19. I tracked zone time (so that I could track how well a penalty kill was able to keep a power play out of its offensive zone and also calculate shot rates for offensive zone time only), as well as zone entries and exits.\n   Selected Teams     Chicago   Colorado   Dallas   Winnipeg   Vegas   Edmonton   Vancouver   New Jersey   NY Islanders   NY Rangers   Philadelphia   Florida    The bulk of the penalty kill data is available in the slides from my OTTHAC presentation and the data visualization that accompanied it.\nPower play data However, tracking penalty kills means I automatically have some data on power plays! I ended up with some data for all 31 teams, but here I have restricted the plots to the 11 teams for which I had the most data (60 to ~100 minutes). This isn't enough game time to make sweeping conclusions, but it's enough to look at a few graphs.\n   Selected Teams     Buffalo   Calgary   Colorado   Edmonton   Florida   Nashville   Ottawa   Philadelphia   St. Louis   Vancouver   Washington    Shown below is the percent of total power play time on ice that the power play spent in its offensive zone. I was surprised to see a relatively small spread in the percentages (besides Ottawa).\n   As I mentioned previously with the penalty kill data, having zone time makes it possible to calculate a per 60 shot rate (for unblocked shot attempts, in this case) that is based on offensive zone time only, instead of just total power play time on ice. This shows how frequently teams generate shots when they're actually in the zone.\n   It's interesting to contrast Ottawa and Colorado in these two graphs. Ottawa wasn't particularly successful at staying in the offensive zone, but when they were there, they generated a lot of unblocked shot attempts. Colorado was nearly the opposite, in that they spent a lot of time in the offensive zone but weren't as prolific with the shots. The \u0026quot;regular\u0026quot; unblocked shot attempt rates for the two teams over these games were nearly identical.\n   Lastly, since I tracked zone entry data, as well, we can look at how power play teams tended to enter their own zone by examining the percentage of all entries that were dump-ins versus carries and passes. In this smallish sample there was a wide variation between teams that tended to dump the puck in more (Nashville, Calgary) and teams that ver rarely did (Colorado, Ottawa).\n","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"def377eb53bb076039b4aa5ee5ea1b0b","permalink":"/post/pk_project/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/post/pk_project/","section":"post","summary":"Last year, I tracked 1146 minutes of penalty kills, spread across 12 teams from the 2018-19 season. The teams were chosen semi-randomly (to get a decent distribution of shot attempt rates, both for and against), and games were selected randomly to end up with about a quarter of all penalty kill minutes for that team during 2018-19. I tracked zone time (so that I could track how well a penalty kill was able to keep a power play out of its offensive zone and also calculate shot rates for offensive zone time only), as well as zone entries and exits.","tags":null,"title":"Wrapping Up The Penalty Kill Project","type":"post"},{"authors":[],"categories":null,"content":"","date":1581120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581120000,"objectID":"ba664c536a4dbe92df229013c91a56ea","permalink":"/talk/cbjhac/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/cbjhac/","section":"talk","summary":"Examining the rise of the power kill.","tags":[],"title":"The Anatomy of a Power Kill","type":"talk"},{"authors":null,"categories":null,"content":"Score effects in hockey are well-known, whether you're watching the game or looking at numbers: teams that are losing tend to generate a greater share of the shot attempts. Micah Blake McCurdy developed an adjustment method for 5v5 events that is currently used to create the score- and venue-adjusted shot attempt metrics available at hockey stats websites like Natural Stat Trick and Evolving Hockey.\nSince I spend a lot of time looking at special teams data, I have long been curious as to whether score effects might also be a factor for power plays. Here, I've modified Micah's method to investigate score effects at 5v4: I created the weights, compared them to the 5v5 weights, and examined the repeatability and predictivity of the adjusted values compared to the raw ones. Investigating this problem taught me a lot of new R skills, so I've included all code and will go through the process step-by-step. (So if you're only interested in the conclusion, just scroll past the code and look at the graphs!)\nGet and prepare the data To start, I used the play-by-play query tool available at Evolving Hockey to collect all power play events from the past 10 seasons. (This was much quicker than scraping full seasons of data, and if you'd like access to the tool, support them on Patreon.) With all of the files in the same folder, instead of reading them in one-by-one, some functions from the plyr package make it easier to read them all in and combine them into one file.\nlibrary(plyr) library(readr) library(tidyverse) library(infer) # All 10 of my csv files (one for each season) are in a folder in my wd called \u0026#34;score_adj\u0026#34; # This will ID all the files and read them in as one file (score_adj_5v4_raw) mydir = \u0026#34;score_adj\u0026#34; myfiles = list.files(path=mydir, pattern=\u0026#34;*.csv\u0026#34;, full.names=TRUE) myfiles score_adj_5v4_raw = ldply(myfiles, read_csv) I filtered down to only the events that I want (all unblocked shot attempts as well as any events with time, so I can create rates) and created some new variables. Per Micah's method, all leads over 2 and below -2 were grouped.\n# We\u0026#39;ll filter down to only unblocked shot attempts and time events (to get fenwick \u0026amp; rates) # And also create variables to get the home lead and an indicator whether the event is home or away # For home lead, everything above 2 and below -2 will be grouped together score_adj_5v4 \u0026lt;- score_adj_5v4_raw %\u0026gt;% filter(event_type %in% c(\u0026#34;SHOT\u0026#34;, \u0026#34;GOAL\u0026#34;, \u0026#34;MISS\u0026#34;) | event_length \u0026gt; 0) %\u0026gt;% mutate(home_lead = home_score - away_score, home_lead = ifelse(home_lead \u0026lt;= -3, -3, ifelse(home_lead \u0026gt;= 3, 3, home_lead)), home_away = ifelse(event_team == home_team, \u0026#34;home\u0026#34;, \u0026#34;away\u0026#34;), fenwick = ifelse(event_type %in% c(\u0026#34;SHOT\u0026#34;, \u0026#34;GOAL\u0026#34;, \u0026#34;MISS\u0026#34;), 1, 0), goal = ifelse(event_type == \u0026#34;GOAL\u0026#34;, 1, 0), PP_team = ifelse(game_strength_state == \u0026#34;5v4\u0026#34;, home_team, away_team), type = ifelse(fenwick == 1 \u0026amp; PP_team == event_team, \u0026#34;PP\u0026#34;, ifelse(fenwick == 1, \u0026#34;PK\u0026#34;, NA)), PP_fenwick = ifelse(type == \u0026#34;PP\u0026#34; \u0026amp; fenwick == 1, 1, 0), f = ifelse(PP_fenwick == 1, \u0026#34;Y\u0026#34;, NA)) %\u0026gt;% select(type, PP_fenwick, f, game_strength_state, home_team, away_team, PP_team, event_team, everything())  Create the weights The first step is to determine what the weights should be. In our raw data, all unblocked shot attempts count the same: as one. But they are not all truly equal since we know that trailing teams generate more shot attempts. If the team that is currently winning by two goals generates a shot attempt, for example, that should count as more than one shot attempt because it's more difficult. And vice versa for the team that is currently losing. We can calculate how much these shot attempts should be boosted or penalized by comparing the actual values to the average values (i.e., if there was no difference).\nThe first data frame below, score_adj_f, just sums the unblocked shot attempts for each state to give us the raw values. (You'll see that the lockout-shortened season is filtered out, we'll discuss why later.) The second data frame score_adj_f_avg sums unblocked shot attempts for each score state only. Those average values get joined back into the first data frame, and then we can create a weight by dividing the true value by the average. score_adj_f_reshape just uses the pivot_wider function to reshape the values to make them more readable.\n# We filter by power play fenwick events only, then # group by home_away and home_lead score_adj_f \u0026lt;- score_adj_5v4 %\u0026gt;% filter(PP_fenwick == 1) %\u0026gt;% filter(season != 20122013) %\u0026gt;% group_by(home_lead, home_away, f) %\u0026gt;% summarize(fenwick = sum(PP_fenwick)) # Group by home_lead to get the average fenwicks score_adj_f_avg \u0026lt;- score_adj_f %\u0026gt;% group_by(home_lead) %\u0026gt;% summarize(avg = mean(fenwick)) # Join the average back into the previous data frame # and create the adjusted score by dividing the average by the raw fenwick score_adj_f \u0026lt;- score_adj_f %\u0026gt;% left_join(score_adj_f_avg, by = \u0026#34;home_lead\u0026#34;) %\u0026gt;% mutate(fenwick_adj = avg / fenwick) # Reshape to get the numbers in a more readable format score_adj_f_reshape \u0026lt;- score_adj_f %\u0026gt;% select(-c(avg, fenwick)) %\u0026gt;% pivot_wider(names_from = home_away, values_from = fenwick_adj) %\u0026gt;% select(home_lead, home, away)  This is what the score_adj_f data frame looks like before we reshape it for easier analysis. fenwick has the raw values, avg has the average values, and fenwick_adj has the weights (found by dividing avg by fenwick).\n   And this is what it looks like after.\n   Compare to 5v5 weights Now that we have our weights (shown above), we can compare to Micah's established weights for 5v5 events.\n# Read in Micah\u0026#39;s 5v5 values # And join to compare score_adj_5v5 \u0026lt;- read_csv(\u0026#34;score_adj_5v5_MBM.csv\u0026#34;) score_adj_f_reshape_compare \u0026lt;- score_adj_f_reshape %\u0026gt;% left_join(score_adj_5v5, by = \u0026#34;home_lead\u0026#34;) %\u0026gt;% mutate(diff_home = home - home_5v5, diff_away = away - away_5v5) # Create simple bar graphs to compare ggplot(data = score_adj_f_reshape_compare, aes(x = as.factor(home_lead), y = diff_home)) + geom_bar(stat = \u0026#34;identity\u0026#34;) + labs(title = \u0026#34;Home Event Weights, 5v4 vs 5v5\u0026#34;, x = \u0026#34;Home Lead\u0026#34;, y = \u0026#34;Difference From 5v5 Weight\u0026#34;) + theme_linedraw() + theme(axis.ticks = element_blank()) ggplot(data = score_adj_f_reshape_compare, aes(x = as.factor(home_lead), y = diff_away)) + geom_bar(stat = \u0026#34;identity\u0026#34;) + labs(title = \u0026#34;Away Event Weights, 5v4 vs 5v5\u0026#34;, x = \u0026#34;Home Lead\u0026#34;, y = \u0026#34;Difference From 5v5 Weight\u0026#34;) + theme_linedraw() + theme(axis.ticks = element_blank()) for_comparison \u0026lt;- score_adj_f_reshape_compare %\u0026gt;% select(-c(diff_home:diff_away)) %\u0026gt;% rename(home_5v4 = home, away_5v4 = away) %\u0026gt;% pivot_longer(home_5v4:away_5v5, names_to = \u0026#34;type\u0026#34;, values_to = \u0026#34;value\u0026#34;) %\u0026gt;% mutate(home_away = substr(type, 1, 4), state = substr(type, 6, 8)) for_comparison %\u0026gt;% filter(home_away == \u0026#34;home\u0026#34;) %\u0026gt;% ggplot(aes(fill = state, x = as.character(home_lead), y = value)) + geom_bar(stat = \u0026#34;identity\u0026#34;, position = \u0026#34;dodge\u0026#34;) + labs(title = \u0026#34;Home Event Weights, 5v4 vs 5v5\u0026#34;, x = \u0026#34;Home Lead\u0026#34;, y = \u0026#34;Weight\u0026#34;, fill = \u0026#34;State?\u0026#34;) + theme_linedraw() + theme(axis.ticks = element_blank()) + geom_hline(yintercept = 1) + scale_fill_manual(values = c(\u0026#34;#CF8BA8\u0026#34;, \u0026#34;#DDDDDD\u0026#34;)) for_comparison %\u0026gt;% filter(home_away == \u0026#34;away\u0026#34;) %\u0026gt;% ggplot(aes(fill = state, x = as.character(home_lead), y = value)) + geom_bar(stat = \u0026#34;identity\u0026#34;, position = \u0026#34;dodge\u0026#34;) + labs(title = \u0026#34;Away Event Weights, 5v4 vs 5v5\u0026#34;, x = \u0026#34;Home Lead\u0026#34;, y = \u0026#34;Weight\u0026#34;, fill = \u0026#34;State?\u0026#34;) + theme_linedraw() + theme(axis.ticks = element_blank()) + geom_hline(yintercept = 1) + scale_fill_manual(values = c(\u0026#34;#CF8BA8\u0026#34;, \u0026#34;#DDDDDD\u0026#34;))              We can see from these comparisons that overall, the magnitude of these 5v4 weights is not too different from those at 5v5. The largest difference is when the home team is trailing by one: the 5v5 weight for the away team is 1.103. At 5v4, the weight for the away team is 1.221. (Interestingly, when the home team is leading by one, the weights are nearly identical.) This difference fits the overall pattern that you can see in the graphs: the weights for the away team at 5v4 are consistently higher than they are at 5v5, regardless of the score state, and vice versa for the home team. This could suggest that events for the home team are slightly easier to generate at 5v4, regardless of score state, and therefore the away team gets more \u0026quot;credit\u0026quot; (i.e., a higher weight) for their events.\nOur last step, before we can test the repeatability and prediction of the weights, is to join these adjusted values back into the raw event data so we can use them for comparison.\n# Join the adjusted values back into the raw data score_adj_5v4_w_values \u0026lt;- score_adj_5v4 %\u0026gt;% left_join(select(score_adj_f, home_lead, home_away, fenwick_adj, f), by = c(\u0026#34;home_lead\u0026#34;, \u0026#34;home_away\u0026#34;, \u0026#34;f\u0026#34;)) Testing repeatability and prediction To examine these adjusted values in comparison to the raw ones, we'll look at both repeatability and prediction. Repeatability is measured by how well the unblocked shot attempts in one sample of a season correlate to those in another sample, and prediction looks at how well the unblocked shot attempts in one sample correlate to goals in another. Our metric of interest in both cases will be R2, and we're curious to see whether those R2 values are higher for the adjusted values.\nThe first step is to create the data set, sampling_team, that we'll use for sampling purposes. (Again, we'll filter out the lockout-shortened season.) We'll group by game_id as well as PP_team and summarize the raw fenwick values, adjusted fenwick values, goals, and total TOI. We'll also create a unique identifier of team_season that will be important for sampling purposes.\n# Testing repeatability and prediction---- # We\u0026#39;ll test repeatability of the adjusted values, compared to the raw ones # But we\u0026#39;ll be removing the shortened season # Group by game, season, and team; sum values sampling_team \u0026lt;- score_adj_5v4_w_values %\u0026gt;% select(game_id, season, PP_team, PP_fenwick, fenwick_adj, event_length, goal) %\u0026gt;% filter(season != 20122013) %\u0026gt;% group_by(season, game_id, PP_team) %\u0026gt;% summarize(fenwick = sum(PP_fenwick, na.rm = TRUE), fen_adj = sum(fenwick_adj, na.rm = TRUE), goals = sum(goal), TOI = sum(event_length) / 60) %\u0026gt;% unite(team_season, season, PP_team, sep = \u0026#34;-\u0026#34;, remove = FALSE) That sampling_team data frame will be the starting point for the sampling function I wrote that's based on the rep_sample_n function in the infer package. In a nutshell, the function below will do the following: filter the sampling_team data frame to a specific team_season; take a sample of x number of games; split that sample into two groups; sum the unblocked shot attempts (both raw and adjusted), goals, and TOI in each group; and repeat 1000 times. The pivot_wider function at the end will just reshape the data into a format that's easier to use later.\n# The function below will filter by teamseason, sample a specified amount of games,  # split into two groups, sum fenwick and fen_adj, repeat 1000 times per season sampling_fn \u0026lt;- function(value, samplesize) { sampling_team_done \u0026lt;- sampling_team %\u0026gt;% filter(team_season == value) %\u0026gt;% rep_sample_n(size = samplesize, replace = FALSE, reps = 1000) %\u0026gt;% group_by(replicate, team_season) %\u0026gt;% mutate(game_no = row_number()) %\u0026gt;% mutate(group = game_no %% 2) %\u0026gt;% mutate(samplesize = samplesize) %\u0026gt;% group_by(replicate, group, team_season, samplesize) %\u0026gt;% summarize(fenwick = sum(fenwick, na.rm = TRUE), fen_adj = sum(fen_adj, na.rm = TRUE), TOI = sum(TOI), goals = sum(goals)) %\u0026gt;% mutate(f_60 = fenwick * 60 / TOI, f_adj_60 = fen_adj * 60 / TOI, goal_60 = goals * 60 / TOI) %\u0026gt;% pivot_wider(names_from = group, values_from = c(fenwick, fen_adj, TOI, f_60, f_adj_60, goals, goal_60)) } Once the function is written, we need to apply it! You can see that our sampling_fn function requires two arguments to be supplied: value and samplesize. The value is each individual team season, since we want the function to run over each one separately. And I was curious to see how the results vary by the sample size chosen, so the code below will run separately for sample sizes of 40, 50, 60, and 70 games. Micah's 5v5 method was tested with sample sizes of 40 games, but since 5v4 time is comparatively much more rare, I wanted to test larger sample sizes as well. (This is why we eliminated the lockout-shortened season, which only had 48 games.)\nlapply is a very useful function that will perform the sampling_fn, with the selected samplesize, over each unique value of team_season from the sampling_team data frame. And the bind_rows function will collect all of the function results into one data frame.\n# Run the function for the various sample sizes summary_team \u0026lt;- lapply(unique(sampling_team$team_season), sampling_fn, samplesize = 40) sampling_team_40 \u0026lt;- bind_rows(summary_team, .id = \u0026#34;column_label\u0026#34;) summary_team \u0026lt;- lapply(unique(sampling_team$team_season), sampling_fn, samplesize = 50) sampling_team_50 \u0026lt;- bind_rows(summary_team, .id = \u0026#34;column_label\u0026#34;) summary_team \u0026lt;- lapply(unique(sampling_team$team_season), sampling_fn, samplesize = 60) sampling_team_60 \u0026lt;- bind_rows(summary_team, .id = \u0026#34;column_label\u0026#34;) summary_team \u0026lt;- lapply(unique(sampling_team$team_season), sampling_fn, samplesize = 70) sampling_team_70 \u0026lt;- bind_rows(summary_team, .id = \u0026#34;column_label\u0026#34;) I will end up with four data frames, sampling_team_40 through sampling_team_70 that are all the same size. Here is a sample:\n   replicate indicates which sample it is (each individual team_season has 1000) and the _0 and _1 appended to each metric show the two separate groups. For example, the first row here is one sample of 40 games from the Bruins' 2009-10 season. Those 40 games were divided into two groups: the first group had 111 total unblocked shot attempts, and the second group had 132. Having the data structured this way will allow us to easily calculate the correlations.\nWe'll create another function to do so, called correlations, and use the lapply function again to apply that function over our four data frames.\ncorrelations \u0026lt;- function(df) { correlation \u0026lt;- df %\u0026gt;% group_by(samplesize) %\u0026gt;% summarize(raw_f = cor(fenwick_0, fenwick_1) ^ 2, adj_f = cor(fen_adj_0, fen_adj_1) ^ 2, raw_rate = cor(f_60_0, f_60_1) ^ 2, adj_rate = cor(f_adj_60_0, f_adj_60_1) ^ 2, raw_f_pred = cor(fenwick_0, goals_1) ^ 2, adj_f_pred = cor(fen_adj_0, goals_1) ^ 2, raw_rate_pred = cor(f_60_0, goal_60_1) ^ 2, adj_rate_pred = cor(f_adj_60_0, goal_60_1) ^ 2) } cor_all \u0026lt;- lapply(list(sampling_team_40, sampling_team_50, sampling_team_60, sampling_team_70), correlations) cor \u0026lt;- bind_rows(cor_all) That will result in a very simple data frame that looks like this:\n   And we can reshape that data to more easily create some graphs.\n# Create graphs to compare---- # Reshape data to make it easier cor_reshape \u0026lt;- cor %\u0026gt;% pivot_longer(raw_f:adj_rate_pred, names_to = \u0026#34;metric\u0026#34;, values_to = \u0026#34;R2\u0026#34;) %\u0026gt;% mutate(type = substr(metric, 1, 3)) %\u0026gt;% arrange(type) %\u0026gt;% mutate(type = factor(type, levels=c(\u0026#34;raw\u0026#34;, \u0026#34;adj\u0026#34;))) cor_reshape %\u0026gt;% filter(metric %in% c(\u0026#34;raw_f\u0026#34;, \u0026#34;adj_f\u0026#34;)) %\u0026gt;% ggplot(aes(fill = type, x = samplesize, y = R2)) + geom_bar(stat = \u0026#34;identity\u0026#34;, position = \u0026#34;dodge\u0026#34;) + labs(title = \u0026#34;Repeatability: Unblocked Shot Attempts\u0026#34;, x = \u0026#34;Sample Size\u0026#34;, y = \u0026#34;R-Squared\u0026#34;, fill = \u0026#34;Adjusted?\u0026#34;) + theme_linedraw() + theme(axis.ticks = element_blank()) + geom_text(aes(label = round(R2, 3)), position = position_dodge(10), vjust = -0.5, size = 3) + scale_fill_manual(values = c(\u0026#34;#DDDDDD\u0026#34;, \u0026#34;#CF8BA8\u0026#34;)) cor_reshape %\u0026gt;% filter(metric %in% c(\u0026#34;raw_rate\u0026#34;, \u0026#34;adj_rate\u0026#34;)) %\u0026gt;% ggplot(aes(fill = type, x = samplesize, y = R2)) + geom_bar(stat = \u0026#34;identity\u0026#34;, position = \u0026#34;dodge\u0026#34;) + labs(title = \u0026#34;Repeatability: Unblocked Shot Attempt Rate\u0026#34;, x = \u0026#34;Sample Size\u0026#34;, y = \u0026#34;R-Squared\u0026#34;, fill = \u0026#34;Adjusted?\u0026#34;) + theme_linedraw() + theme(axis.ticks = element_blank()) + geom_text(aes(label = round(R2, 3)), position = position_dodge(10), vjust = -0.5, size = 3) + scale_fill_manual(values = c(\u0026#34;#DDDDDD\u0026#34;, \u0026#34;#CF8BA8\u0026#34;)) cor_reshape %\u0026gt;% filter(metric %in% c(\u0026#34;raw_rate_pred\u0026#34;, \u0026#34;adj_rate_pred\u0026#34;)) %\u0026gt;% ggplot(aes(fill = type, x = samplesize, y = R2)) + geom_bar(stat = \u0026#34;identity\u0026#34;, position = \u0026#34;dodge\u0026#34;) + labs(title = \u0026#34;Predictivity: Unblocked Shot Attempt Rate\u0026#34;, x = \u0026#34;Sample Size\u0026#34;, y = \u0026#34;R-Squared\u0026#34;, fill = \u0026#34;Adjusted?\u0026#34;) + theme_linedraw() + theme(axis.ticks = element_blank()) + geom_text(aes(label = round(R2, 3)), position = position_dodge(10), vjust = -0.5, size = 3) + scale_fill_manual(values = c(\u0026#34;#DDDDDD\u0026#34;, \u0026#34;#CF8BA8\u0026#34;)) The two figures below show the R2 values for raw and adjusted values, by sample size, for the unblocked shot attempts and the unblocked shot attempt rate, respectively. As expected, the correlation increases along with the sample size. And although none of the R2 values are particularly large, they are consistently higher for the adjusted values. (As a comparison, Micah found a R2 value of 0.530 for a similar test with 5v5 adjusted values.)\n      Also as expected, the R2 value is much smaller when we look at prediction: how the unblocked shot attempt rate in one group predicts the goal rate in another group. Again as comparison: Micah used goal percentage instead for his prediction test and found a R2 value of 0.113.\n   My personal conclusion is that there appears to be some value from adjusting for score effects at 5v4, but I'm not sure it's enough to recommend score adjustment as common practice. For context, I'd also like to explore further into how score effects might affect the rate of drawing or taking penalties.\n","date":1579046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579046400,"objectID":"0ed8ca98ebb3b775ed3636b7c4c0da16","permalink":"/post/score-effects/","publishdate":"2020-01-15T00:00:00Z","relpermalink":"/post/score-effects/","section":"post","summary":"Score effects in hockey are well-known, whether you're watching the game or looking at numbers: teams that are losing tend to generate a greater share of the shot attempts. Micah Blake McCurdy developed an adjustment method for 5v5 events that is currently used to create the score- and venue-adjusted shot attempt metrics available at hockey stats websites like Natural Stat Trick and Evolving Hockey.\nSince I spend a lot of time looking at special teams data, I have long been curious as to whether score effects might also be a factor for power plays.","tags":null,"title":"Examining Score Effects on Special Teams","type":"post"},{"authors":null,"categories":null,"content":"From Hockey-Graphs\nI have written a couple articles over the past few months on using R with hockey data (see here and here), but both of those articles were focused on intermediate techniques and presumed beginner knowledge of R. In contrast, this article is for the complete beginner. Weâ€™ll go through the steps of downloading and setting up R and then, with the use of a sample hockey data set, learn the very basics of R for exploring and visualizing data.\n Click here to read the rest of this article on Hockey-Graphs and here to see the code on Github.   ","date":1576022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576022400,"objectID":"38fec2007e46690304b837eb7d9b5a56","permalink":"/post/r-tutorial/","publishdate":"2019-12-11T00:00:00Z","relpermalink":"/post/r-tutorial/","section":"post","summary":"From Hockey-Graphs\nI have written a couple articles over the past few months on using R with hockey data (see here and here), but both of those articles were focused on intermediate techniques and presumed beginner knowledge of R. In contrast, this article is for the complete beginner. Weâ€™ll go through the steps of downloading and setting up R and then, with the use of a sample hockey data set, learn the very basics of R for exploring and visualizing data.","tags":null,"title":"An Introduction to R With Hockey Data","type":"post"},{"authors":[],"categories":null,"content":"   Click link above to view data   ","date":1573862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573862400,"objectID":"56257c951fac30e79ca6b96277812379","permalink":"/talk/otthac/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/otthac/","section":"talk","summary":"Tracking data from the 2018-19 season to discuss broad defensive themes on the penalty kill.","tags":[],"title":"Discrete Defensive Strategies on the Penalty Kill","type":"talk"},{"authors":null,"categories":null,"content":"From Nightingale, the journal of the Data Visualization Society\nAs the 2019â€“20 NHL hockey season kicks off, Tableau data-vizzists (data visualizers? data viz artisans?) Meghan Hall and Sean Tierney took some time to discuss how they got started with visualizations in hockey, their processes for creating interactive charts, and what they see as the future of data visualization for the NHL.\n Click here to read the rest of this article on Medium.   ","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"830a7eb642c3f30419a68341348f33ea","permalink":"/post/medium/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/post/medium/","section":"post","summary":"From Nightingale, the journal of the Data Visualization Society\nAs the 2019â€“20 NHL hockey season kicks off, Tableau data-vizzists (data visualizers? data viz artisans?) Meghan Hall and Sean Tierney took some time to discuss how they got started with visualizations in hockey, their processes for creating interactive charts, and what they see as the future of data visualization for the NHL.\n Click here to read the rest of this article on Medium.","tags":null,"title":"So You Want to Make a Hockey Data Viz?","type":"post"},{"authors":null,"categories":null,"content":"From Hockey-Graphs\nWelcome to the second article in our series on basic data cleaning and data manipulation! In this article, weâ€™re going to use play-by-play data from two NHL games and answer two questions:\n which power play unit generated the best shot rate in each game? which defenseman played the most 5v5 minutes in each game?  In the process of doing so, weâ€™ll cover several topics of basic data manipulation in the tidyverse, including using functions, creating joins, grouping and summarizing data, and working with string data.\n Click here to read the rest of this article on Hockey-Graphs and here to see the code on Github.   ","date":1570492800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570492800,"objectID":"c76a5641ebf06b396537e2c8bf47cc93","permalink":"/post/r-article-2/","publishdate":"2019-10-08T00:00:00Z","relpermalink":"/post/r-article-2/","section":"post","summary":"From Hockey-Graphs\nWelcome to the second article in our series on basic data cleaning and data manipulation! In this article, weâ€™re going to use play-by-play data from two NHL games and answer two questions:\n which power play unit generated the best shot rate in each game? which defenseman played the most 5v5 minutes in each game?  In the process of doing so, weâ€™ll cover several topics of basic data manipulation in the tidyverse, including using functions, creating joins, grouping and summarizing data, and working with string data.","tags":null,"title":"Exploratory Data Analysis Using Tidyverse","type":"post"},{"authors":null,"categories":null,"content":"From Hockey-Graphs\nA tutorial on how to combine NHL play-by-play data with manually-tracked data, with the help of the padr package in R.\n Click here to read this article on Hockey-Graphs.   ","date":1568851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568851200,"objectID":"16057f3ab74fbecf09ccb3baa47c5833","permalink":"/post/r-article-1/","publishdate":"2019-09-19T00:00:00Z","relpermalink":"/post/r-article-1/","section":"post","summary":"From Hockey-Graphs\nA tutorial on how to combine NHL play-by-play data with manually-tracked data, with the help of the padr package in R.\n Click here to read this article on Hockey-Graphs.   ","tags":null,"title":"Combining Manually-Tracked Data with Play-by-Play Data","type":"post"},{"authors":[],"categories":null,"content":"   Click link above to view data   ","date":1568419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568419200,"objectID":"1f0bf0694b1ae5af9a86b5c3fac32400","permalink":"/talk/ritsac/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/ritsac/","section":"talk","summary":"Examining the last four seasons of NHL data to explore the trend of increasing offense on the penalty kill.","tags":[],"title":"Tracking Increasing Offense on the Penalty Kill","type":"talk"},{"authors":["Meghan Hall"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":null,"content":"   Click link above to view data   ","date":1552176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552176000,"objectID":"0bc95d4d7e60b7396e21935238fa87d8","permalink":"/talk/seahac/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/seahac/","section":"talk","summary":"Exploring the trend of when goalies get pulled in the NHL.","tags":[],"title":"Aggression and Success in Goalie Pulling","type":"talk"},{"authors":[],"categories":[],"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three  A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let's make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Meghan Hall","Robert Ford"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Meghan Hall","Robert Ford"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]